{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed950b5f2833500a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.132289Z",
     "start_time": "2024-07-24T08:49:53.119738Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"api.env\")\n",
    "INDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:53.620255Z",
     "start_time": "2024-07-24T08:49:53.133121Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from indox.IndoxEval.llms import IndoxApi\n",
    "llm = IndoxApi(api_key=INDOX_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e799a33363755e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.096687Z",
     "start_time": "2024-07-24T08:49:54.093084Z"
    }
   },
   "outputs": [],
   "source": [
    "test_case = {\n",
    "                    \"messages\": [\n",
    "                        {\"query\": \"What are the best books for learning Python?\",\n",
    "                         \"llm_response\": \"Automate the Boring Stuff with Python, Python Crash Course, etc.\"},\n",
    "                        {\"query\": \"What is the capital of France?\", \"llm_response\": \"The capital of France is Paris.\"}\n",
    "                    ]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb70816e2109925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.427198Z",
     "start_time": "2024-07-24T08:49:54.423147Z"
    }
   },
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"llm_response\": \"The capital of France is Paris.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Can you tell me about the Eiffel Tower?\",\n",
    "        \"llm_response\": \"The Eiffel Tower, located in Paris, is one of the most iconic landmarks in the world.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are some famous foods in France?\",\n",
    "        \"llm_response\": \"France is known for its cuisine, including dishes like croissants, baguettes, and cheese.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983cd49670fe3394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:54.838628Z",
     "start_time": "2024-07-24T08:49:54.835267Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"What is the capital of Japan?\"\n",
    "llm_response = \"The capital of Japan is Tokyo.\"\n",
    "retrieval_context = [\"Tokyo is the most populous city in Japan and serves as the country's political and economic center.\",\n",
    "                     \"Tokyo has been the capital since 1868, after the Meiji Restoration moved the capital from Kyoto.\",\n",
    "                     \"The city is known for its mix of modern architecture and traditional temples, as well as its bustling districts like Shibuya and Shinjuku.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741f1c522cf4a890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.227679Z",
     "start_time": "2024-07-24T08:49:55.223760Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import Faithfulness\n",
    "faithfulness_evaluator = Faithfulness(llm_response=llm_response,retrieval_context=retrieval_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383c8eb91a2c2532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.527497Z",
     "start_time": "2024-07-24T08:49:55.523784Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import AnswerRelevancy\n",
    "answer_relevancy_metric = AnswerRelevancy(query=query,llm_response=llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1de7123674cce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:55.827986Z",
     "start_time": "2024-07-24T08:49:55.824382Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import Bias\n",
    "bias_metric = Bias(llm_response=llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e25716ada96525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.104118Z",
     "start_time": "2024-07-24T08:49:56.101015Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import ContextualRelevancy\n",
    "contextual = ContextualRelevancy(query=query,retrieval_context=retrieval_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c2d17a25f3a714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.408440Z",
     "start_time": "2024-07-24T08:49:56.404515Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import GEval\n",
    "geval = GEval(parameters=\"Rag application\",query=query,llm_response=llm_response,ground_truth=\"Tokyo\",context=\"geographic knowledge\",\n",
    "              retrieval_context=retrieval_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73cd7a7ecae55215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.654507Z",
     "start_time": "2024-07-24T08:49:56.651101Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import KnowledgeRetention\n",
    "knowledge_retention = KnowledgeRetention(messages=test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843c43180fbc887c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:56.928481Z",
     "start_time": "2024-07-24T08:49:56.925478Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import Hallucination\n",
    "hallucination = Hallucination(llm_response=llm_response,retrieval_context=retrieval_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f34c622dbf50063",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:49:57.229825Z",
     "start_time": "2024-07-24T08:49:57.226918Z"
    }
   },
   "outputs": [],
   "source": [
    "from indox.IndoxEval import Toxicity\n",
    "toxicity = Toxicity(messages=test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d786bf6",
   "metadata": {},
   "source": [
    "## Classic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f489e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from indox.IndoxEval import BertScore\n",
    "# bertscore = BertScore(llm_response=llm_response, retrieval_context=retrieval_context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07bed789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indox.IndoxEval import BLEU\n",
    "bleu = BLEU(llm_response=llm_response, retrieval_context=retrieval_context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce8b0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indox.IndoxEval import Rouge\n",
    "rouge = Rouge(llm_response=llm_response, retrieval_context=retrieval_context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b47a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indox.IndoxEval import METEOR\n",
    "meteor = METEOR(llm_response=llm_response, retrieval_context=retrieval_context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef6ca615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluator initialized with model and metrics.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mModel set for all metrics.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indox.IndoxEval import Evaluator\n",
    "evaluator = Evaluator(model=llm,metrics=[contextual,faithfulness_evaluator,answer_relevancy_metric,\n",
    "                                         hallucination,toxicity,geval,knowledge_retention,bias_metric,bleu,rouge,meteor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff8285425030450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T08:50:51.680754Z",
     "start_time": "2024-07-24T08:49:57.959790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: ContextualRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: ContextualRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Faithfulness\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Faithfulness\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: AnswerRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: AnswerRelevancy\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Hallucination\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Hallucination\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Toxicity\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Toxicity\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: GEval\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: GEval\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: KnowledgeRetention\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: KnowledgeRetention\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Bias\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Bias\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: BLEU\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: BLEU\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: Rouge\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: Rouge\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEvaluating metric: METEOR\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted evaluation for metric: METEOR\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de150d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
       "    'reason': 'No reason provided'},\n",
       "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The context provided does not mention anything about the capital of Japan, which is the main focus of the input question.'}],\n",
       "  'reason': {'reason': 'The score is 0 because the context provided does not mention anything related to the capital of Japan or provide any information about it.'}},\n",
       " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
       "  'truths': ['The capital of Japan is Tokyo.'],\n",
       "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
       "  'reason': 'The score is 1 because the actual output correctly states that the capital of Japan is Tokyo, aligning perfectly with the information presented in the retrieval context.'},\n",
       " 'answer_relevancy': {'score': 1,\n",
       "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
       "  'statements': [],\n",
       "  'verdicts': []},\n",
       " 'hallucination': {'score': 0.6666666666666666,\n",
       "  'reason': \"The score is 0.67 because the actual output lacks detail and contradicts the description of Tokyo's mix of modern architecture and traditional temples, as well as bustling districts like Shibuya and Shinjuku in the provided context.\",\n",
       "  'verdicts': [{'verdict': 'yes',\n",
       "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
       "   {'verdict': 'no',\n",
       "    'reason': 'The actual output lacks detail but does not contradict the provided context which states that Tokyo has been the capital since 1868.'},\n",
       "   {'verdict': 'no',\n",
       "    'reason': \"The actual output contradicts the provided context which describes Tokyo's mix of modern architecture and traditional temples, as well as bustling districts like Shibuya and Shinjuku.\"}]},\n",
       " 'toxicity': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []},\n",
       " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The Rag application response is relevant, accurate, and well-incorporated, but lacks comprehensive contextuality.\"\\n}',\n",
       " 'knowledge_retention': {'score': 1.0,\n",
       "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, demonstrating strong knowledge retention.',\n",
       "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
       "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
       "  'knowledges': [{'Capital of France': 'Paris'},\n",
       "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
       "   {'Capital of France': 'Paris',\n",
       "    'Landmark': 'Eiffel Tower',\n",
       "    'Famous foods in France': []}]},\n",
       " 'bias': {'score': 0,\n",
       "  'reason': 'The score is 0.00 because there are no reasons provided for bias in the actual output, indicating a lack of bias.',\n",
       "  'opinions': [],\n",
       "  'verdicts': []},\n",
       " 'BLEU': {'score': 0.027160352634090387},\n",
       " 'Rouge': {'score': {'Precision': 0.611111111111111,\n",
       "   'Recall': 0.21079852230747373,\n",
       "   'F1-score': 0.3118916732110135}},\n",
       " 'Meteor': {'score': 0.7132217453882491}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10933510c81655",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'contextual_relevancy': {'verdicts': [{'verdict': 'yes',\n",
    "    'reason': 'No reason provided'},\n",
    "   {'verdict': 'yes', 'reason': 'No reason provided'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The context provided does not mention anything about the capital of Japan, which is the main focus of the input question.'}],\n",
    "  'reason': {'reason': 'The score is 0 because the context provided does not mention anything related to the capital of Japan or provide any information about it.'}},\n",
    " 'faithfulness': {'claims': ['The capital of Japan is Tokyo.'],\n",
    "  'truths': ['The capital of Japan is Tokyo.'],\n",
    "  'verdicts': [{'verdict': 'yes', 'reason': None}],\n",
    "  'reason': 'The score is 1 because the actual output correctly states that the capital of Japan is Tokyo, aligning perfectly with the information presented in the retrieval context.'},\n",
    " 'answer_relevancy': {'score': 1,\n",
    "  'reason': 'The score is 1.00 because the answer is perfectly relevant to the input provided. Great job!',\n",
    "  'statements': [],\n",
    "  'verdicts': []},\n",
    " 'hallucination': {'score': 0.6666666666666666,\n",
    "  'reason': \"The score is 0.67 because the actual output lacks detail and contradicts the description of Tokyo's mix of modern architecture and traditional temples, as well as bustling districts like Shibuya and Shinjuku in the provided context.\",\n",
    "  'verdicts': [{'verdict': 'yes',\n",
    "    'reason': \"The actual output agrees with the provided context which states that Tokyo is the most populous city in Japan and serves as the country's political and economic center.\"},\n",
    "   {'verdict': 'no',\n",
    "    'reason': 'The actual output lacks detail but does not contradict the provided context which states that Tokyo has been the capital since 1868.'},\n",
    "   {'verdict': 'no',\n",
    "    'reason': \"The actual output contradicts the provided context which describes Tokyo's mix of modern architecture and traditional temples, as well as bustling districts like Shibuya and Shinjuku.\"}]},\n",
    " 'toxicity': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no reasons provided for toxicity, indicating a non-toxic output.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []},\n",
    " 'geval': '{\\n    \"score\": 8,\\n    \"reason\": \"The Rag application response is relevant, accurate, and well-incorporated, but lacks comprehensive contextuality.\"\\n}',\n",
    " 'knowledge_retention': {'score': 1.0,\n",
    "  'reason': 'The score is 1.00 because there are no attritions indicating forgetfulness, demonstrating strong knowledge retention.',\n",
    "  'verdicts': [{'index': 0, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 1, 'verdict': 'no', 'reason': None},\n",
    "   {'index': 2, 'verdict': 'no', 'reason': None}],\n",
    "  'knowledges': [{'Capital of France': 'Paris'},\n",
    "   {'Capital of France': 'Paris', 'Landmark': 'Eiffel Tower'},\n",
    "   {'Capital of France': 'Paris',\n",
    "    'Landmark': 'Eiffel Tower',\n",
    "    'Famous foods in France': []}]},\n",
    " 'bias': {'score': 0,\n",
    "  'reason': 'The score is 0.00 because there are no reasons provided for bias in the actual output, indicating a lack of bias.',\n",
    "  'opinions': [],\n",
    "  'verdicts': []},\n",
    " 'BLEU': {'score': 0.027160352634090387},\n",
    " 'Rouge': {'score': {'Precision': 0.611111111111111,\n",
    "   'Recall': 0.21079852230747373,\n",
    "   'F1-score': 0.3118916732110135}},\n",
    " 'Meteor': {'score': 0.7132217453882491}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
