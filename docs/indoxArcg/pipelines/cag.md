# Cache-Augmented Generation (CAG) Pipeline Documentation

## Overview

The **Cache-Augmented Generation (CAG) Pipeline** is a retrieval-augmented generation system designed to enhance the quality and relevance of responses generated by a Large Language Model (LLM). It combines local caching, similarity search, and fallback mechanisms to provide accurate and context-aware answers.

The pipeline supports:

- **Multi-query retrieval**: Expands a single query into multiple related queries for better context retrieval.
- **Smart retrieval**: Validates retrieved context for relevance and hallucination.
- **Web search fallback**: Performs web searches when local cache retrieval is insufficient.
- **Customizable similarity search**: Supports TF-IDF, BM25, and Jaccard similarity algorithms.

## Components

### 1. **AnswerValidator**

Validates generated answers for quality and hallucination.

#### Methods:

- `check_hallucination(answer: str, context: List[str]) -> bool`: Checks if the answer is hallucinated based on the provided context.
- `grade_relevance(context: List[str], query: str) -> List[str]`: Grades the relevance of documents to the query.

### 2. **WebSearchFallback**

Handles web search when local context is insufficient.

#### Methods:

- `search(query: str) -> List[str]`: Performs a web search using DuckDuckGo and returns relevant results.

### 3. **CacheEntry**

Represents a cached document with optional embeddings.

#### Attributes:

- `text`: The text content of the document.
- `embedding`: The embedding vector of the document (optional).

### 4. **CAG (Cache-Augmented Generation)**

The core pipeline for cache-augmented generation.

#### Initialization:

```python
def __init__(
    self,
    llm,
    embedding_model: Optional[Any] = None,
    cache: Optional[KVCache] = None,
    default_similarity_search_type: str = "tfidf",
):
```

# Parameters

- **llm**: The LLM instance.
- **embedding_model**: The embedding model instance (optional).
- **cache**: The KV cache instance (optional).
- **default_similarity_search_type**: Default similarity search type ("tfidf", "bm25", or "jaccard").

# Key Methods

- **preload_documents(documents: List[str], cache_key: str)**: Precomputes and saves the KV cache for pre-chunked documents.
- **multi_query_retrieval(query: str, top_k: int = 5) -> List[str]**: Expands the query into multiple queries and retrieves context.
- **smart_retrieve(query: str, cache_entries: List[CacheEntry], top_k: int = 5, similarity_threshold: float = 0.5, similarity_search_type: Optional[str] = None) -> List[str]**: Retrieves context with validation and fallback mechanisms.
- **infer(query: str, cache_key: str, context_strategy: str = "recent", context_turns: int = 5, top_k: int = 5, similarity_threshold: float = 0.5, web_search: bool = False, similarity_search_type: Optional[str] = None, smart_retrieval: bool = False, use_multi_query: bool = False) -> str**: Performs cache-augmented inference.

# Usage

1. **Initialization**
   from your_module import CAG, KVCache

# Initialize LLM, embedding model, and cache

llm = YourLLM()
embedding_model = YourEmbeddingModel()
cache = KVCache()

# Initialize CAG pipeline

cag = CAG(llm, embedding_model, cache)

2. **Preloading Documents**

```python
pythonCopydocuments = ["Document 1 text...", "Document 2 text..."]
cag.preload_documents(documents, cache_key="my_cache")
```

3. **Performing Inference**

```python
query = "What is the capital of France?"
response = cag.infer(
    query=query,
    cache_key="my_cache",
    context_strategy="recent",
    context_turns=5,
    top_k=5,
    similarity_threshold=0.5,
    web_search=True,
    smart_retrieval=True,
)
print(response)
```

# Similarity Search Types

The pipeline supports three similarity search algorithms:

## TF-IDF

- Computes similarity using Term Frequency-Inverse Document Frequency.
- Suitable for general-purpose text retrieval.

## BM25

- A probabilistic retrieval model that improves upon TF-IDF.
- Better for longer documents and keyword-heavy queries.

## Jaccard Similarity

- Measures the overlap between query and document tokens.
- Useful for short queries and documents.

# Error Handling

The pipeline includes robust error handling for:

- **Cache loading failures**: Logs an error if the specified cache key is not found.
- **Web search failures**: Logs a warning if web search fallback fails.
- **Hallucination detection**: Regenerates answers if hallucination is detected.

# Logging

The pipeline uses `loguru` for logging. Logs are formatted as follows:

- **INFO**: General operational messages.
- **ERROR**: Critical errors during execution.
- **WARNING**: Non-critical issues (e.g., no relevant context found).

# Example Workflow

1. Preload documents into the cache.
2. Perform inference with a user query.
3. Retrieve context from the cache using the selected similarity search type.
4. Validate context for relevance and hallucination.
5. Generate a response using the LLM.
6. Fallback to web search if local context is insufficient.

# Notes

- Ensure the `embedding_model` is compatible with the `embed_query` method.
- The `KVCache` class should implement `save_cache` and `load_cache` methods.
- The `MultiQueryRetrieval` tool should be implemented for multi-query expansion.
