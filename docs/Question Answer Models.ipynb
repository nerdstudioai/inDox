{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question Answer Models\n",
    "\n",
    "Indox supports three different types of question-answer (QA) models. These models are:\n",
    "\n",
    "1. **OpenAI QA Model**\n",
    "2. **Mistral QA Model from Hugging Face**\n",
    "3. **OpenAI QA Model with Chain of Thought from the dspy Framework**\n",
    "\n",
    "### Initial Setup\n",
    "\n",
    "For all QA models, the initial setup is the same. Start by importing the necessary Indox module and creating an instance of `IndoxRetrievalAugmentation`:\n",
    "\n",
    "```python\n",
    "from Indox import IndoxRetrievalAugmentation\n",
    "Indox = IndoxRetrievalAugmentation()\n",
    "```\n",
    "\n",
    "## Using OpenAI QA Model\n",
    "To use the OpenAI QA model, follow these steps:\n",
    "\n",
    "1. Import necessary libraries and load environment variables:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "```\n",
    "\n",
    "2. Import Indox modules and set the OpenAI QA model:\n",
    "\n",
    "```python\n",
    "from Indox.QaModels import OpenAiQA\n",
    "\n",
    "openai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "```\n",
    "\n",
    "## Using Mistral QA Model from Hugging Face\n",
    "To use the Mistral QA model from Hugging Face, follow these steps:\n",
    "\n",
    "1. Import necessary libraries and load environment variables:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_API_KEY = os.getenv('HF_API_KEY')\n",
    "```\n",
    "\n",
    "2. Import Indox modules and set the Mistral QA model:\n",
    "\n",
    "```python\n",
    "from Indox.QaModels import MistralQA\n",
    "\n",
    "mistral_qa = MistralQA(api_key=HF_API_KEY, model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "```\n",
    "Note: Users can choose other models from Hugging Face as well, but we recommend the free Mistral model, which only requires a Hugging Face access token.\n",
    "\n",
    "## Using OpenAI QA Model with Chain of Thought from the dspy Framework\n",
    "To use the OpenAI QA model with Chain of Thought from the dspy framework, follow these steps:\n",
    "\n",
    "1. Import necessary libraries and load environment variables:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "```\n",
    "\n",
    "2. Import Indox modules and set the OpenAI QA model with Chain of Thought:\n",
    "\n",
    "```python\n",
    "from Indox.QaModels import DspyCotQA\n",
    "\n",
    "dspy_qa = DspyCotQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "```\n",
    "\n",
    "### Future Plans\n",
    "We are committed to continuously improving Indox and will be adding support for more QA models in the future.\n",
    "   \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7ef1783664bbe54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
