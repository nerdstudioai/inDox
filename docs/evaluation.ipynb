{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Evaluation Metrics\n",
    "---"
   ],
   "id": "c69a273a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how `InDox` uses various evaluation metrics to assess the quality of generated outputs. The available metrics are:\n",
    "\n",
    "- **BertScore**: Evaluates the similarity between the generated text and reference text using BERT embeddings.\n",
    "- **Toxicity**: Measures the level of toxicity in the generated text.\n",
    "- **Similarity**: Assesses the similarity between different pieces of text.\n",
    "- **Reliability**: Evaluates the factual accuracy and trustworthiness of the content.\n",
    "- **Fairness**: Analyzes the text for potential biases and ensures equitable representation.\n",
    "- **Readability**: Assesses how easy the text is to read and understand.\n",
    "\n",
    "You can select the metrics you want to use and then provide the context and answer text for evaluation.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "First, we need to import the `Evaluation` library to build our evaluator function:\n"
   ],
   "id": "e9c25976"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:42:19.506456Z",
     "start_time": "2024-06-09T07:42:10.471927Z"
    }
   },
   "source": [
    "from indox.evaluation import Evaluation"
   ],
   "id": "1dfff2a2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start \n",
    "initiate your evaluator with the *default* config :  \n"
   ],
   "id": "3e0f617c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "evaluator = Evaluation()"
   ],
   "id": "8c46ba58",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "You can choose a list of your disired metrics from `\"BertScore\"`, `\"Toxicity\"`, `\"Similarity\"`, `\"Reliability\"`, `\"Fairness\"` , `\"Readibility\"` \n",
    ":::\n",
    "\n",
    "::: {.callout-tip}\n",
    "if you want to change models with your custom models you can define a `config` in `dict` and customize your `InDox` evaluator  \n",
    "```python \n",
    "cfg = {\"bert_toxic_tokenizer\": \"unitary/toxic-bert\",\n",
    "       \"bert_toxic_model\": \"unitary/toxic-bert\",\n",
    "       \"semantic_similarity\": \"sentence-transformers/bert-base-nli-mean-tokens\",\n",
    "       \"bert_score_model\": \"bert-base-uncased\",\n",
    "       \"reliability\": 'vectara/hallucination_evaluation_model',\n",
    "       \"fairness\": \"wu981526092/Sentence-Level-Stereotype-Detector\",\n",
    "\n",
    "       }\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.callout-caution collapse=\"true\"}\n",
    "it's important to say `InDox` uses open source models and all models are scratched from **HuggingFace**\n",
    ":::\n",
    "\n",
    "After initiate your evalutor check it with sample question answer response , each inputs should be have `question`, `answer`, `context` in a `dict` format , for example : \n"
   ],
   "id": "309b9cf5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample = {\n",
    "    'question' : \"What is your Question?\",\n",
    "    'answer' : \"It's your responsed answer from InDox\",\n",
    "    'context' : \"this can be a list of context or a single context.\"\n",
    "}"
   ],
   "id": "1cfc1ee1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then use evaluator to calculate metrics, evaluator is callable function which in `__call__` function calculates metrics. \n",
    "InDox evaluator returns metrics as `dict` file :  \n"
   ],
   "id": "3db4ec71"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "evaluator(sample)"
   ],
   "id": "bf9934dc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "\"Precision\": 0.46,\n",
    "\"Recall\": 0.75,\n",
    "\"F1-score\": 0.68,\n",
    "\"Toxicity\": 0.01,\n",
    "\"BLEU\": 0.01,\n",
    "\"Jaccard Similarity\": 0.55,\n",
    "\"Cosine Similarity\": 0.68,\n",
    "\"Semantic\": 0.79,\n",
    "\"hallucination_score\" : 0.11, \n",
    "\"Perplexity\": 11,\n",
    "\"ARI\": 0.5,\n",
    "\"Flesch-Kincaid Grade Level\": 0.91\n",
    "}\n",
    "```\n",
    "\n",
    "## Evaluation Updater \n",
    "The `inDox` evaluator provides an `update` function to store evaluation metrics in a data frame. To stack your results, use the `update` function as shown below:\n"
   ],
   "id": "c9051816"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = evaluator.update(inputs)"
   ],
   "id": "64416e06",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be stored in the results data frame, which might look like this:\n"
   ],
   "id": "86aa94c1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results"
   ],
   "id": "5a527bd5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "if you want to clean results use `reset` method to clean it. \n",
    ":::\n",
    "\n",
    "### Metric Explanations\n",
    "- **Precision**: Measures the accuracy of positive predictions. It's the ratio of true positive results to the total predicted positives.\n",
    "- **Recall**: Measures the ability to find all relevant instances. It's the ratio of true positive results to all actual positives.\n",
    "- **F1-score**: The harmonic mean of precision and recall. It balances the two metrics, providing a single measure of a model's performance.\n",
    "- **Toxicity**: Quantifies the level of harmful or abusive content in the text.\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: A metric for evaluating the quality of machine-translated text against one or more reference translations.\n",
    "- **Jaccard Similarity**: Measures the similarity between two sets by dividing the size of the intersection by the size of the union of the sets.\n",
    "- **Cosine Similarity**: Measures the cosine of the angle between two vectors in a multi-dimensional space. Itâ€™s used to determine the similarity between two text samples.\n",
    "- **Semantic Similarity**: Evaluates the degree to which two pieces of text carry the same meaning.\n",
    "- **Hallucination Score**: Indicates the extent to which a model generates information that is not present in the source data.\n",
    "- **Perplexity**: Measures how well a probability model predicts a sample. Lower perplexity indicates a better predictive model.\n",
    "- **ARI** (Automated Readability Index): An index that assesses the readability of a text based on sentence length and word complexity.\n",
    "- **Flesch-Kincaid Grade Level**: An index that indicates the US school grade level required to understand the text.\n",
    "\n",
    "### Custom Evaluator Example Usage\n",
    "Below code provide an example usage of `inDox` evaluation with custom config : \n"
   ],
   "id": "aa154712"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from indox.evaluation import Evaluation\n",
    "\n",
    "cfg = {\n",
    "       \"bert_score_model\": \"bert-base-uncased\",\n",
    "       }\n",
    "\n",
    "dimanstions = [\"BertScore\"]\n",
    "\n",
    "evaluator = Evaluation(cfg = cfg , dimanstions = dimanstions)\n",
    "\n",
    "\n",
    "inputs1 = {\n",
    "    'question': 'What is the capital of France?',\n",
    "    'answer': 'The capital of France is Paris.',\n",
    "    'context': 'France is a country in Western Europe. Paris is its capital.'\n",
    "}\n",
    "\n",
    "inputs2 = {\n",
    "    'question': 'What is the largest planet in our solar system?',\n",
    "    'answer': 'The largest planet in our solar system is Jupiter.',\n",
    "    'context': 'Our solar system consists of eight planets. Jupiter is the largest among them.'\n",
    "}\n",
    "results = evaluator.update(inputs1)\n",
    "results = evaluator.update(inputs2)"
   ],
   "id": "099b2617",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results:\n"
   ],
   "id": "415ab114"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(results)"
   ],
   "id": "a92710a2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "---\n",
    "The results data frame will contain evaluation metrics for each input:"
   ],
   "id": "08763721"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ">>>"
   ],
   "id": "b955faf2",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
