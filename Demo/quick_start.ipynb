{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unstructured Load And Split",
   "id": "7e95779738aca6f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T09:39:37.883023Z",
     "start_time": "2024-06-09T09:39:37.866903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ],
   "id": "88e8c38ba3b8886d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial Setup\n",
    "\n",
    "The following imports are essential for setting up the Indox application. These imports include the main Indox retrieval augmentation module, question-answering models, embeddings, and data loader splitter."
   ],
   "id": "ac995737f9b2fe6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:05.988214Z",
     "start_time": "2024-06-09T05:05:05.942450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "id": "131692307c9154db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating response using OpenAI's language models \n",
    "OpenAIQA class is used to handle question-answering task using OpenAI's language models. This instance creates OpenAiEmbedding class to specifying embedding model. Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ],
   "id": "759db8f502cbd91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:23.229746Z",
     "start_time": "2024-06-09T05:05:15.047165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import OpenAi\n",
    "from indox.embeddings import OpenAiEmbedding\n",
    "openai_qa = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "embed_openai = OpenAiEmbedding(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n",
    "\n",
    "from indox.vector_stores import ChromaVectorStore\n",
    "db = ChromaVectorStore(collection_name=\"sample\",embedding=embed_openai)\n",
    "indox.connect_to_vectorstore(vectorstore_database=db)"
   ],
   "id": "f32d98545c6d3c3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x1d0407cb440>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ],
   "id": "1e3408e8f8a8ad17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:23.234626Z",
     "start_time": "2024-06-09T05:05:23.231753Z"
    }
   },
   "cell_type": "code",
   "source": "file_path = \"sample.txt\"",
   "id": "1d8e56a9f88e03cf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:42.358388Z",
     "start_time": "2024-06-09T05:05:23.235633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\n",
    "docs = loader_splitter.load_and_chunk()"
   ],
   "id": "827c44ce67f972c7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:45.415988Z",
     "start_time": "2024-06-09T05:05:42.368097Z"
    }
   },
   "cell_type": "code",
   "source": "indox.store_in_vectorstore(docs=docs)",
   "id": "4557891dec337e31",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x1d0407cb440>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ],
   "id": "cd6bd4924ad116fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:45.421284Z",
     "start_time": "2024-06-09T05:05:45.416996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"How Cinderella reach her happy ending?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa, top_k=5)"
   ],
   "id": "593ec3a85c796115",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "invoke(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information.\n",
    "Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used."
   ],
   "id": "9e778403c8d864c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:48.940349Z",
     "start_time": "2024-06-09T05:05:45.422293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever.invoke(query)\n",
    "retriever.context"
   ],
   "id": "58d79450c0807286",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"to appear among the number, they were delighted, called cinderella\\n\\nand said, comb our hair for us, brush our shoes and fasten our\\n\\nbuckles, for we are going to the wedding at the king's palace.\\n\\nCinderella obeyed, but wept, because she too would have liked to\\n\\ngo with them to the dance, and begged her step-mother to allow\\n\\nher to do so. You go, cinderella, said she, covered in dust and\",\n",
       " \"which they had wished for, and to cinderella he gave the branch\\n\\nfrom the hazel-bush. Cinderella thanked him, went to her mother's\\n\\ngrave and planted the branch on it, and wept so much that the tears\\n\\nfell down on it and watered it. And it grew and became a handsome\\n\\ntree. Thrice a day cinderella went and sat beneath it, and wept and\\n\\nprayed, and a little white bird always came on the tree, and if\",\n",
       " 'by the hearth in the cinders. And as on that account she always\\n\\nlooked dusty and dirty, they called her cinderella.\\n\\nIt happened that the father was once going to the fair, and he\\n\\nasked his two step-daughters what he should bring back for them.\\n\\nBeautiful dresses, said one, pearls and jewels, said the second.\\n\\nAnd you, cinderella, said he, what will you have. Father',\n",
       " \"Then the maiden was delighted, and believed that she might now go\\n\\nwith them to the wedding. But the step-mother said, all this will\\n\\nnot help. You cannot go with us, for you have no clothes and can\\n\\nnot dance. We should be ashamed of you. On this she turned her\\n\\nback on cinderella, and hurried away with her two proud daughters.\\n\\nAs no one was now at home, cinderella went to her mother's\",\n",
       " \"danced with her only, and if any one invited her to dance, he said\\n\\nthis is my partner.\\n\\nWhen evening came, cinderella wished to leave, and the king's\\n\\nson was anxious to go with her, but she escaped from him so quickly\\n\\nthat he could not follow her. The king's son, however, had\\n\\nemployed a ruse, and had caused the whole staircase to be smeared\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### With AgenticRag\n",
    "\n",
    "AgenticRag stands for Agentic Retrieval-Augmented Generation. This concept combines retrieval-based methods and generation-based methods in natural language processing (NLP). The key idea is to enhance the generative capabilities of a language model by incorporating relevant information retrieved from a database or a vector store. \n",
    " AgenticRag is designed to provide more contextually rich and accurate responses by utilizing external knowledge sources. It retrieves relevant pieces of information (chunks) from a vector store based on a query and then uses a language model to generate a comprehensive response that incorporates this retrieved information."
   ],
   "id": "486878b3d49c9871"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T05:05:55.712801Z",
     "start_time": "2024-06-09T05:05:48.941354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = indox.AgenticRag(llm=openai_qa,vector_database=db,top_k=5)\n",
    "agent.run(query)"
   ],
   "id": "394533a0e6ab8228",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Relevant doc\n",
      "Relevant doc\n",
      "Not Relevant doc\n",
      "Not Relevant doc\n",
      "Not Relevant doc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Cinderella reached her happy ending by receiving a branch from the hazel-bush from the prince, planting it on her mother's grave, and weeping and praying beneath it. The branch grew into a handsome tree, and a little white bird always came to the tree, bringing her comfort and assistance.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b9a54963634a56c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
