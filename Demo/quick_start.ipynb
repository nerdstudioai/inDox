{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Quick Start",
   "id": "7e95779738aca6f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/Demo/quick_start.ipynb)",
   "id": "f24e01441be4b95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install indox\n",
    "!pip install openai\n",
    "!pip install chromadb"
   ],
   "id": "2e19065cca32a590"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:47:31.424039Z",
     "start_time": "2024-07-12T12:47:31.374968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ],
   "id": "88e8c38ba3b8886d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial Setup\n",
    "\n",
    "The following imports are essential for setting up the Indox application. These imports include the main Indox retrieval augmentation module, question-answering models, embeddings, and data loader splitter."
   ],
   "id": "ac995737f9b2fe6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:47:31.704047Z",
     "start_time": "2024-07-12T12:47:31.425046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ],
   "id": "131692307c9154db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mIndoxRetrievalAugmentation initialized\u001B[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generating response using OpenAI's language models \n",
    "OpenAIQA class is used to handle question-answering task using OpenAI's language models. This instance creates OpenAiEmbedding class to specifying embedding model. Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ],
   "id": "759db8f502cbd91f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:47:38.829749Z",
     "start_time": "2024-07-12T12:47:33.257956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.llms import OpenAi\n",
    "from indox.embeddings import OpenAiEmbedding\n",
    "openai_qa = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "embed_openai = OpenAiEmbedding(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n",
    "\n",
    "from indox.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed_openai)\n",
    "indox.connect_to_vectorstore(vectorstore_database=db)"
   ],
   "id": "f32d98545c6d3c3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitializing OpenAi with model: gpt-3.5-turbo-0125\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mOpenAi initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mInitialized OpenAI embeddings with model: text-embedding-3-small\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mConnection to the vector store database established successfully\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x226c96bb7a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ],
   "id": "1e3408e8f8a8ad17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt",
   "id": "e765b79a32060e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:47:43.119679Z",
     "start_time": "2024-07-12T12:47:43.117117Z"
    }
   },
   "cell_type": "code",
   "source": "file_path = \"sample.txt\"",
   "id": "1d8e56a9f88e03cf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:48:24.113981Z",
     "start_time": "2024-07-12T12:47:43.445664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\n",
    "docs = loader_splitter.load_and_chunk()"
   ],
   "id": "827c44ce67f972c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mUnstructuredLoadAndSplit initialized successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGetting all documents\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mStarting processing\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mUsing title-based chunking\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mCompleted chunking process\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mSuccessfully obtained all documents\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:48:30.136711Z",
     "start_time": "2024-07-12T12:48:24.114998Z"
    }
   },
   "cell_type": "code",
   "source": "indox.store_in_vectorstore(docs=docs)",
   "id": "4557891dec337e31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mStoring documents in the vector store\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocument added successfully to the vector store.\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mDocuments stored successfully\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x226c96bb7a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ],
   "id": "cd6bd4924ad116fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:48:30.140666Z",
     "start_time": "2024-07-12T12:48:30.137717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"How Cinderella reach her happy ending?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa, top_k=5)"
   ],
   "id": "593ec3a85c796115",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "invoke(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information.\n",
    "Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used."
   ],
   "id": "9e778403c8d864c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:48:37.347604Z",
     "start_time": "2024-07-12T12:48:30.142674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = retriever.invoke(query)\n",
    "context = retriever.context"
   ],
   "id": "58d79450c0807286",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO\u001B[0m: \u001B[1mRetrieving context and scores from the vector database\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating answer without document relevancy filter\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mAnswering question\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mGenerating response\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mResponse generated successfully\u001B[0m\n",
      "\u001B[32mINFO\u001B[0m: \u001B[1mQuery answered successfully\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:49:57.280578Z",
     "start_time": "2024-07-12T12:49:57.275540Z"
    }
   },
   "cell_type": "code",
   "source": "answer",
   "id": "4a443f8a7116bd41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cinderella reached her happy ending by being kind, patient, and having a pure heart. Despite facing mistreatment from her step-family, she remained humble and continued to do good deeds. With the help of a little white bird, she was able to receive what she wished for and planted a branch from a hazel bush on her mother's grave, which grew into a beautiful tree. Eventually, during a festival where the prince was to choose a bride, Cinderella's pure heart and beauty shone through, and with the help of the two white doves, she was able to prove her identity as the true bride. This led to her marrying the prince and living happily ever after.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### With AgenticRag\n",
    "\n",
    "AgenticRag stands for Agentic Retrieval-Augmented Generation. This concept combines retrieval-based methods and generation-based methods in natural language processing (NLP). The key idea is to enhance the generative capabilities of a language model by incorporating relevant information retrieved from a database or a vector store. \n",
    " AgenticRag is designed to provide more contextually rich and accurate responses by utilizing external knowledge sources. It retrieves relevant pieces of information (chunks) from a vector store based on a query and then uses a language model to generate a comprehensive response that incorporates this retrieved information."
   ],
   "id": "486878b3d49c9871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agent = indox.AgenticRag(llm=openai_qa,vector_database=db,top_k=5)\n",
    "agent.run(query)"
   ],
   "id": "394533a0e6ab8228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "6ef035b53ef74d65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:57:27.131723Z",
     "start_time": "2024-07-02T06:57:15.072106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from indox.evaluation import Evaluation\n",
    "evaluator = Evaluation([\"BertScore\", \"Toxicity\",  \"Reliability\", \"Fairness\" , \"Readibility\"])"
   ],
   "id": "750a052ed290bdf9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 10:27:22,324 INFO:Use pytorch device: cpu\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:57:32.071506Z",
     "start_time": "2024-07-02T06:57:29.153596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = {\n",
    "    \"question\" : query,\n",
    "    \"answer\" : answer,\n",
    "    \"context\" : context\n",
    "}\n",
    "result = evaluator(inputs)"
   ],
   "id": "caeb4a7cb21df923",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9eb4a59dbd7c439b8b514decbe503037"
      },
      "application/json": {
       "n": 0,
       "total": 1,
       "elapsed": 0.010510921478271484,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f5252e1e84f4ad7a84fa2a8fda2ca64"
      },
      "application/json": {
       "n": 0,
       "total": 1,
       "elapsed": 0.004000425338745117,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76d383160a4a44e1bbcdf383b8238fc1"
      },
      "application/json": {
       "n": 0,
       "total": 1,
       "elapsed": 0.005513668060302734,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a76ad5c6d4674b23b801b62c6873d7ed"
      },
      "application/json": {
       "n": 0,
       "total": 1,
       "elapsed": 0.004008054733276367,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5843a130485d4ff98c7fa65ce8cf19e1"
      },
      "application/json": {
       "n": 0,
       "total": 1,
       "elapsed": 0.004518032073974609,
       "ncols": null,
       "nrows": null,
       "prefix": "Batches",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T06:57:33.713540Z",
     "start_time": "2024-07-02T06:57:33.707971Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "8af894ed689460b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                    0\n",
       "Precision                    0.493488\n",
       "Recall                       0.517235\n",
       "F1-score                     0.505083\n",
       "Toxicity                     0.081307\n",
       "hallucination_score          0.620000\n",
       "Fairness                     0.496180\n",
       "Perplexity                  38.828209\n",
       "ARI                         12.800000\n",
       "Flesch-Kincaid Grade Level  10.100000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.493488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.517235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score</th>\n",
       "      <td>0.505083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toxicity</th>\n",
       "      <td>0.081307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hallucination_score</th>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fairness</th>\n",
       "      <td>0.496180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>38.828209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARI</th>\n",
       "      <td>12.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flesch-Kincaid Grade Level</th>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "34dd57882af42333"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
