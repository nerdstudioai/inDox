{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e400a2c6-ede3-4d45-adc3-85aff39a78a8",
   "metadata": {},
   "source": [
    "# Indox Retrieval Augmentation\n",
    "Here, we will explore how to work with Indox Retrieval Augmentation. First, if you are using OpenAI, you should set your OpenAI key as an environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef1431e-0c38-4553-8f2e-5beb75afb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32925ea9-5073-49a3-a6ff-7bc48e076887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHKAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from Indox import IndoxRetrievalAugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81226b63-5bf9-41a5-8e4f-84b82a9af3a9",
   "metadata": {},
   "source": [
    "### Creating an Instance of IndoxRetrievalAugmentation\r\n",
    "To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba27204a-c2cb-4c3e-a367-0c035601e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRA = IndoxRetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13509229-b8ff-4760-93ff-d397041b20b2",
   "metadata": {},
   "source": [
    "Please ensure that the configuration is adjusted accordingly; however, it is imperative that both the embedding model and the question-answer model are initialized within the YAML configuration file prior to the instantiation of the IndoxRetrievalAugmentation instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f854224-6028-4799-a746-5bdf556ac989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clustering': {'dim': 10, 'threshold': 0.1},\n",
       " 'embedding_model': 'openai',\n",
       " 'postgres': {'conn_string': 'postgresql+psycopg2://postgres:xxx@localhost:port/db_name'},\n",
       " 'prompts': {'summary_model': {'content': 'You are a helpful assistant. Give a detailed summary of the documentation provided'}},\n",
       " 'qa_model': {'temperature': 0},\n",
       " 'splitter': 'semantic-text-splitter',\n",
       " 'summary_model': {'max_tokens': 100,\n",
       "  'min_len': 30,\n",
       "  'model_name': 'gpt-3.5-turbo-0125'},\n",
       " 'vector_store': 'pgvector'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee80714-82a1-4ec6-9e68-10a003b4640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRA.config[\"vector_store\"] = \"chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d201ddb8-720b-4fbf-a359-24532ce7f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRA.update_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc286682-a24e-454d-9bf6-5f24c83f8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file_path = \"sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81211d-0a72-4226-ab75-1677857fb046",
   "metadata": {},
   "source": [
    "### Chunk Creation and Processing Workflow\n",
    "\n",
    "When initiating the chunk creation process from a document, specify the file path of the document (which can be either a plain text or PDF file) along with the maximum chunk size. This setup configures how the document will be segmented into chunks.\n",
    "\n",
    "For the segmentation, you have the option to utilize the Semantic Text Splitter, which focuses on the semantic separation of the text into coherent chunks. Alternatively, you may choose the Raptor Text Splitter.\n",
    "\n",
    "During this step, you will be prompted to decide whether you want to add an additional clustering layer. If you opt for yes, the process will proceed as follows:\n",
    "- Leaf Chunk Creation: Initially, leaf chunks are created based on the specified text splitter.\n",
    "- Clustering: These leaf chunks are then clustered to group semantically similar segments together.\n",
    "- Summarization: Each cluster is summarized, with the summaries themselves potentially being used to create additional, finer chunks.\n",
    "- Iteration: This process iterates, continuing to cluster and summarize, until no further clustering is viable.\n",
    "\n",
    "The maximum token limit for the summaries is defined by the summary model settings in your configuration file. This iterative process is designed to refine and enhance the coherence and relevance of the chunks produced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54726aa5-a33d-4d81-b5f1-0849c065bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to add a clustering and summarization layer? This may double your token usage. Please select 'y' for yes or 'n' for no:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:33:25,168 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:33:29,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create 12 Chunks, 11 leaf chunks plus 1 extra chunks\n",
      "End Chunking & Clustering process\n"
     ]
    }
   ],
   "source": [
    "all_chunks = IRA.create_chunks_from_document(docs=docs_file_path,max_chunk_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406123ea-2b92-4477-8094-4cd7f8135cc4",
   "metadata": {},
   "source": [
    "### Connecting to the Vector Database and Storing Data\n",
    "\n",
    "Step 1: Connect to the Vector Database:\n",
    "Start by extracting the connection settings from your configuration file. These settings should include the database connection string and any other necessary parameters.Just pass the collection name.\n",
    "\n",
    "Step 2: Store Chunks:\r\n",
    "Use the store_in_vectorstore method of your database client to store the prepared chunksre.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e373f6ff-3533-4366-9dd8-a40ee88e1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:33:37,472 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "IRA.connect_to_vectorstore(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45891ce6-c527-4974-82e5-adce51574a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:33:40,416 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-27 14:33:40,740 - INFO - Document added successfully to the vector store.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Indox.vectorstore.ChromaVectorStore at 0x2a5b853c590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRA.store_in_vectorstore(all_chunks=all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73c7ae-04e2-4a59-9092-8a0b476c9631",
   "metadata": {},
   "source": [
    "Execute a query and retrieve the responses, along with the scores of the retrieved chunks and the context that was sent to the language learning model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca40f413-7a0a-4472-a41b-df0eef08f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How did Cinderella reach her happy ending?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199a9e27-f38e-492e-9e59-bcd2521a978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-27 14:35:19,837 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-04-27 14:35:22,988 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response, scores, context = IRA.answer_question(query,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a654a211-a55d-47ca-8c15-e470c7200c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Cinderella reached her happy ending by attending a royal festival in a splendid dress and golden slippers provided by a magical bird. At the festival, she captivated the king's son, who danced with her exclusively. When she tried to leave, the king's son attempted to follow her, but she escaped, leaving behind a golden slipper. The king's son then searched for the owner of the slipper, declaring that only the woman whose foot fit the slipper would be his wife. Eventually, the slipper fit Cinderella's foot, and she was reunited with the king's son, leading to their happily ever after.\n"
     ]
    }
   ],
   "source": [
    "print(\"Response:\\n\",response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f5d23c-4dab-413f-961f-343d76ead860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      " ['The provided text is a retelling of the classic fairy tale \"Cinderella.\" It begins with the death of Cinderella\\'s mother and the introduction of her cruel stepmother and stepsisters. They mistreat Cinderella, giving her the nickname \"Cinderella\" due to her dirty appearance from working in the ashes.  Despite her hardships, Cinderella remains kind and pious, seeking solace at her mother\\'s grave under a hazel tree. Here, a magical bird grants her wishes,', \"had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown. On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree -      shiver and quiver, my little tree,      silver and gold throw down over me. And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden.  And when she went to the festival in the dress, no one knew how to speak for astonishment.  The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her.  The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck.  The king's son picked it up, and it was small and dainty, and all golden.  Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits.  Then were the two sisters glad,\", \"and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again.  In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders.  And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them. Beautiful dresses, said one, pearls and jewels, said the second. And you, cinderella, said he, what will you have.  Father break off for me the first branch which knocks against your hat on your way home.  So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat.  Then he broke off the branch and took it with him.  When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush.  Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it.  And it grew and became a handsome tree. Thrice a day cinderella went and sat beneath it, and wept and\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\\n\",context) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72878fc-3dd0-4cb8-b705-4409e155f5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      " [0.7857171297073364, 0.844165027141571, 0.8484514951705933]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores:\\n\",scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abfdde-f884-4c30-921f-f7df1aa0af89",
   "metadata": {},
   "source": [
    "With the evaluate function, you can assess the effectiveness of your most recent response.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a99d0fde-a207-481e-8c86-475dc039192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore scores:\n",
      "   Precision@3: 0.5233\n",
      "   Recall@3: 0.5158\n",
      "   F1@3: 0.5377\n"
     ]
    }
   ],
   "source": [
    "IRA.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683ac73-c4cc-4aa5-888e-2039953ddbe6",
   "metadata": {},
   "source": [
    "Additionally, you can monitor the number of tokens utilized during the storage process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce2c388-91cb-4ad9-b19c-b7d1e4268e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Overview of All Tokens Used:\n",
      "                Input tokens sent to GPT-3.5 Turbo (Model ID: 0125) for summarizing: 3400\n",
      "                Output tokens received from GPT-3.5 Turbo (Model ID: 0125): 100\n",
      "                Tokens used in the embedding section that were sent to the database: 3458\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "IRA.get_tokens_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e46c60-b641-4c01-aaaa-e2eaac2dbf27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
