{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b992ee491b528602",
   "metadata": {},
   "source": [
    "## Indox Retrieval Augmentation\n",
    "Here, we will explore how to work with Indox Retrieval Augmentation. We are using Mistral as LLM model and HuggingFace for our embedding, we should set our HUGGINGFACE_API_KEY and MISTRAL_API_KEY as an environment variable.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxRag/mistral_clusteredSplit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7a71fa49e18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxRag\n",
    "!pip install mistralai\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342aa23beeb948c",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "  python -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "  indox\\Scripts\\activate\n",
    "```\n",
    "\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "   \n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "  source indox/bin/activate\n",
    "```\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "  pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180e33d8226d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424880a731bf8af",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:43:19.981844Z",
     "start_time": "2024-07-24T05:43:19.969220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1f16d899f2423",
   "metadata": {},
   "source": [
    "### Creating an instance of IndoxTetrivalAugmentation\n",
    "\n",
    "To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92018a2d0b19b301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:43:21.115173Z",
     "start_time": "2024-07-24T05:43:20.930482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mIndoxRetrievalAugmentation initialized\u001b[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "from indoxRag import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff10959e7dbf65be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:43:21.905827Z",
     "start_time": "2024-07-24T05:43:21.899330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.15'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indox.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497dae86563ecb3",
   "metadata": {},
   "source": [
    "### Generating response using Mistral's language models \n",
    "MistralQA class is used to handle question-answering task using Mistral's language models from HuggingFace. This instance creates HuggingFaceEmbedding class to specifying embedding model.By using UnstructuredLoadAndSplit function we can import various file types and split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8255e31a4b3e951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:44:34.937195Z",
     "start_time": "2024-07-24T05:44:34.488676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing MistralAI with model: mistral-medium-latest\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mMistralAI initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized MistralEmbedding with model: mistral-embed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxRag.llms import Mistral\n",
    "from indoxRag.embeddings import HuggingFaceEmbedding\n",
    "from indoxRag.data_loader_splitter import ClusteredSplit\n",
    "from indoxRag.embeddings import MistralEmbedding\n",
    "mistral_qa = Mistral(api_key=MISTRAL_API_KEY)\n",
    "# embed_hf = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\")\n",
    "embed_mistral = MistralEmbedding(MISTRAL_API_KEY,model=\"mistral-embed\")\n",
    "file_path = \"sample.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1927daaf1dd2db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:18.671382Z",
     "start_time": "2024-07-24T05:44:36.205057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mClusteredSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing for documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m--Generated 6 clusters--\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m--Generated 1 clusters--\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking & clustering process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loader_splitter = ClusteredSplit(file_path=file_path,summary_model=mistral_qa,embeddings=embed_mistral)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b62989d2708c2",
   "metadata": {},
   "source": [
    " Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3923803005cbc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:20.112580Z",
     "start_time": "2024-07-24T05:45:19.512146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mConnection to the vector store database established successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.chroma.Chroma at 0x27bb81d07a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indoxRag.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55887dfe04c1f8fd",
   "metadata": {},
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554442a48598a32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:33.396418Z",
     "start_time": "2024-07-24T05:45:27.012223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUnstructuredLoadAndSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGetting all documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing title-based chunking\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxRag.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f6d80399dd8ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:36.301219Z",
     "start_time": "2024-07-24T05:45:36.297358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea39b02fcf49dca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:39.914566Z",
     "start_time": "2024-07-24T05:45:36.590646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.chroma.Chroma at 0x27bb81d07a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add(docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8415803dad81e57",
   "metadata": {},
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff68754e1bc960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:42.993478Z",
     "start_time": "2024-07-24T05:45:42.989372Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\"\n",
    "retriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acc665c7968a42",
   "metadata": {},
   "source": [
    "invoke(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information.\n",
    "Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade4b6dc40510ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:53.591676Z",
     "start_time": "2024-07-24T05:45:45.073440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAttempting to generate an answer for the question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Cinderella's happy ending began when her fairy godmother appeared and granted her wishes. With the help of the fairy godmother, Cinderella was able to attend the king's festival and dance with the prince. Despite the stepsisters' attempts to keep her from attending, Cinderella was able to go to the ball with the help of her magical dress, glass slippers, and carriage.\\n\\nAt the ball, the prince fell in love with Cinderella and danced with her all night. However, when the clock struck midnight, Cinderella had to leave in a hurry and accidentally left behind one of her glass slippers. The prince, determined to find the woman he had fallen in love with, searched the entire kingdom for the owner of the glass slipper. When he finally found Cinderella and the slipper fit her foot, he knew he had found his true love.\\n\\nIn the end, Cinderella married the prince and lived happily ever after. Despite the hardships she faced with her evil stepmother and stepsisters, Cinderella was able to overcome them and find her happily ever after with the help of a little bit of magic and a lot of determination.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c0c09a366864a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:59:48.698313Z",
     "start_time": "2024-07-09T15:59:48.692530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by the hearth in the cinders. And as on that account she alwayslooked dusty and dirty, they called her cinderella.It happened that the father was once going to the fair, and heasked his two step-daughters what he should bring back for them.Beautiful dresses, said one, pearls and jewels, said the second.And you, cinderella, said he, what will you have. Father',\n",
       " \"to appear among the number, they were delighted, called cinderellaand said, comb our hair for us, brush our shoes and fasten ourbuckles, for we are going to the wedding at the king's palace.Cinderella obeyed, but wept, because she too would have liked togo with them to the dance, and begged her step-mother to allowher to do so. You go, cinderella, said she, covered in dust and\",\n",
       " 'cinderella expressed a wish, the bird threw down to her what shehad wished for.It happened, however, that the king gave orders for a festivalwhich was to last three days, and to which all the beautiful younggirls in the country were invited, in order that his son might choosehimself a bride. When the two step-sisters heard that they too were',\n",
       " \"danced with her only, and if any one invited her to dance, he saidthis is my partner.When evening came, cinderella wished to leave, and the king'sson was anxious to go with her, but she escaped from him so quicklythat he could not follow her. The king's son, however, hademployed a ruse, and had caused the whole staircase to be smeared\",\n",
       " \"Then the maiden was delighted, and believed that she might now gowith them to the wedding. But the step-mother said, all this willnot help. You cannot go with us, for you have no clothes and cannot dance. We should be ashamed of you. On this she turned herback on cinderella, and hurried away with her two proud daughters.As no one was now at home, cinderella went to her mother's\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb878e173a36425a",
   "metadata": {},
   "source": [
    "### With AgenticRag\n",
    "\n",
    "AgenticRag stands for Agentic Retrieval-Augmented Generation. This concept combines retrieval-based methods and generation-based methods in natural language processing (NLP). The key idea is to enhance the generative capabilities of a language model by incorporating relevant information retrieved from a database or a vector store. \n",
    " AgenticRag is designed to provide more contextually rich and accurate responses by utilizing external knowledge sources. It retrieves relevant pieces of information (chunks) from a vector store based on a query and then uses a language model to generate a comprehensive response that incorporates this retrieved information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f1a9b5c6bf58d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T16:00:40.691245Z",
     "start_time": "2024-07-09T16:00:21.487778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:30:22,358 INFO:HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-07-09 19:30:23,247 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-09 19:30:29,288 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-09 19:30:30,881 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-09 19:30:32,840 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-09 19:30:33,450 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mNo Relevant document found, Start web search\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mNo Relevant Context Found, Start Searching On Web...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswer Base On Web Search\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAttempting to generate an answer for the question\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:30:39,215 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mCheck For Hallucination In Generated Answer Base On Web Search\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:30:39,726 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mHallucination detected, Regenerate the answer...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAttempting to generate an answer for the question\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:30:40,687 INFO:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context information, Lionel Messi currently plays for Inter Miami CF in Major League Soccer (MLS).'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = indox.AgenticRag(llm=mistral_qa,vector_database=db,top_k=5)\n",
    "agent.run(\"where does messi plays right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f8d5c255e240c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
