{
 "cells": [
  {
   "cell_type": "raw",
   "id": "95a4ad1add8df314",
   "metadata": {
    "id": "95a4ad1add8df314"
   },
   "source": [
    "---\n",
    "title: Mistral As Question Answer Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4dc7534ac5f88",
   "metadata": {
    "id": "e8b4dc7534ac5f88"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will demonstrate how to securely handle `inDox` as system for question answering system with open source models which are available on internet like `Mistral`. so firstly you should buil environment variables and API keys in Python using the `dotenv` library. Environment variables are a crucial part of configuring your applications, especially when dealing with sensitive information like API keys.\n",
    "\n",
    "\n",
    "Let's start by importing the required libraries and loading our environment variables.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxRag/mistral_unstructured.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nxmzO5Wei_3U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxmzO5Wei_3U",
    "outputId": "05bd74c8-890c-41ff-9408-d021dd834ba2"
   },
   "outputs": [],
   "source": [
    "!pip install mistralai\n",
    "!pip install indoxRag\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d9e8da377c4b5",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indox`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indox\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indox_judge\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indox\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indox/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "ئ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9b422e9d6063e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:27.250957Z",
     "start_time": "2024-07-12T12:59:27.238294Z"
    },
    "id": "1d9b422e9d6063e4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688318bb5b4dfa7",
   "metadata": {
    "id": "7688318bb5b4dfa7"
   },
   "source": [
    "### Import Essential Libraries\n",
    "Then, we import essential libraries for our `Indox` question answering system:\n",
    "- `IndoxRetrievalAugmentation`: Enhances the retrieval process for better QA performance.\n",
    "- `Mistral`: A powerful QA model from Indox, built on top of the semantic understanding.\n",
    "- `UnstructuredLoadAndSplit`: A utility for loading and splitting unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f40fc686cd072e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:29.786010Z",
     "start_time": "2024-07-12T12:59:29.707179Z"
    },
    "id": "a9f40fc686cd072e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mIndoxRetrievalAugmentation initialized\u001b[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "from indoxRag import IndoxRetrievalAugmentation\n",
    "indox = IndoxRetrievalAugmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449eb2a7ca2e5bce",
   "metadata": {
    "id": "449eb2a7ca2e5bce"
   },
   "source": [
    "### Building the Indox System and Initializing Models\n",
    "\n",
    "Next, we will build our `inDox` system and initialize the Mistral question answering model along with the embedding model. This setup will allow us to leverage the advanced capabilities of Indox for our question answering tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ff6002e2497b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:36.521212Z",
     "start_time": "2024-07-12T12:59:31.185755Z"
    },
    "id": "ac5ff6002e2497b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing MistralAI with model: mistral-medium-latest\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mMistralAI initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized Mistral embeddings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxRag.llms import Mistral\n",
    "from indoxRag.embeddings import MistralEmbedding\n",
    "mistral_qa = Mistral(api_key=MISTRAL_API_KEY)\n",
    "embed_mistral = MistralEmbedding(MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23f48af26265ca",
   "metadata": {
    "id": "fd23f48af26265ca"
   },
   "source": [
    "### Setting Up Reference Directory and File Path\n",
    "\n",
    "To demonstrate the capabilities of our Indox question answering system, we will use a sample directory. This directory will contain our reference data, which we will use for testing and evaluation.\n",
    "\n",
    "First, we specify the path to our sample file. In this case, we are using a file named `sample.txt` located in our working directory. This file will serve as our reference data for the subsequent steps.\n",
    "\n",
    "Let's define the file path for our reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706a7ba1cc8deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38c913b696a2642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:45.881729Z",
     "start_time": "2024-07-12T12:59:45.878322Z"
    },
    "id": "b38c913b696a2642"
   },
   "outputs": [],
   "source": [
    "file_path = \"sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88dd6c433fc600c",
   "metadata": {
    "id": "e88dd6c433fc600c"
   },
   "source": [
    "### Chunking Reference Data with UnstructuredLoadAndSplit\n",
    "\n",
    "To effectively utilize our reference data, we need to process and chunk it into manageable parts. This ensures that our question answering system can efficiently handle and retrieve relevant information.\n",
    "\n",
    "We use the `UnstructuredLoadAndSplit` utility for this task. This tool allows us to load the unstructured data from our specified file and split it into smaller chunks. This process enhances the performance of our retrieval and QA models by making the data more accessible and easier to process.\n",
    "\n",
    "In this step, we define the file path for our reference data and use `UnstructuredLoadAndSplit` to chunk the data with a maximum chunk size of 400 characters.\n",
    "\n",
    "Let's proceed with chunking our reference data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc52c1d0416383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T13:00:10.670787Z",
     "start_time": "2024-07-12T12:59:54.354652Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dcc52c1d0416383",
    "outputId": "c43a25f4-7c29-470c-8f82-6cfbb83be6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUnstructuredLoadAndSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGetting all documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing title-based chunking\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxRag.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "load_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\n",
    "docs = load_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d312cf4791f60f",
   "metadata": {
    "id": "72d312cf4791f60f"
   },
   "source": [
    "### Connecting Embedding Model to Indox\n",
    "\n",
    "With our reference data chunked and ready, the next step is to connect our embedding model to the Indox system. This connection enables the system to leverage the embeddings for better semantic understanding and retrieval performance.\n",
    "\n",
    "\n",
    "Let's connect the embedding model to Indox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc33cc4fb58a305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T13:00:11.364428Z",
     "start_time": "2024-07-12T13:00:10.670787Z"
    },
    "id": "ebc33cc4fb58a305"
   },
   "outputs": [],
   "source": [
    "from indoxRag.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250da1a633bef038",
   "metadata": {
    "id": "250da1a633bef038"
   },
   "source": [
    "### Storing Data in the Vector Store\n",
    "\n",
    "After connecting our embedding model to the Indox system, the next step is to store our chunked reference data in the vector store. This process ensures that our data is indexed and readily available for retrieval during the question-answering process.\n",
    "\n",
    "Let's proceed with storing the data in the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2f51f1a359477",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-12T13:00:11.372085Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83b2f51f1a359477",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "c2d4c310-d550-4e15-9776-24b7c23a7ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db.add(docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766ed35249fef6e",
   "metadata": {
    "id": "7766ed35249fef6e"
   },
   "source": [
    "## Query from RAG System with Indox\n",
    "With our Retrieval-Augmented Generation (RAG) system built using Indox, we are now ready to test it with a sample question. This test will demonstrate how effectively our system can retrieve and generate accurate answers based on the reference data stored in the vector store.\n",
    "\n",
    "We'll use a sample query to test our system:\n",
    "- **Query**: \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "This question will be processed by our Indox system to retrieve relevant information and generate an appropriate response.\n",
    "\n",
    "Let's test our RAG system with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a41f4d7293b39",
   "metadata": {
    "id": "c30a41f4d7293b39",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58639a3d46eb327f",
   "metadata": {
    "id": "58639a3d46eb327f"
   },
   "source": [
    "Now that our Retrieval-Augmented Generation (RAG) system with Indox is fully set up, we can test it with a sample question. We'll use the `invoke` submethod to get a response from the system.\n",
    "\n",
    "\n",
    "The `invoke` method processes the query using the connected QA model and retrieves relevant information from the vector store. It returns a list where:\n",
    "- The first index contains the answer.\n",
    "- The second index contains the contexts and their respective scores.\n",
    "\n",
    "\n",
    "We'll pass this query to the `invoke` method and print the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecb3768f04d326",
   "metadata": {
    "id": "66ecb3768f04d326",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "retriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbffdb7d5427bd",
   "metadata": {
    "id": "9adbffdb7d5427bd",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "answer = retriever.invoke(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f905f84906433aab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "f905f84906433aab",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "da7808e4-2408-4ff6-b185-a97413f08713"
   },
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db289e0dae276aee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db289e0dae276aee",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "cafedfc6-1137-48ae-c095-b0052f393dcf"
   },
   "outputs": [],
   "source": [
    "context = retriever.context\n",
    "context"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
