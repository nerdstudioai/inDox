{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f454fd6293a873f7",
   "metadata": {},
   "source": [
    "\n",
    "## Indox Retrieval Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67508fa389e569f",
   "metadata": {},
   "source": [
    "Here, we will explore how to work with Indox Retrieval Augmentation. First, if you are using OpenAI, you should set your OpenAI key as an environment variable.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/openai_unstructured.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a1f15146dc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install indoxArcg\n",
    "!pip install chromadb\n",
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2eada68410ff3",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxArcg/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e8c38ba3b8886d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:10.365320Z",
     "start_time": "2024-07-12T12:59:10.353525Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac995737f9b2fe6e",
   "metadata": {},
   "source": [
    "### Creating an instance of IndoxRetrivalAugmentation\n",
    "\n",
    "To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759db8f502cbd91f",
   "metadata": {},
   "source": [
    "### Generating response using OpenAI's language models \n",
    "OpenAIQA class is used to handle question-answering task using OpenAI's language models. This instance creates OpenAiEmbedding class to specifying embedding model. Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d98545c6d3c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:16.953419Z",
     "start_time": "2024-07-12T12:59:12.210458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-3.5-turbo-0125\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mOpenAi initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized OpenAI embeddings with model: text-embedding-3-small\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mConnection to the vector store database established successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x249ce6666c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indoxArcg.llms import OpenAi\n",
    "from indoxArcg.embeddings import OpenAiEmbedding\n",
    "\n",
    "openai_qa = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n",
    "embed_openai = OpenAiEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n",
    "from indoxArcg.vector_stores import Chroma\n",
    "\n",
    "db = Chroma(collection_name=\"sample\", embedding_function=embed_openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3408e8f8a8ad17",
   "metadata": {},
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ee3b881d4be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8e56a9f88e03cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:16.957329Z",
     "start_time": "2024-07-12T12:59:16.954425Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"sample.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c44ce67f972c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:37.453098Z",
     "start_time": "2024-07-12T12:59:16.958336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUnstructuredLoadAndSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGetting all documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing title-based chunking\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path, max_chunk_size=400, remove_sword=False)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82af3fb1c9f5643a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:37.458989Z",
     "start_time": "2024-07-12T12:59:37.454106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The wife of a rich man fell sick, and as she felt that her endwas drawing near, she called her only daughter to her bedside andsaid, dear child, be good and pious, and then thegood God will always protect you, and I will look down on youfrom heaven and be near you. Thereupon she closed her eyes anddeparted. Every day the maiden went out to her mother's grave,\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4557891dec337e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:45.534128Z",
     "start_time": "2024-07-12T12:59:37.459999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x249ce6666c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add(docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6bd4924ad116fd",
   "metadata": {},
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593ec3a85c796115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:45.538294Z",
     "start_time": "2024-07-12T12:59:45.534646Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\"\n",
    "from indoxArcg.pipelines.rag import RAG\n",
    "retriever = RAG(llm=openai_qa,vector_store=db,top_k= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e778403c8d864c4",
   "metadata": {},
   "source": [
    "infer(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information.\n",
    "Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a2d55199cf0ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-12T12:59:52.302283Z",
     "start_time": "2024-07-12T12:59:45.539299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating response\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mResponse generated successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cinderella reached her happy ending by being kind, patient, and having a pure heart. Despite facing mistreatment from her step-family, she remained humble and continued to do good deeds. With the help of a little white bird and magical assistance, Cinderella was able to attend the royal festival where the prince fell in love with her. Ultimately, her kindness, inner beauty, and resilience led her to marry the prince and live happily ever after.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.infer(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
