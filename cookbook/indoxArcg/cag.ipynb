{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('E:/Codes/inDox/libs/indoxArcg')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASHKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASHKAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import OpenAi\n",
    "from indoxArcg.embeddings import OpenAiEmbedding\n",
    "from indoxArcg.data_loaders import Txt,DoclingReader\n",
    "from indoxArcg.splitter import RecursiveCharacterTextSplitter,SemanticTextSplitter\n",
    "from indoxArcg.pipelines.cag import CAG, KVCache\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing OpenAi with model: gpt-4o-mini\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mOpenAi initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized OpenAiEmbedding with model: text-embedding-3-small\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAi(model=\"gpt-4o-mini\",api_key=OPENAI_API_KEY)\n",
    "embed_model = OpenAiEmbedding(api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "The wife of a rich man fell sick, and as she felt that her end\n",
      "\n",
      "was drawing near, she called her only daughter to her bedside and\n",
      "\n",
      "said, dear child, be good and pious, and then the\n",
      "\n",
      "good God will always protect you, and I will look down on you\n",
      "\n",
      "from heaven and be near you.  Thereupon she closed her eyes and\n",
      "\n",
      "departed.  Every day the maiden went out to her mother's grave,\n"
     ]
    }
   ],
   "source": [
    "txt_loader = Txt(txt_path=\"sample.txt\")\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "docs = txt_loader.load()\n",
    "split_docs = splitter.split_text(text=docs)\n",
    "print(len(split_docs))\n",
    "print(split_docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:50:03,575 - docling.document_converter - INFO - Going to convert document batch...\n",
      "2025-01-22 18:50:05,382 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-01-22 18:50:07,318 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-01-22 18:50:07,852 - docling.utils.accelerator_utils - INFO - Accelerator device: 'cpu'\n",
      "2025-01-22 18:50:08,230 - docling.pipeline.base_pipeline - INFO - Processing document LLM output verification.pdf\n",
      "2025-01-22 18:50:29,571 - docling.document_converter - INFO - Finished converting document LLM output verification.pdf in 26.03 sec.\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = \"LLM output verification.pdf\"\n",
    "docling_reader = DoclingReader(file_path=pdf_file_path)\n",
    "pdf_doc = docling_reader.load()\n",
    "text_docs = pdf_doc.document.export_to_text()\n",
    "semantic_splitter = SemanticTextSplitter()\n",
    "pdf_doc_split = semantic_splitter.split_text(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zkLLM is a cryptographic framework designed to ensure verifiable execution '\n",
      " 'of large language models (LLMs) using Zero-Knowledge Proofs (ZKPs) . The key '\n",
      " 'idea is that a third-party executor can prove they used the specified model '\n",
      " 'to compute the given input and produce the output without revealing the '\n",
      " 'underlying model parameters or the input data. zkLLM achieves this through '\n",
      " 'efficient protocols tailored for LLM operations, such as transformer '\n",
      " 'attention mechanisms, enabling secure and scalable verification.\\n'\n",
      " '\\n'\n",
      " 'This innovative approach is detailed in the paper, zkLLM: Zero Knowledge '\n",
      " 'Proofs for Large Language Models , which introduces core components like '\n",
      " 'tlookup for non-arithmetic operations and zkAttn for attention mechanisms. '\n",
      " 'The official implementation is available on: '\n",
      " 'https://github.com/jvhs0706/zkllm-ccs2024\\n'\n",
      " '\\n'\n",
      " 'Detailed Explanation of zkLLM\\n'\n",
      " '\\n'\n",
      " 'The zkLLM framework introduces an innovative method for verifiable '\n",
      " 'computations of large language models (LLMs) using Zero-Knowledge Proofs '\n",
      " '(ZKPs) . This solution allows third-party executors to prove that they have '\n",
      " 'correctly executed a specified LLM on a given input without revealing '\n",
      " 'sensitive model parameters or input data.\\n'\n",
      " '\\n'\n",
      " 'How zkLLM Works\\n'\n",
      " '\\n'\n",
      " '1. Model Commitment :\\n'\n",
      " '\\n'\n",
      " ' · The model parameters are hashed into a cryptographic commitment, creating '\n",
      " 'a \"fingerprint\" of the LLM. This ensures that any tampering with the model '\n",
      " 'parameters can be detected.\\n'\n",
      " '\\n'\n",
      " '2. Input Execution :\\n'\n",
      " '\\n'\n",
      " ' · The LLM processes the input query to generate an output. While doing so, '\n",
      " 'the zkLLM framework captures intermediate computations for proof '\n",
      " 'construction.\\n'\n",
      " '\\n'\n",
      " '3. Proof Generation :\\n'\n",
      " '\\n'\n",
      " ' · The computations performed by the LLM (e.g., matrix multiplications, '\n",
      " 'attention mechanisms) are converted into arithmetic circuits , representing '\n",
      " 'the operations as a series of mathematical equations.')\n"
     ]
    }
   ],
   "source": [
    "pprint(pdf_doc_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mPrecomputing KV cache for 38 document chunks...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mKV cache saved: kv_cache\\no_embed_cache.pkl\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPreloaded 38 document chunks into KV cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPrecomputing KV cache for 16 document chunks...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mKV cache saved: kv_cache\\no_embed_cache_pdf.pkl\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPreloaded 16 document chunks into KV cache\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# cag_with_embedding = CAG(\n",
    "#     llm=llm,\n",
    "    \n",
    "#     embedding_model=embed_model,\n",
    "#     cache=KVCache(),\n",
    "# )\n",
    "# cache_embed_key = \"embed_cache\"\n",
    "# cag_with_embedding.preload_documents(split_docs,cache_embed_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cache_no_embed_key = \"no_embed_cache\"\n",
    "cag_without_embedding = CAG(llm=llm, cache=KVCache())\n",
    "cag_without_embedding.preload_documents(split_docs, cache_no_embed_key)\n",
    "\n",
    "\n",
    "cache_no_embed_key_pdf = \"no_embed_cache_pdf\"\n",
    "cag_without_embedding = CAG(llm=llm, cache=KVCache())\n",
    "cag_without_embedding.preload_documents(pdf_doc_split, cache_no_embed_key_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_cinderella = \"How Cinderella reach her happy ending?\"\n",
    "query_pdf = \"how users could earn tokens?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(cag_with_embedding.infer(query,cache_key=cache_embed_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving relevant context...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming inference with filtered context...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:53:59,656 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The context does not provide sufficient information to answer this query.'\n"
     ]
    }
   ],
   "source": [
    "response_tfidf = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key_pdf,similarity_search_type=\"tfidf\",similarity_threshold=0.3)\n",
    "pprint(response_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving relevant context...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 5 relevant chunks from cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming inference with filtered context...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:43:29,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Users can earn tokens by participating in the verification process of '\n",
      " 'computations performed by third-party executors. The earning process '\n",
      " 'includes:\\n'\n",
      " '\\n'\n",
      " '- **Computation Tasks**: Users can submit computation tasks where they run a '\n",
      " 'large language model (LLM) and submit the result. If the result is verified, '\n",
      " 'they receive the token as a reward.\\n'\n",
      " '- **Computation Rewards**: Users will earn tokens by successfully completing '\n",
      " 'computations. The reward structure will be tiered based on the complexity of '\n",
      " 'tasks completed, with higher rewards for more challenging verifications.\\n'\n",
      " '- **Staking Rewards**: Users can stake their tokens to support network '\n",
      " 'security and operations. In return, they will receive additional tokens as '\n",
      " 'staking rewards, promoting long-term holding and reducing circulating '\n",
      " 'supply.')\n"
     ]
    }
   ],
   "source": [
    "response_bm25 = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"bm25\",similarity_threshold=0.7)\n",
    "pprint(response_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving relevant context...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming inference with filtered context...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:54:56,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The context does not provide sufficient information to answer this query.'\n"
     ]
    }
   ],
   "source": [
    "response_jaccard = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"jaccard\",similarity_threshold=0.1)\n",
    "pprint(response_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving relevant context...\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing smart retrieval\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSelected 0 relevant chunks from cache\u001b[0m\n",
      "\u001b[32mWARNING\u001b[0m: \u001b[33m\u001b[1mNo relevant context found in cache\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming web search for additional context\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:43,705 - primp - INFO - response: https://html.duckduckgo.com/html 200 26971\n",
      "2025-01-22 18:55:46,083 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRelevant doc\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:46,986 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRelevant doc\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:48,049 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRelevant doc\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:48,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRelevant doc\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:49,872 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRelevant doc\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mPerforming inference with filtered context...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 18:55:51,932 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Users can earn tokens by using the Grass platform, as the more they engage '\n",
      " 'with it, the more tokens they will accumulate. Additionally, users can earn '\n",
      " 'tokens by participating in Play-to-Earn (P2E) games, such as Tamadoge, where '\n",
      " 'players raise and nurture digital pets to earn TAMA tokens. Furthermore, '\n",
      " 'users may also earn tokens through delegating tokens to validators, who can '\n",
      " 'then generate rewards based on the tokens delegated to them.')\n"
     ]
    }
   ],
   "source": [
    "response_jaccard = cag_without_embedding.infer(query_pdf,cache_key=cache_no_embed_key,similarity_search_type=\"jaccard\",similarity_threshold=0.1,smart_retrieval=True)\n",
    "pprint(response_jaccard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
