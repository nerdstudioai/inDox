{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c416e33efe74032",
   "metadata": {},
   "source": [
    "# LanternDB\n",
    "In this notebook, we will demonstrate how to use LanternDB, for accessing and querying data efficiently. LanternDB is designed to work seamlessly with modern analytical workloads, making it a powerful tool for data analysis, research, and question-answering systems.\n",
    "\n",
    "To begin, ensure you have LanternDB installed in your Python environment. You can easily install it using `pip install pgcopg2`. so you can start querying data directly in your local environment without any additional setup.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/lanterndb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff860b3e1246531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxArcg\n",
    "!pip install psycopg2\n",
    "!pip install semantic_text_splitter\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddd9e06704e8c2",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxArcg/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f20d8186160f8",
   "metadata": {},
   "source": [
    "### Load Hugging face API key \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0ba9fc233722f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ceeb43c88eb0f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:19:56.966480Z",
     "start_time": "2024-09-08T14:19:56.956886Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('api.env')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b6054d57943f3",
   "metadata": {},
   "source": [
    "Initialize a language model and an embedding model using the indox library with Hugging Face . The HuggingFaceModel class is used to create an instance of the Mistral-7B-Instruct model for tasks like question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b122b67b62a902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:06.428643Z",
     "start_time": "2024-09-08T14:20:00.047537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing HuggingFaceModel with model: mistralai/Mistral-7B-Instruct-v0.2\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mHuggingFaceModel initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized HuggingFaceEmbedding with model: multi-qa-mpnet-base-cos-v1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import HuggingFaceModel\n",
    "from indoxArcg.embeddings import HuggingFaceEmbedding\n",
    "mistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "embed = HuggingFaceEmbedding(api_key=HUGGINGFACE_API_KEY,model=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2146b8c745697",
   "metadata": {},
   "source": [
    "### Load sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae66a77bd283acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772e5448163c94bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:06.436738Z",
     "start_time": "2024-09-08T14:20:06.434307Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = 'sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f59422f7e45412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:06.491601Z",
     "start_time": "2024-09-08T14:20:06.485675Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc61c8d226e886",
   "metadata": {},
   "source": [
    "use the `SemanticTextSplitter` class from the indox library to divide a large text into smaller, manageable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa6973fcdc5ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:08.447082Z",
     "start_time": "2024-09-08T14:20:07.980599Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxArcg.splitter import SemanticTextSplitter\n",
    "splitter = SemanticTextSplitter(400)\n",
    "content_chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11514c4c72eae13c",
   "metadata": {},
   "source": [
    "### Set up vector store\n",
    "Set up a vector store using the `LanternDB` class from the indox library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94486559ba9879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:14.647411Z",
     "start_time": "2024-09-08T14:20:10.575590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LanternDB collection 'Vector_collection'\n",
      "Collection 'Vector_collection' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.vector_stores import LanternDB\n",
    "connection_params = {\n",
    "    \"dbname\": \"DatabaseName\",\n",
    "    \"user\": \"User\",\n",
    "    \"password\": \"password\",\n",
    "    \"host\": \"host\",\n",
    "    \"port\": \"port\"\n",
    "}\n",
    "lantern_db = LanternDB(\n",
    "    collection_name=\"Vector_collection\",\n",
    "    embedding_function=embed,\n",
    "    connection_params=connection_params,\n",
    "    dimension=768,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404beeb91b8186",
   "metadata": {},
   "source": [
    "### Storing Data in the Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abf7bdbe94e4580c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:22.537287Z",
     "start_time": "2024-09-08T14:20:17.056571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 9 documents into collection 'Vector_collection'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['852d6289-a105-48f3-94e8-4169f109abb7',\n",
       " '9de00e1c-736f-41f7-a4cf-2337bb62d5d7',\n",
       " 'f00c66ba-ba5e-4928-b7bc-980688a7efd2',\n",
       " '21c94f7b-ee16-4c49-b914-da0c2e5247c4',\n",
       " '09323629-9f81-4236-9619-c84e025f3b8f',\n",
       " 'b88cdb3a-79dd-48bf-b3d3-f44d7ee258be',\n",
       " '126dd7fb-4dba-4189-bfaa-c220fd69570e',\n",
       " 'c223a9a1-232e-493b-aa90-129a21d05d19',\n",
       " 'ebbcb9cc-c00e-4036-a942-28fb1f079d6e']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lantern_db.add_texts(content_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da94506679714392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:22.692858Z",
     "start_time": "2024-09-08T14:20:22.689696Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1507c295310beb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:27.145267Z",
     "start_time": "2024-09-08T14:20:23.356157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 54.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSending request to Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mReceived successful response from Hugging Face API\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.pipelines.rag import RAG\n",
    "retriever = RAG(llm=mistral_qa,vector_store=lantern_db,enable_web_fallback=False,top_k= 5)\n",
    "answer = retriever.infer(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19abf4792769c9fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T14:20:27.562959Z",
     "start_time": "2024-09-08T14:20:27.557418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Cinderella reached her happy ending by attending the royal ball in a beautiful dress and golden slippers that were magically provided to her by a little white bird that lived on a tree she had once wept and prayed under. The king's son fell in love with her and was determined to marry her, so he searched for the woman whose foot the golden slipper fit. When the two stepsisters attempted to wear the slipper, it didn't fit because Cinders\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
