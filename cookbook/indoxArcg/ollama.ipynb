{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0981252e4322d58",
   "metadata": {},
   "source": [
    "## Retrieval Augmentation Using Ollama\n",
    "Here, we will explore how to work with Indox Retrieval Augmentation with Ollama. Actually Ollama is an open-source project that running LLMs on your local nachine. Ollama provides access to a diverse and continuously expanding library of pre-trained LLM models.\n",
    "\n",
    "### Effortless Installation and Setup\n",
    "One of Ollama’s standout features is its user-friendly installation process. Whether you’re a Windows, macOS, or Linux user, Ollama offers intuitive\n",
    " installation methods tailored to your operating system, ensuring a smooth and hassle-free setup experience.\n",
    " \n",
    "### How to Download Ollama\n",
    "You need to download Ollama, head on to the official website of [Ollama](https://ollama.com/) and hit the download button.Ollama supports 3 different operating systems.\n",
    "\n",
    "### How to Run Ollama\n",
    "you can download the intended model using the following cammand:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257d031d72d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama run llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b95e88ee1d3d7",
   "metadata": {},
   "source": [
    "## Now lets run Indox\n",
    "If you haven't install required packages, please download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ae34d97113c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "!pip install mistralai\n",
    "!pip install chromadb\n",
    "!pip install indoxArcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa43994945a017",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "python -m venv indoxArcg\n",
    "```\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "    ```bash\n",
    "   source indoxArcg/bin/activate\n",
    "```\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc3e1ed0e324d6",
   "metadata": {},
   "source": [
    " we should set our MISTRAL_API_KEY as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:17:34.365253Z",
     "start_time": "2024-07-10T18:17:34.355252Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a339acc4254a8b",
   "metadata": {},
   "source": [
    "### Import Essential Libraries\n",
    "Then, we import essential libraries for our `indoxArcg` question answering system:\n",
    "- `Ollama`: Any intended model from Ollama.\n",
    "- `MistralEmbedding`: Utilizes Mistral embeddings for improved semantic understanding.\n",
    "- `SimpleLoadAndSplit`: A utility for loading and splitting PDF files.\n",
    "- `ChromaVectorStore`: Using ChromaVectorStore to store our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fed528e7a718f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:17:46.782342Z",
     "start_time": "2024-07-10T18:17:37.093763Z"
    }
   },
   "outputs": [],
   "source": [
    "from indoxArcg.data_loader_splitter import SimpleLoadAndSplit\n",
    "from indoxArcg.llms import Ollama\n",
    "from indoxArcg.vector_stores import Chroma\n",
    "from indoxArcg.embeddings import MistralEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc2ce70ef8291f",
   "metadata": {},
   "source": [
    "### Building the Indox System and Initializing Models\n",
    "\n",
    "Next, we will build our `indoxArcg` system and initialize the Ollama model which we have already downloaded,  along with the embedding model. This setup will allow us to leverage the advanced capabilities of Indox for our tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5882489fb5469db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:17:46.788015Z",
     "start_time": "2024-07-10T18:17:46.783392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mIndoxRetrievalAugmentation initialized\u001b[0m\n",
      "\n",
      "            ██  ███    ██  ██████   ██████  ██       ██\n",
      "            ██  ████   ██  ██   ██ ██    ██   ██  ██\n",
      "            ██  ██ ██  ██  ██   ██ ██    ██     ██\n",
      "            ██  ██  ██ ██  ██   ██ ██    ██   ██   ██\n",
      "            ██  ██  █████  ██████   ██████  ██       ██\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "llm_model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4594f26e7046af2",
   "metadata": {},
   "source": [
    "### Connecting Embedding Model to Indox\n",
    "\n",
    "With our reference data chunked and ready, the next step is to connect our embedding model to the Indox system. This connection enables the system to leverage the embeddings for better semantic understanding and retrieval performance.\n",
    "\n",
    "We use the `connect_to_vectorstore` method to link the `HuggingFaceEmbedding` model with our Indox system. By specifying the embeddings and a collection name, we ensure that our reference data is appropriately indexed and stored, facilitating efficient retrieval during the question-answering process.\n",
    "\n",
    "Let's connect the embedding model to Indox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fd39eaaa8f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mistral = MistralEmbedding(MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323c0ae0d62b467",
   "metadata": {},
   "source": [
    "### Setting Up Reference Directory and File Path\n",
    "\n",
    "To demonstrate the capabilities of our Indox question answering system, we will use a sample directory. This directory will contain our reference data, which we will use for testing and evaluation.\n",
    "\n",
    "First, we specify the path to our sample file. In this case, we are using a file named `sample.txt` located in our working directory. This file will serve as our reference data for the subsequent steps.\n",
    "\n",
    "Let's define the file path for our reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb98959fb81e1b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:17:56.050765Z",
     "start_time": "2024-07-10T18:17:54.864608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUnstructuredLoadAndSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCreated initial document elements\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n",
    "file_path = \"Demo/sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80640d8be9a688fd",
   "metadata": {},
   "source": [
    "### Chunking Reference Data with SimpleLoadAndSplit\n",
    "\n",
    "To effectively utilize our reference data, we need to process and chunk it into manageable parts. This ensures that our question answering system can efficiently handle and retrieve relevant information.\n",
    "\n",
    "We use the `SimpleLoadAndSplit` utility for this task. This tool allows us to load the PDF files and split it into smaller chunks. This process enhances the performance of our retrieval and QA models by making the data more accessible and easier to process. We are using 'bert-base-uncased' model for splitting data.\n",
    "\n",
    "In this step, we define the file path for our reference data and use `SimpleLoadAndSplit` to chunk the data with a maximum chunk size of 200 characters. Also we can handle to remove stop words or not by initializing `remove-sword` parameter.\n",
    "\n",
    "Let's proceed with chunking our reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ba3b12a6b4bcbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:18:08.095992Z",
     "start_time": "2024-07-10T18:18:08.091370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing Ollama with model: llama3\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mOllama initialized successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loader_splitter = SimpleLoadAndSplit(file_path=file_path)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac95f8485fd64a5",
   "metadata": {},
   "source": [
    "### Storing Data in the Vector Store\n",
    "\n",
    "After connecting our embedding model to the Indox system, the next step is to store our chunked reference data in the vector store. This process ensures that our data is indexed and readily available for retrieval during the question-answering process.\n",
    "\n",
    "We use the `store_in_vectorstore` method to store the processed data in the vector store. By doing this, we enhance the system's ability to quickly access and retrieve relevant information based on the embeddings generated earlier.\n",
    "\n",
    "Let's proceed with storing the data in the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ed207f88927896b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:18:18.912304Z",
     "start_time": "2024-07-10T18:18:16.726513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized Mistral embeddings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(collection_name=\"sample\", embedding_function=embed_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e945c2bc3d348a57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:18:22.667383Z",
     "start_time": "2024-07-10T18:18:22.659873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mConnection to the vector store database established successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.Chroma.ChromaVectorStore at 0x2528c616300>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add(docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdb398b438c802",
   "metadata": {},
   "source": [
    "## Query from RAG System with Indox\n",
    "With our Retrieval-Augmented Generation (RAG) system built using Indox, we are now ready to test it with a sample question. This test will demonstrate how effectively our system can retrieve and generate accurate answers based on the reference data stored in the vector store.\n",
    "\n",
    "We'll use a sample query to test our system:\n",
    "- **Query**: \"How did Cinderella reach her happy ending?\"\n",
    "\n",
    "This question will be processed by our Indox system to retrieve relevant information and generate an appropriate response.\n",
    "\n",
    "Let's test our RAG system with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2484cfb4314bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:18:49.559773Z",
     "start_time": "2024-07-10T18:18:49.556257Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7130213f529abd9",
   "metadata": {},
   "source": [
    "Now that our Retrieval-Augmented Generation (RAG) system with Indox is fully set up, we can test it with a sample question. We'll use the `infer` submethod to get a response from the system.\n",
    "\n",
    "\n",
    "The `infer` method processes the query using the connected QA model and retrieves relevant information from the vector store. It returns a list where:\n",
    "- The first index contains the answer.\n",
    "- The second index contains the contexts and their respective scores.\n",
    "\n",
    "\n",
    "We'll pass this query to the `infer` method and print the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2efd8d91526c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:21:41.286921Z",
     "start_time": "2024-07-10T18:19:00.123359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mRetrieving context and scores from the vector database\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating answer without document relevancy filter\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mAnswering question\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating response\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mResponse generated successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mQuery answered successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.pipelines.rag import RAG\n",
    "retriever = RAG(llm=llm_model,vector_store=db,top_k= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f2002910cb744b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T18:22:03.681929Z",
     "start_time": "2024-07-10T18:22:03.677384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the given context, here are the options:A) Cinderella's fairy godmother helped her get ready for the ball.B) Cinderella's kindness and hard work earned her a magical dress from the bird on the hazel-tree.C) Cinderella's step-sisters and mother helped her get ready for the wedding.D) Cinderella used her magic to transform herself into a beautiful princess.And the best full answer is:B) Cinderella's kindness and hard work earned her a magical dress from the bird on the hazel-tree.This option stands out as the most correct because it highlights Cinderella's humble nature and her good deeds, which ultimately led to her receiving a magical dress that transformed her into a beautiful princess.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.infer(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
