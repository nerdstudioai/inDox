```yaml
# Jupyter Notebook Metadata (Optional)
# {
#   "nbformat": 4,
#   "nbformat_minor": 5,
#   "metadata": {
#       "kernelspec": {
#           "name": "python3",
#           "display_name": "Python 3"
#       }
#   }
# }
```

```markdown
# Demo: Using `LocalHFModel` with the indoxArcg Pipeline

This notebook demonstrates how to use the **local 4-bit quantized Hugging Face model** (`LocalHFModel`) in place of a remote Hugging Face Inference API call.

We will:
1. Import our `LocalHFModel`.
2. Set up an embedding model (for retrieval) with `HuggingFaceEmbedding`.
3. Load and split a text file using `Txt` and `RecursiveCharacterTextSplitter`.
4. Build a pipeline with `CAG` and `KVCache`.
5. Ask a question about **p53** protein to see how the local model responds.
```

```python
# install bitsandbytes & other dependencies if not installed
# Uncomment and run this cell if you haven't installed them yet.
# !pip install bitsandbytes accelerate transformers

import sys
import torch
from pprint import pprint

# Imports from indoxArcg or your local package
from indoxArcg.embeddings import HuggingFaceEmbedding
from indoxArcg.data_loaders import Txt
from indoxArcg.splitter import RecursiveCharacterTextSplitter
from indoxArcg.pipelines.cag import CAG, KVCache

# Import your local HF model class
# If it's in your package, adjust the path accordingly:
#   from indoxArcg.llms.local_hf_model import LocalHFModel
#
# For demonstration, let's pretend we have:
from local_hf_model import LocalHFModel  # <- replace with your real import path

# If you need an API key for embeddings, define it (or set in env variables)
HF_API_KEY = "YOUR_HUGGINGFACE_API_KEY"  # if required for embeddings
```

```python
# 1. Initialize the LOCAL Hugging Face model in 4-bit quantized mode
local_model = LocalHFModel(
    model_id="BioMistral/BioMistral-7B"  
    # You can also use another 4-bit compatible model, e.g.:
    # model_id="meta-llama/Llama-2-7b-chat-hf"
)
```

```python
# 2. Set up an embedding model for retrieval
embedding_model = HuggingFaceEmbedding(
    api_key=HF_API_KEY,  # or remove if not required
    model="multi-qa-mpnet-base-cos-v1"
    # or any other embedding model you prefer
)
```

```python
# 3. Load your text data from a .txt file
#    Make sure sample.txt exists or provide the correct path.
data_loader = Txt(txt_path="sample.txt")
docs = data_loader.load()

print("Loaded text:\n", docs[:200], "...")
```

```python
# 4. Split the text into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_text(text=docs)

print("Number of chunks:", len(split_docs))
print("First chunk preview:\n", split_docs[0])
```

```python
# 5. Create a CAG pipeline with embeddings
cag_with_embedding = CAG(
    llm=local_model,
    embedding_model=embedding_model,
    cache=KVCache(),
)
```

```python
# 6. Preload documents (to embed them and store in the cache)
cache_embed_key = "embed_cache"
cag_with_embedding.preload_documents(split_docs, cache_embed_key)
```

```python
# 7. Optionally, create a second pipeline if you want no embeddings
cag_without_embedding = CAG(llm=local_model, cache=KVCache())
cache_no_embed_key = "no_embed_cache"
cag_without_embedding.preload_documents(split_docs, cache_no_embed_key)
```

```python
# 8. Run a test query through the pipeline with embeddings
query = "What are the functions of the p53 protein in cellular processes?"
print(f"\nQuery: {query}")

answer_with_emb = cag_with_embedding.infer(query, cache_key=cache_embed_key)
print("\nAnswer:")
pprint(answer_with_emb)
```

```markdown
## Conclusion

Youâ€™ve now used the local 4-bit quantized Hugging Face model with the `indoxArcg` pipeline to retrieve context from `sample.txt` and generate an answer about the p53 protein.

If you have any issues or want to experiment with different models, simply change the `model_id` in the `LocalHFModel` constructor to a different 4-bit quantizable model on Hugging Face.
```