{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b992ee491b528602",
   "metadata": {},
   "source": [
    "## Indox Retrieval Augmentation\n",
    "Here, we will explore how to work with Indox Retrieval Augmentation. We are using Mistral as LLM model and HuggingFace for our embedding, we should set our HUGGINGFACE_API_KEY and MISTRAL_API_KEY as an environment variable.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/osllmai/inDox/blob/master/cookbook/indoxArcg/mistral_clusteredSplit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7a71fa49e18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install indoxArcg\n",
    "!pip install mistralai\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342aa23beeb948c",
   "metadata": {},
   "source": [
    "## Setting Up the Python Environment\n",
    "\n",
    "If you are running this project in your local IDE, please create a Python environment to ensure all dependencies are correctly managed. You can follow the steps below to set up a virtual environment named `indoxArcg`:\n",
    "\n",
    "### Windows\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "```bash\n",
    "  python -m venv indoxArcg\n",
    "```\n",
    "\n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "  indoxArcg\\Scripts\\activate\n",
    "```\n",
    "\n",
    "\n",
    "### macOS/Linux\n",
    "\n",
    "1. **Create the virtual environment:**\n",
    "   ```bash\n",
    "   python3 -m venv indoxArcg\n",
    "   \n",
    "2. **Activate the virtual environment:**\n",
    "```bash\n",
    "  source indoxArcg/bin/activate\n",
    "```\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "Once the virtual environment is activated, install the required dependencies by running:\n",
    "\n",
    "```bash\n",
    "  pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180e33d8226d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:43:19.981844Z",
     "start_time": "2024-07-24T05:43:19.969220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c1f16d899f2423",
   "metadata": {},
   "source": [
    "### Creating an instance of IndoxTetrivalAugmentation\n",
    "\n",
    "To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497dae86563ecb3",
   "metadata": {},
   "source": [
    "### Generating response using Mistral's language models \n",
    "MistralQA class is used to handle question-answering task using Mistral's language models from HuggingFace. This instance creates HuggingFaceEmbedding class to specifying embedding model.By using UnstructuredLoadAndSplit function we can import various file types and split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8255e31a4b3e951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:44:34.937195Z",
     "start_time": "2024-07-24T05:44:34.488676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitializing MistralAI with model: mistral-medium-latest\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mMistralAI initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mInitialized MistralEmbedding with model: mistral-embed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.llms import Mistral\n",
    "from indoxArcg.data_loader_splitter import ClusteredSplit\n",
    "from indoxArcg.embeddings import MistralEmbedding\n",
    "mistral_qa = Mistral(api_key=MISTRAL_API_KEY)\n",
    "embed_mistral = MistralEmbedding(MISTRAL_API_KEY,model=\"mistral-embed\")\n",
    "file_path = \"sample.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1927daaf1dd2db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:18.671382Z",
     "start_time": "2024-07-24T05:44:36.205057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mClusteredSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing for documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m--Generated 6 clusters--\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1m--Generated 1 clusters--\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGenerating summary for documentation\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking & clustering process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loader_splitter = ClusteredSplit(file_path=file_path,summary_model=mistral_qa,embeddings=embed_mistral)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b62989d2708c2",
   "metadata": {},
   "source": [
    " Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3923803005cbc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:20.112580Z",
     "start_time": "2024-07-24T05:45:19.512146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mConnection to the vector store database established successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.chroma.Chroma at 0x27bb81d07a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indoxArcg.vector_stores import Chroma\n",
    "db = Chroma(collection_name=\"sample\",embedding_function=embed_mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55887dfe04c1f8fd",
   "metadata": {},
   "source": [
    "### load and preprocess data\n",
    "This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554442a48598a32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:33.396418Z",
     "start_time": "2024-07-24T05:45:27.012223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mUnstructuredLoadAndSplit initialized successfully\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mGetting all documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting processing\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mUsing title-based chunking\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mCompleted chunking process\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mSuccessfully obtained all documents\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from indoxArcg.data_loader_splitter import UnstructuredLoadAndSplit\n",
    "loader_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\n",
    "docs = loader_splitter.load_and_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f6d80399dd8ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:36.301219Z",
     "start_time": "2024-07-24T05:45:36.297358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea39b02fcf49dca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:39.914566Z",
     "start_time": "2024-07-24T05:45:36.590646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m: \u001b[1mStoring documents in the vector store\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mEmbedding documents\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mStarting to fetch embeddings for texts using engine: mistral-embed\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocument added successfully to the vector store.\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m: \u001b[1mDocuments stored successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<indox.vector_stores.chroma.Chroma at 0x27bb81d07a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add(docs=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8415803dad81e57",
   "metadata": {},
   "source": [
    "### Retrieve relevant information and generate an answer\n",
    "The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff68754e1bc960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T05:45:42.993478Z",
     "start_time": "2024-07-24T05:45:42.989372Z"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How cinderella reach her happy ending?\"\n",
    "from indoxArcg.pipelines.rag import RAG\n",
    "retriever = RAG(llm=mistral_qa,vector_store=db,top_k= 5)\n",
    "answer = retriever.infer(query)\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
