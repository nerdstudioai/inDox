{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OSLLM.ai Documentation","text":"<p>Welcome to the official documentation for the InDox project! This platform serves as a comprehensive guide to understanding and leveraging the powerful tools within the Indox ecosystem.</p>"},{"location":"#what_is_indox","title":"What is InDox?","text":"<p>InDox is an advanced Retrieval-Augmented Generation (RAG) system, designed to combine the capabilities of retrieval systems with generative AI to deliver context-aware, intelligent data handling solutions. It is the backbone of the OSLLM.ai project and provides the foundation for seamless data retrieval, storage, and generation workflows.</p>"},{"location":"#indox_ecosystem_overview","title":"Indox Ecosystem Overview","text":"<p>The Indox ecosystem is divided into four main components, each offering specialized functionalities. Click on any component below to explore its documentation in detail:</p>"},{"location":"#indox","title":"Indox","text":"<p>Indox serves as the central RAG system of the ecosystem. It is designed to streamline data handling and enable intelligent workflows through advanced retrieval and generation features.</p>"},{"location":"#key_features","title":"Key Features:","text":"<ul> <li>Retrieval-Augmented Generation (RAG): Combine retrieval and generation for enhanced data insights.</li> <li>Data Connectors: Seamlessly connect to diverse data sources like Discord, GitHub, Google Drive, YouTube, and more.</li> <li>Data Loaders: Import and handle data in various formats, such as PDFs, Excel sheets, and plain text files.</li> <li>Splitters: Efficiently split large datasets into manageable chunks for processing.</li> <li>Vectorstores: Store vectorized data using integrations with Milvus, Pinecone, MongoDB, and more.</li> </ul> <p>Learn more about Indox \u2192</p>"},{"location":"#indoxjudge","title":"IndoxJudge","text":"<p>IndoxJudge focuses on evaluating data and machine learning models using robust metrics and pipelines. It provides a suite of tools to assess the reliability, fairness, and safety of AI systems.</p>"},{"location":"#key_features_1","title":"Key Features:","text":"<ul> <li>Metrics: Evaluate critical aspects like relevancy, faithfulness, toxicity, and adversarial robustness.</li> <li>Pipelines: Automate evaluation workflows for LLMs, fairness analysis, and safety assessments.</li> </ul> <p>Learn more about IndoxJudge \u2192</p>"},{"location":"#indoxgen","title":"IndoxGen","text":"<p>IndoxGen specializes in generating synthetic data and enabling creative workflows through advanced generative models. It supports tasks such as data synthesis for training and analysis.</p>"},{"location":"#key_features_2","title":"Key Features:","text":"<ul> <li>Data Synthesis: Generate synthetic data using GANs and hybrid GAN-LLM models.</li> <li>Prompt-Based Data Generation: Leverage prompts for customized data creation.</li> <li>Few-Shot Learning: Enable data generation with minimal examples.</li> </ul> <p>Learn more about IndoxGen \u2192</p>"},{"location":"#indoxminer","title":"IndoxMiner","text":"<p>IndoxMiner focuses on extracting and organizing structured data from various sources. It enhances data workflows with schema detection and integration with advanced language models.</p>"},{"location":"#key_features_3","title":"Key Features:","text":"<ul> <li>Automatic Schema Detection: Identify and structure data schemas automatically.</li> <li>Data Extraction: Extract structured data from unstructured formats, including images and scanned documents.</li> <li>Predefined Schema Support: Work with predefined schemas to streamline workflows.</li> <li>LLM Integration: Enhance data extraction with context-aware capabilities powered by large language models.</li> </ul> <p>Learn more about IndoxMiner \u2192</p>"},{"location":"#getting_started","title":"Getting Started","text":"<p>If you're new to InDox, start with our Quick Start Guide to set up your environment and explore the key features of the ecosystem.</p>"},{"location":"#feedback_and_contributions","title":"Feedback and Contributions","text":"<p>We value your feedback and input! If you have any suggestions, questions, or would like to contribute, feel free to reach out or submit an issue on our Git repository.</p> <p>Thank you for choosing InDox, your trusted system for intelligent data solutions!</p>"},{"location":"license/","title":"License","text":"<pre><code>                GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.</p> <p>A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.</p> <p>The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.</p> <p>An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU Affero General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Remote Network Interaction; Use with the GNU General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.</p> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\n</code></pre> <p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a \"Source\" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.</p>"},{"location":"Indox/","title":"Indox Retrieval Augmentation","text":"<p>Indox Retrieval Augmentation is an innovative application designed to streamline information extraction from a wide range of document types, including text files, PDF, HTML, Markdown, and LaTeX. Whether structured or unstructured, Indox provides users with a powerful toolset to efficiently extract relevant data.</p> <p>Indox Retrieval Augmentation is an innovative application designed to streamline information extraction from a wide range of document types, including text files, PDF, HTML, Markdown, and LaTeX. Whether structured or unstructured, Indox provides users with a powerful toolset to efficiently extract relevant data. One of its key features is the ability to intelligently cluster primary chunks to form more robust groupings, enhancing the quality and relevance of the extracted information. With a focus on adaptability and user-centric design, Indox aims to deliver future-ready functionality with more features planned for upcoming releases. Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs.</p>"},{"location":"Indox/#join_us","title":"Join Us","text":"<p>Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs.</p>"},{"location":"Indox/agenticRag/","title":"Agentic Rag","text":""},{"location":"Indox/agenticRag/#agenticrag","title":"AgenticRag","text":""},{"location":"Indox/agenticRag/#overview","title":"Overview","text":"<p>The <code>AgenticRag</code> class is designed to handle the interaction between a Language Learning Model (LLM) and a vector database. It facilitates the process of retrieving relevant information from the vector database to answer questions posed to the LLM. This class maintains a history of question-answer interactions and context, ensuring efficient and accurate responses.</p>"},{"location":"Indox/agenticRag/#how_it_works","title":"How It Works","text":"<ol> <li>Context Grading: The class first grades each context and filters them based on relevancy.</li> <li>Web Search Fallback: If no relevant context is found, it performs a web search and generates an answer based on the search results.</li> <li>Answer Generation from Context: If relevant contexts are found, it filters these contexts and attempts to generate an answer based on the filtered contexts.</li> <li>Hallucination Check: After generating the answer, it checks for hallucination (i.e., ensuring the answer is plausible and accurate).</li> <li>Retry on Hallucination: If the answer is found to be hallucinated, it attempts to generate the answer again.</li> <li>Return Answer: If the answer is not hallucinated, it returns the answer.</li> </ol>"},{"location":"Indox/agenticRag/#class_definition","title":"Class Definition","text":"<pre><code>class AgenticRag:\n    def __init__(self, llm, vector_database, top_k: int = 5)\n</code></pre>"},{"location":"Indox/agenticRag/#hyperparameters","title":"Hyperparameters","text":"<ul> <li> <p>llm: The Language Learning Model used for generating responses. This could be any model that processes natural language input and produces relevant outputs.</p> </li> <li> <p>vector_database: The database that stores vectors representing pieces of information. It is used to retrieve the most relevant information based on the input queries.</p> </li> <li> <p>top_k: (int, default=5) The number of top relevant vectors to retrieve from the vector database for each query. This determines how many pieces of information are considered for generating the response.</p> </li> </ul>"},{"location":"Indox/agenticRag/#usage","title":"Usage","text":"<pre><code># Create an instance of the AgenticRag class\nagent = Indox.AgenticRag(llm=llm, vector_database=db, top_k=5)\n\n# Run the agent with a query\nanswer = agent.run(query)\n</code></pre>"},{"location":"Indox/embedding_models/","title":"Embedding Models","text":"<p>Indox currently supports various embedding models. Below is a list of supported models, along with instructions on how to use each one:</p> <ol> <li>OpenAI Embedding Model</li> <li>Hugging Face Embedding Models</li> <li>IndoxApi Embedding Models</li> <li>Mistral Embedding Models</li> <li>Clarifai Embedding Models</li> <li>Cohere Embedding Models</li> <li>Elasticsearch Embedding Models</li> <li>GPT4All Embedding Models</li> <li>Ollama Embedding Models</li> </ol>"},{"location":"Indox/embedding_models/#using_openai_embedding_model","title":"Using OpenAI Embedding Model","text":"<p>To use the OpenAI embedding model, follow these steps:</p> <ol> <li>Install the OpenAI Python package:</li> </ol> <pre><code>pip install openai\n</code></pre> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <ol> <li>Import Indox modules and set the OpenAI embedding model:</li> </ol> <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.embeddings import OpenAiEmbedding\n\nIndox = IndoxRetrievalAugmentation()\nopenai_embeddings = OpenAiEmbedding(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n</code></pre>"},{"location":"Indox/embedding_models/#using_indoxapi_embedding_model","title":"Using IndoxApi Embedding Model","text":"<p>To use the IndoxApi embedding model, follow these steps:</p> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nINDOX_API_KEY = os.environ['INDOX_API_KEY']\n</code></pre> <ol> <li>Import Indox modules and set the IndoxApi embedding model:</li> </ol> <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.embeddings import IndoxApiEmbedding\n\nIndox = IndoxRetrievalAugmentation()\nembed_openai_indox = IndoxApiEmbedding(api_key=INDOX_API_KEY, model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_mistral_embedding_model","title":"Using Mistral Embedding Model","text":"<p>To use the Mistral embedding model, follow these steps:</p> <ol> <li>Install the Mistral Python package:</li> </ol> <pre><code>pip install mistralai\n</code></pre> <ol> <li>Load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nMISTRAL_API_KEY = os.environ['MISTRAL_API_KEY']\n</code></pre> <ol> <li>Import Indox modules and set the Mistral embedding model:</li> </ol> <pre><code>from indox.embeddings import MistralEmbedding\n\nmistral_embedding = MistralEmbedding(api_key=MISTRAL_API_KEY)\n</code></pre>"},{"location":"Indox/embedding_models/#using_hugging_face_embedding_model","title":"Using Hugging Face Embedding Model","text":"<p>To use the Hugging Face embedding model, follow these steps:</p> <ol> <li>Load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n</code></pre> <ol> <li>Import Indox modules and set the Hugging Face embedding model:</li> </ol> <pre><code>from indox.embeddings import HuggingFaceEmbedding\n\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_clarifai_embedding_model","title":"Using Clarifai Embedding Model","text":"<p>To use the Clarifai embedding model, follow these steps:</p> <ol> <li>Install the Clarifai Python package:</li> </ol> <pre><code>pip install clarifai\n</code></pre> <ol> <li>Load environment variables and import the Indox modules:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nCLARIFAI_PAT = os.environ['CLARIFAI_PAT']\n\nfrom indox.embeddings import ClarifaiEmbeddings\n\nclarifai_embeddings = ClarifaiEmbeddings(pat=CLARIFAI_PAT, model_id=\"model-id\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_cohere_embedding_model","title":"Using Cohere Embedding Model","text":"<p>To use the Cohere embedding model, follow these steps:</p> <ol> <li>Install the Cohere Python package:</li> </ol> <pre><code>pip install cohere\n</code></pre> <ol> <li>Load environment variables and import the Indox modules:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nCOHERE_API_KEY = os.environ['COHERE_API_KEY']\n\nfrom indox.embeddings import CohereEmbeddings\n\ncohere_embeddings = CohereEmbeddings(cohere_api_key=COHERE_API_KEY)\n</code></pre>"},{"location":"Indox/embedding_models/#using_elasticsearch_embedding_model","title":"Using Elasticsearch Embedding Model","text":"<p>To use the Elasticsearch embedding model, follow these steps:</p> <ol> <li> <p>Ensure you have an embedding model loaded and deployed in your Elasticsearch cluster.</p> </li> <li> <p>Import necessary libraries and set the Elasticsearch embedding model:</p> </li> </ol> <pre><code>from elasticsearch import Elasticsearch\nfrom indox.embeddings import ElasticsearchEmbeddings\n\nes_client = Elasticsearch(\"http://localhost:9200\")\nes_embeddings = ElasticsearchEmbeddings(client=es_client, model_id=\"model-id\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_gpt4all_embedding_model","title":"Using GPT4All Embedding Model","text":"<p>To use the GPT4All embedding model, follow these steps:</p> <ol> <li>Install the GPT4All Python package:</li> </ol> <pre><code>pip install gpt4all\n</code></pre> <ol> <li>Import Indox modules and set the GPT4All embedding model:</li> </ol> <pre><code>from indox.embeddings import GPT4AllEmbeddings\n\ngpt4all_embeddings = GPT4AllEmbeddings(model_name=\"gpt4all-lora\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_ollama_embedding_model","title":"Using Ollama Embedding Model","text":"<p>To use the Ollama embedding model, follow these steps:</p> <ol> <li> <p>Follow the instructions at Ollama to set up the model locally.</p> </li> <li> <p>Import Indox modules and set the Ollama embedding model:</p> </li> </ol> <pre><code>from indox.embeddings import OllamaEmbeddings\n\nollama_embeddings = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"llama2\")\n</code></pre>"},{"location":"Indox/embedding_models/#using_azure_openai_embedding_model","title":"Using Azure OpenAi Embedding Model","text":"<p>To use the Azure OpenAI embedding model, follow these steps:</p> <ol> <li>Install the OpenAI Python package:</li> </ol> <pre><code>pip install openai\n</code></pre> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <ol> <li>Import Indox modules and set the Azure OpenAI embedding model:</li> </ol> <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.embeddings import AzureOpenAIEmbeddings\n\nIndox = IndoxRetrievalAugmentation()\nopenai_embeddings = AzureOpenAIEmbeddings(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"Indox/embedding_models/#future_plans","title":"Future Plans","text":"<p>We are committed to continuously improving Indox and will be adding support for more embedding models in the future.</p>"},{"location":"Indox/llms/","title":"LLMs","text":"<p>Indox supports three different types of question-answer (QA) models. These models are:</p> <ol> <li>OpenAI Model</li> <li>Mistral Model</li> <li>HuggingFace Models</li> <li>GoogleAi Models</li> <li>Ollama</li> </ol>"},{"location":"Indox/llms/#initial_setup","title":"Initial Setup","text":"<p>For all QA models, the initial setup is the same. Start by importing the necessary Indox module and creating an instance of <code>IndoxRetrievalAugmentation</code>:</p> <pre><code>from indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n</code></pre>"},{"location":"Indox/llms/#using_openai_model","title":"Using OpenAI Model","text":"<p>To use the OpenAI QA model, follow these steps: First install the OpenAI Python package: <pre><code>pip install openai\n</code></pre></p> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <ol> <li>Import Indox modules and set the OpenAI model:</li> </ol> <pre><code>from indox.llms import OpenAi\n\nopenai_qa = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\nretriever = indox.QuestionAnswer(vector_database=db,llm=openai_qa,top_k=5)\nretriever.invoke(query)\n</code></pre>"},{"location":"Indox/llms/#using_mistral_model","title":"Using Mistral Model","text":"<p>First install the Hugging Face Python package: <pre><code>pip install mistralai\n</code></pre></p> <p>To use the Mistral model, follow these steps:</p> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nMISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n</code></pre> <ol> <li>Import Indox modules and set the Mistral model:</li> </ol> <pre><code>from indox.llms import Mistral\n\nmistral_qa = Mistral(api_key=MISTRAL_API_KEY, model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)\nretriever.invoke(query)\n</code></pre>"},{"location":"Indox/llms/#using_googleai_model","title":"Using GoogleAi Model","text":"<p>First install the Hugging Face Python package: <pre><code>pip install google-generativeai\n</code></pre></p> <p>To use the GoogleAi model, follow these steps:</p> <ol> <li>Import necessary libraries and load environment variables:</li> </ol> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n</code></pre> <ol> <li>Import Indox modules and set the Mistral model:</li> </ol> <pre><code>from indox.llms import GoogleAi\n\nmistral_qa = GoogleAi(api_key=GOOGLE_API_KEY, model=\"gemini-1.5-flash-latest\")\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)\nretriever.invoke(query)\n</code></pre>"},{"location":"Indox/llms/#future_plans","title":"Future Plans","text":"<p>We are committed to continuously improving Indox and will be adding support for more QA models in the future.</p>"},{"location":"Indox/quick_start/","title":"Quick Start","text":""},{"location":"Indox/quick_start/#quick_start","title":"Quick Start","text":""},{"location":"Indox/quick_start/#overview","title":"Overview","text":"<p>This documentation provides a detailed explanation of how to use the <code>IndoxRetrievalAugmentation</code> package for QA model and embedding selection, document splitting, and storing in a vector store.</p>"},{"location":"Indox/quick_start/#setup","title":"Setup","text":""},{"location":"Indox/quick_start/#install_the_required_packages","title":"Install the Required Packages","text":"<pre><code>!pip install indox\n!pip install openai\n!pip install chromadb\n</code></pre>"},{"location":"Indox/quick_start/#load_environment_variables","title":"Load Environment Variables","text":"<p>To start, you need to load your API keys from the environment.</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre>"},{"location":"Indox/quick_start/#import_indox_package","title":"Import Indox Package","text":"<p>Import the necessary classes from the Indox package.</p> <pre><code>from indox import IndoxRetrievalAugmentation\n</code></pre>"},{"location":"Indox/quick_start/#importing_llm_and_embedding_models","title":"Importing LLM and Embedding Models","text":"<pre><code>from indox.llms import OpenAi\n</code></pre> <pre><code>from indox.embeddings import OpenAiEmbedding\n</code></pre>"},{"location":"Indox/quick_start/#initialize_indox","title":"Initialize Indox","text":"<p>Create an instance of IndoxRetrievalAugmentation.</p> <pre><code>Indox = IndoxRetrievalAugmentation()\n</code></pre> <pre><code>openai_qa = OpenAiQA(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo-0125\")\nopenai_embeddings = OpenAiEmbedding(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n</code></pre> <pre><code>file_path = \"sample.txt\"\n</code></pre> <p>In this section, we take advantage of the <code>unstructured</code> library to load documents and split them into chunks by title. This method helps in organizing the document into manageable sections for further processing.</p> <pre><code>from indox.data_loader_splitter import UnstructuredLoadAndSplit\n</code></pre> <pre><code>loader_splitter = UnstructuredLoadAndSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <pre><code>Starting processing...\nEnd Chunking process.\n</code></pre> <p>Storing document chunks in a vector store is crucial for enabling efficient retrieval and search operations. By converting text data into vector representations and storing them in a vector store, you can perform rapid similarity searches and other vector-based operations.</p> <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed_openai)\nIndox.connect_to_vectorstore(db)\nIndox.store_in_vectorstore(docs)\n</code></pre> <pre><code>2024-05-14 15:33:04,916 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n2024-05-14 15:33:12,587 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-05-14 15:33:13,574 - INFO - Document added successfully to the vector store.\n\nConnection established successfully.\n\n&lt;Indox.vectorstore.ChromaVectorStore at 0x28cf9369af0&gt;\n</code></pre>"},{"location":"Indox/quick_start/#quering","title":"Quering","text":"<pre><code>query = \"how cinderella reach her happy ending?\"\n</code></pre> <pre><code>retriever = indox.QuestionAnswer(vector_database=db,llm=openai_qa,top_k=5)\nretriever.invoke(query)\n</code></pre> <pre><code>2024-05-14 15:34:55,380 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-05-14 15:35:01,917 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n'Cinderella reached her happy ending by enduring mistreatment from her step-family, finding solace and help from the hazel tree and the little white bird, attending the royal festival where the prince recognized her as the true bride, and ultimately fitting into the golden shoe that proved her identity. This led to her marrying the prince and living happily ever after.'\n</code></pre> <p><pre><code>retriever.context\n</code></pre>     [\"from the hazel-bush. Cinderella thanked him, went to her mother's\\n\\ngrave and planted the branch on it, and wept so much that the tears\\n\\nfell down on it and watered it. And it grew and became a handsome\\n\\ntree. Thrice a day cinderella went and sat beneath it, and wept and\\n\\nprayed, and a little white bird always came on the tree, and if\\n\\ncinderella expressed a wish, the bird threw down to her what she\\n\\nhad wished for.\\n\\nIt happened, however, that the king gave orders for a festival\",      'worked till she was weary she had no bed to go to, but had to sleep\\n\\nby the hearth in the cinders. And as on that account she always\\n\\nlooked dusty and dirty, they called her cinderella.\\n\\nIt happened that the father was once going to the fair, and he\\n\\nasked his two step-daughters what he should bring back for them.\\n\\nBeautiful dresses, said one, pearls and jewels, said the second.\\n\\nAnd you, cinderella, said he, what will you have. Father',      'face he recognized the beautiful maiden who had danced with\\n\\nhim and cried, that is the true bride. The step-mother and\\n\\nthe two sisters were horrified and became pale with rage, he,\\n\\nhowever, took cinderella on his horse and rode away with her. As\\n\\nthey passed by the hazel-tree, the two white doves cried -\\n\\nturn and peep, turn and peep,\\n\\nno blood is in the shoe,\\n\\nthe shoe is not too small for her,\\n\\nthe true bride rides with you,\\n\\nand when they had cried that, the two came flying down and',      \"to send her up to him, but the mother answered, oh, no, she is\\n\\nmuch too dirty, she cannot show herself. But he absolutely\\n\\ninsisted on it, and cinderella had to be called. She first\\n\\nwashed her hands and face clean, and then went and bowed down\\n\\nbefore the king's son, who gave her the golden shoe. Then she\\n\\nseated herself on a stool, drew her foot out of the heavy\\n\\nwooden shoe, and put it into the slipper, which fitted like a\\n\\nglove. And when she rose up and the king's son looked at her\",      'slippers embroidered with silk and silver. She put on the dress\\n\\nwith all speed, and went to the wedding. Her step-sisters and the\\n\\nstep-mother however did not know her, and thought she must be a\\n\\nforeign princess, for she looked so beautiful in the golden dress.\\n\\nThey never once thought of cinderella, and believed that she was\\n\\nsitting at home in the dirt, picking lentils out of the ashes. The\\n\\nprince approached her, took her by the hand and danced with her.']</p>"},{"location":"Indox/data_connectors/arxiv/","title":"ArxivReader","text":"<p>ArxivReader is a data connector for loading paper information from the arXiv repository. It retrieves paper details such as title, abstract, authors, and metadata for given arXiv paper IDs.</p> <p>Note: To use ArxivReader, users need to install the <code>arxiv</code> package. You can install it using <code>pip install arxiv</code>.</p> <p>To use ArxivReader:</p> <pre><code>from indox.data_connectors import ArxivReader\n\nreader = ArxivReader()\ndocuments = reader.load_data(paper_ids=[\"1234.56789\"])\n</code></pre>"},{"location":"Indox/data_connectors/arxiv/#methods","title":"Methods","text":"<p>init() Initializes the ArxivReader. Checks if the <code>arxiv</code> package is installed. class_name() Returns the name of the class as a string.</p> <p>load_data(paper_ids: List[str], load_kwargs: Any) -&gt; List[Document] Loads paper data from arXiv for the given paper IDs.</p> <p>Parameters: - paper_ids [List[str]]: List of arXiv paper IDs to retrieve. - load_kwargs[Any]:  Additional keyword arguments (not used in current implementation).</p> <ul> <li>Returns:</li> <li>List[Document]: List of Document objects containing paper content and metadata.</li> </ul>"},{"location":"Indox/data_connectors/arxiv/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/arxiv/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/arxiv/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/arxiv/#get_started","title":"Get started","text":""},{"location":"Indox/data_connectors/arxiv/#import_essential_libraries","title":"Import Essential Libraries","text":"<p><pre><code>from indox.data_connectors import ArxivReader\n\nreader = ArxivReader()\n\npaper_ids = [\"2201.08239\", \"2203.02155\"]\ndocuments = reader.load_data(paper_ids)\n\nfor doc in documents:\n    print(f\"Title: {doc.metadata['title']}\")\n    print(f\"Authors: {doc.metadata['authors']}\")\n    print(f\"Abstract: {doc.content[:200]}...\")\n    print(f\"arXiv URL: {doc.metadata['arxiv_url']}\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use ArxivReader to retrieve information about specific arXiv papers and access their content and metadata.</p>"},{"location":"Indox/data_connectors/discord/","title":"DiscordChannelReader","text":"<p>DiscordChannelReader is a data connector for loading messages from specified Discord channels. It retrieves messages using a Discord bot.</p> <p>Important:: To use DiscordChannelReader, you need to create a Discord bot and obtain its token. For more information on creating a bot and obtaining its token, visit https://discord.com/developers/docs/intro.</p> <p>Note: To use DiscordChannelReader, users need to install the <code>discord.py</code> package. You can install it using <code>pip install discord.py.</code></p> <p>To use DiscordChannelReader:</p> <pre><code>from indox.data_connectors import DiscordChannelReader\n\nreader = DiscordChannelReader(bot_token=\"your_bot_token\")\ndocuments = reader.load_data(channel_ids=[\"channel_id1\", \"channel_id2\"])\n</code></pre>"},{"location":"Indox/data_connectors/discord/#class_attributes","title":"Class Attributes","text":"<ul> <li>bot_token [str]: The bot token for your Discord bot.</li> <li>num_messages [Optional[int]]: Default number of messages to retrieve from a channel.</li> </ul> <p>init(bot_token: str, num_messages: Optional[int] = 100):</p> <p>Initializes the DiscordChannelReader with the given bot token and optional number of messages. class_name():</p> <p>Returns the name of the class as a string (\"DiscordChannelReader\").</p> <p>load_data(channel_ids: List[str], num_messages: Optional[int] = None, load_kwargs: Any) -&gt; List[Document]</p> <p>Loads messages from specified Discord channels.</p> <p>Parameters: - channel_ids [List[str]]: List of Twitter usernames to fetch tweets from. - num_messages [Optional[int]]: The maximum number of messages to retrieve from each channel (overrides the default if specified). - load_kwargs  [Any]: Additional keyword arguments (not used in current implementation).</p> <p>Returns: - List[Document]: List of Document objects containing channel messages and metadata.</p>"},{"location":"Indox/data_connectors/discord/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/discord/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/discord/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/discord/#get_started","title":"Get Started","text":""},{"location":"Indox/data_connectors/discord/#import_essential_libraries_and_use_discordchannelreader","title":"Import Essential Libraries and Use DiscordChannelReader","text":"<p><pre><code>from indox.data_connectors import DiscordChannelReader\nimport os\nfrom dotenv import load_dotenv\nimport nest_asyncio\n\n# Apply the nest_asyncio patch\nnest_asyncio.apply()\n\nload_dotenv('discord.env')\n# Initialize the reader with your bearer token\ndiscord_token = os.environ['discord_token']\nreader = DiscordChannelReader(bot_token=discord_token)\n\n# Fetch messages from specific Discord channels\nchannel_ids = [\"channel_id1\", \"channel_id2\"]\ndocuments = reader.load_data(channel_ids=channel_ids, num_messages=50)\n\n# Process the retrieved documents\nfor doc in documents:\n    print(f\"Channel ID: {doc.metadata['channel_id']}\")\n    print(f\"Channel Name: {doc.metadata['channel_name']}\")\n    print(f\"Number of messages: {doc.metadata['num_messages']}\")\n    print(f\"Messages preview: {doc.content[:200]}...\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use DiscordChannelReader to retrieve messages from specific Discord channels and access their content and metadata.</p>"},{"location":"Indox/data_connectors/document/","title":"Document","text":"<p>Document is a class that represents a data document from various sources. It provides a structured way to store and manage content along with its metadata.</p> <p>To use the Document class:</p> <pre><code>from indox.data_connectors import Document\n\n# Create a new document\ndoc = Document(source=\"Wikipedia\", content=\"Wikipedia is a free online encyclopedia.\")\n\n# Access document attributes\nprint(doc.id_)\nprint(doc.source)\nprint(doc.content)\nprint(doc.metadata)\n\n# Convert to dictionary\ndoc_dict = doc.to_dict()\n\n# Create from dictionary\nnew_doc = Document.from_dict(doc_dict)\n</code></pre>"},{"location":"Indox/data_connectors/document/#class_attributes","title":"Class Attributes","text":"<ul> <li>id_  [str]: A unique identifier for the document (automatically generated).</li> <li>source [str]: The source of the document (e.g., YouTube, Wikipedia).</li> <li>content [str]: The actual content of the document.</li> <li>metadata [Dict[str, Any]]: Additional metadata associated with the document.</li> </ul> <p>init(source: str, content: str, metadata: Optional[Dict[str, Any]] = None):</p> <p>Initializes a new Document object.</p> <p>Parameters: - source [str]: The source of the document. - - content [str]: The content of the document. - metadata [Optional[Dict[str, Any]]]: Optional metadata associated with the document.</p> <p>Raises:   - TypeError: If the source or content is not a string.   - ValueError: If the source or content is empty.</p> <p>str() -&gt; str:</p> <p>Returns a string representation of the document.</p> <p>to_dict() -&gt; Dict[str, Any]:</p> <p>Converts the document to a dictionary representation.</p> <p>Returns: - Dict[str, Any]: A dictionary containing the document's attributes.</p> <p>from_dict(cls, data: Dict[str, Any]) -&gt; Document:</p> <p>Creates a Document object from a dictionary.</p> <p>Parameters: - data [Dict[str, Any]]: A dictionary containing the document's attributes.</p> <p>Returns: - Document: A new Document object.</p> <p>Raises: - KeyError: If the dictionary is missing required keys. - TypeError: If the dictionary values are not of the expected types. - ValueError: If the source or content is empty.</p>"},{"location":"Indox/data_connectors/document/#usage","title":"Usage","text":"<p><pre><code>from indox.data_connectors import Document\n\n# Create a new document\ndoc = Document(\n    source=\"Wikipedia\",\n    content=\"Wikipedia is a free online encyclopedia.\",\n    metadata={\"language\": \"English\", \"accessed_date\": \"2024-08-20\"}\n)\n\n# Access document attributes\nprint(f\"Document ID: {doc.id_}\")\nprint(f\"Source: {doc.source}\")\nprint(f\"Content: {doc.content}\")\nprint(f\"Metadata: {doc.metadata}\")\n\n# Convert to dictionary\ndoc_dict = doc.to_dict()\nprint(\"Document as dictionary:\", doc_dict)\n\n# Create a new document from dictionary\nnew_doc = Document.from_dict(doc_dict)\nprint(\"New document:\", new_doc)\n\n# String representation\nprint(str(doc))\n</code></pre> This example demonstrates how to create Document objects, access their attributes, convert them to and from dictionaries, and get their string representations.</p>"},{"location":"Indox/data_connectors/github/","title":"GithubRepositoryReader","text":"<p>GithubRepositoryReader is a data connector for loading file content from GitHub repositories. It retrieves file contents and metadata for specified repositories, with options to filter by directories and file extensions.</p> <p>Note: To use GithubRepositoryReader, users need to install the <code>PyGithub</code> package. You can install it using <code>pip install PyGithub</code>.</p> <p>To use GithubRepositoryReader:</p> <pre><code>from indox.data_connectors import GithubClient, GithubRepositoryReader\n\ngithub_client = GithubClient(github_token=\"your_github_token\")\nreader = GithubRepositoryReader(\n    github_client=github_client,\n    owner=\"repository_owner\",\n    repo=\"repository_name\"\n)\ndocuments = reader.load_data(branch=\"main\")\n</code></pre>"},{"location":"Indox/data_connectors/github/#class_attributes","title":"Class Attributes","text":"<ul> <li>FilterType  [Enum-like class]:</li> <li>INCLUDE: Used to include specified directories or file extensions.</li> <li>EXCLUDE: Used to exclude specified directories or file extensions.</li> </ul>"},{"location":"Indox/data_connectors/github/#methods","title":"Methods","text":"<p>init(github_client: GithubClient, owner: str, repo: str, use_parser: bool = False, verbose: bool = True, filter_directories: Optional[Tuple[List[str], str]] = None, filter_file_extensions: Optional[Tuple[List[str], str]] = None)</p> <p>Initializes the <code>GithubRepositoryReader</code> with the specified parameters.</p> <p>Parameters:</p> <p>Returns the name of the class as a string.</p> <p>load_data(paper_ids: List[str], load_kwargs: Any) -&gt; List[Document]</p> <p>Loads paper data from arXiv for the given paper IDs.</p> <p>Parameters: - github_client [GithubClient]: Authenticated GitHub client. - owner [str]: Owner of the GitHub repository. - repo [str]: Name of the GitHub repository. - use_parser [bool]: Whether to use a parser (not implemented in current version). - verbose [bool]: Whether to print verbose output. - filter_directories [Optional[Tuple[List[str], str]]]: Tuple of directories to filter and filter type. - filter_file_extensions [Optional[Tuple[List[str], str]]]: Tuple of file extensions to filter and filter type.</p> <p>load_data(branch: str = \"main\") -&gt; List[Document]</p> <p>Loads file data from the specified GitHub repository.</p> <p>Parameters: - branch [str]: The branch to load data from (default is \"main\").</p> <p>Returns:   - List[Document]:  List of Document objects containing file content and metadata.</p>"},{"location":"Indox/data_connectors/github/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/github/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/github/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/github/#get_started","title":"Get started","text":""},{"location":"Indox/data_connectors/github/#import_essential_libraries_and_set_up_client","title":"Import Essential Libraries and Set Up Client","text":"<p><pre><code>from indox.data_connectors import GithubClient, GithubRepositoryReader\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv('github.env')\ngithub_token = os.environ['github_token']\ngithub_client = GithubClient(github_token=github_token)\n\n# Instantiate the repository reader\nrepo_reader = GithubRepositoryReader(\n    github_client=github_client,\n    owner=\"osllmai\",\n    repo=\"indoxjudge\",\n    filter_directories=([\"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n    filter_file_extensions=([\".md\"], GithubRepositoryReader.FilterType.INCLUDE)\n)\n\n# Load data from the repository\ndocuments = repo_reader.load_data(branch=\"main\")\n\n# Print document information\nfor doc in documents:\n    print(f\"File: {doc.metadata['file_name']}\")\n    print(f\"Path: {doc.metadata['file_path']}\")\n    print(f\"Size: {doc.metadata['file_size']} bytes\")\n    print(f\"Content preview: {doc.content[:200]}...\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use GithubRepositoryReader to retrieve information about specific files from a GitHub repository and access their content and metadata.</p>"},{"location":"Indox/data_connectors/google_chat/","title":"Google Chat","text":""},{"location":"Indox/data_connectors/google_chat/#googlechat","title":"GoogleChat","text":""},{"location":"Indox/data_connectors/google_chat/#overview","title":"Overview","text":"<p>The <code>GoogleChat</code> class allows users to interact with the Google Chat API to list spaces (chat rooms) within Google Chat. It handles the authentication process using OAuth 2.0 and manages credentials, including refreshing them when necessary.</p>"},{"location":"Indox/data_connectors/google_chat/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client google-chat\n</code></pre>"},{"location":"Indox/data_connectors/google_chat/#quick_start","title":"Quick Start","text":"<ol> <li>Initialize the <code>GoogleChat</code> Class</li> </ol> <p>To use the Google Chat API, you need to authenticate using OAuth 2.0. This requires a <code>credentials.json</code> file that contains your application's client ID and client secret.</p> <p>Initialize the Class:</p> <pre><code>from google_chat import GoogleChat\n\n# Initialize GoogleChat class\nchat = GoogleChat()\n</code></pre> <ol> <li> <p>List Google Chat Spaces</p> </li> <li> <p>Call the <code>list_spaces</code> method to list the spaces in Google Chat.</p> </li> </ol> <pre><code>spaces = chat.list_spaces()\n</code></pre>"},{"location":"Indox/data_connectors/google_chat/#class_googlechat","title":"Class <code>GoogleChat</code>","text":""},{"location":"Indox/data_connectors/google_chat/#attributes","title":"Attributes","text":"<ul> <li><code>SCOPES</code>: The scopes required for the Google Chat API, specifically <code>https://www.googleapis.com/auth/chat.spaces.readonly</code>.</li> <li><code>TOKEN_FILE</code>: The path to the file where the OAuth2 token is stored.</li> <li><code>CREDENTIALS_FILE</code>: The path to the file containing the OAuth2 client secrets.</li> </ul>"},{"location":"Indox/data_connectors/google_chat/#methods","title":"Methods","text":""},{"location":"Indox/data_connectors/google_chat/#__init__self","title":"<code>__init__(self)</code>","text":"<p>Initializes the <code>GoogleChat</code> object and handles the authentication process.</p> <p>Parameters: - None</p> <p>Returns: - None</p> <p>Notes: - Calls the <code>_authenticate</code> method to handle OAuth2 authentication and create the API client.</p>"},{"location":"Indox/data_connectors/google_chat/#_authenticateself","title":"<code>_authenticate(self)</code>","text":"<p>Handles the OAuth2 authentication flow and creates the Google Chat API client.</p> <p>Notes: - Uses stored credentials if available and valid. - Refreshes credentials if they have expired and a refresh token is available. - Saves new credentials to a token file if a new authentication flow is initiated. - Initializes the Google Chat API client.</p>"},{"location":"Indox/data_connectors/google_chat/#list_spacesself","title":"<code>list_spaces(self)</code>","text":"<p>Lists the spaces in Google Chat and prints the details of each space.</p> <p>Notes: - This method filters for spaces of type \"SPACE\" in Google Chat. - If an error occurs during the API call, the method returns <code>None</code>.</p>"},{"location":"Indox/data_connectors/google_chat/#error_handling","title":"Error Handling","text":"<ul> <li>Errors during authentication, credential saving, and API client creation are caught, printed, and re-raised.</li> <li>Errors during space listing are caught and logged, and <code>None</code> is returned.</li> </ul>"},{"location":"Indox/data_connectors/google_chat/#example_usage","title":"Example Usage","text":"<pre><code>from google_chat import GoogleChat\n\n# Initialize GoogleChat class\nchat = GoogleChat()\n\n# List spaces in Google Chat\nspaces = chat.list_spaces()\n</code></pre>"},{"location":"Indox/data_connectors/google_chat/#notes","title":"Notes","text":"<ul> <li>Ensure you have a valid <code>credentials.json</code> file containing your OAuth2 client ID and client secret.</li> <li>The <code>token.json</code> file is created after successful authentication and is used to store credentials for future runs.</li> </ul>"},{"location":"Indox/data_connectors/google_doc/","title":"Google Doc","text":""},{"location":"Indox/data_connectors/google_doc/#googledoc","title":"GoogleDoc","text":""},{"location":"Indox/data_connectors/google_doc/#overview","title":"Overview","text":"<p>The <code>GoogleDoc</code> class enables users to read the content of a Google Document using the Google Docs API. It handles OAuth2 authentication and provides methods to access and print the content of a specified Google Doc.</p>"},{"location":"Indox/data_connectors/google_doc/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n</code></pre>"},{"location":"Indox/data_connectors/google_doc/#quick_start","title":"Quick Start","text":"<ol> <li>Initialize the <code>GoogleDoc</code> Class</li> </ol> <pre><code>from google_doc import GoogleDoc\n\n# Initialize GoogleDoc with custom paths for credentials files\ndoc = GoogleDoc(creds_file='tokenGoogleDoc.json', credentials_json='./credentials.json')\n</code></pre> <ol> <li> <p>Read the Document Content</p> </li> <li> <p>Call the <code>read</code> method with the document ID you want to access.</p> </li> </ol> <pre><code>document_id = 'YOUR DOCUMENT ID HERE'\ndoc.read(document_id)\n</code></pre>"},{"location":"Indox/data_connectors/google_doc/#class_googledoc","title":"Class <code>GoogleDoc</code>","text":""},{"location":"Indox/data_connectors/google_doc/#methods","title":"Methods","text":""},{"location":"Indox/data_connectors/google_doc/#__init__self_creds_filetokengoogledocjson_credentials_jsoncredentialsjson","title":"<code>__init__(self, creds_file='tokenGoogleDoc.json', credentials_json='./credentials.json')</code>","text":"<p>Initializes the <code>GoogleDoc</code> object and handles OAuth2 authentication.</p> <p>Parameters: - <code>creds_file</code> (str): The path to the file where the credentials are stored. Default is <code>'tokenGoogleDoc.json'</code>. - <code>credentials_json</code> (str): The path to the JSON file containing the client secrets. Default is <code>'./credentials.json'</code>.</p> <p>Notes: - Calls <code>_authenticate</code> to handle the authentication process.</p>"},{"location":"Indox/data_connectors/google_doc/#def__authenticateself","title":"<code>def _authenticate(self)</code>","text":"<p>Handles user authentication and refreshes credentials if necessary.</p> <p>Notes: - Reads existing credentials from <code>creds_file</code> if available. - If credentials are not valid or do not exist, it performs the OAuth2 flow to obtain new credentials and saves them to <code>creds_file</code>.</p>"},{"location":"Indox/data_connectors/google_doc/#readself_document_id_str_-_none","title":"<code>read(self, document_id: str) -&gt; None</code>","text":"<p>Reads the content of a Google Doc by its document ID.</p> <p>Parameters: - <code>document_id</code> (str): The ID of the Google Doc to be read.</p> <p>Notes: - Retrieves and prints the text content of the document. - Iterates through the document\u2019s content and prints text from paragraph elements. - Deletes the <code>creds_file</code> after use.</p>"},{"location":"Indox/data_connectors/google_doc/#error_handling","title":"Error Handling","text":"<ul> <li>Raises an exception if there is an issue with the Google API request or authentication process.</li> </ul>"},{"location":"Indox/data_connectors/google_doc/#example_usage","title":"Example Usage","text":"<pre><code>from google_doc import GoogleDoc\n\n# Initialize GoogleDoc object\ndoc = GoogleDoc(credentials_json='./credentials.json')\n\n# Read the content of a document\ndocument_id = 'YOUR DOCUMENT ID HERE'\ndoc.read(document_id)\n</code></pre>"},{"location":"Indox/data_connectors/google_doc/#notes","title":"Notes","text":"<ul> <li>Ensure that your <code>credentials.json</code> file contains valid OAuth2 credentials and that the <code>tokenGoogleDoc.json</code> file is writable.</li> <li>The <code>document_id</code> must be a valid Google Document ID that you have access to.</li> </ul>"},{"location":"Indox/data_connectors/google_drive/","title":"Google Drive","text":""},{"location":"Indox/data_connectors/google_drive/#googledrive","title":"GoogleDrive","text":""},{"location":"Indox/data_connectors/google_drive/#overview","title":"Overview","text":"<p>The <code>GoogleDrive</code> class provides functionality to authenticate with Google Drive using OAuth2 and read the content of various file types from Google Drive. It supports multiple file formats including plain text, Jupyter Notebooks, PDF, Excel, CSV, PowerPoint, Word documents, JSON, XML, and HTML.</p>"},{"location":"Indox/data_connectors/google_drive/#installation","title":"Installation","text":"<p>Ensure you have the following Python libraries installed:</p> <pre><code>pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pandas nbformat PyPDF2 python-pptx python-docx beautifulsoup4\n</code></pre>"},{"location":"Indox/data_connectors/google_drive/#quick_start","title":"Quick Start","text":"<ol> <li> <p>Set Up Google API Credentials</p> </li> <li> <p>Go to the Google Cloud Console.</p> </li> <li>Create a new project (if you don't have one).</li> <li>Enable the Google Drive API for your project.</li> <li> <p>Create OAuth 2.0 Client IDs and download the <code>credentials.json</code> file.</p> </li> <li> <p>Initialize the <code>GoogleDrive</code> Class</p> </li> </ol> <pre><code>from google_drive import GoogleDrive\n\n# Initialize GoogleDrive object\ndrive = GoogleDrive(creds_file='tokenGoogleDrive.json', credentials_json='credentials.json')\n</code></pre> <ol> <li> <p>Read a File from Google Drive</p> </li> <li> <p>Call the <code>read</code> method with the file ID from Google Drive.</p> </li> </ol> <pre><code>file_id = 'YOUR_FILE_ID_HERE'\ndrive.read(file_id)\n</code></pre>"},{"location":"Indox/data_connectors/google_drive/#class_googledrive","title":"Class <code>GoogleDrive</code>","text":""},{"location":"Indox/data_connectors/google_drive/#methods","title":"Methods","text":""},{"location":"Indox/data_connectors/google_drive/#__init__self_creds_filetokengoogledrivejson_credentials_jsoncredentialsjson","title":"<code>__init__(self, creds_file='tokenGoogleDrive.json', credentials_json='credentials.json')</code>","text":"<p>Initializes the <code>GoogleDrive</code> object and handles OAuth2 authentication.</p> <p>Parameters: - <code>creds_file</code> (str): The path to the file where the credentials are stored. Default is <code>'tokenGoogleDrive.json'</code>. - <code>credentials_json</code> (str): The path to the JSON file containing the client secrets. Default is <code>'credentials.json'</code>.</p>"},{"location":"Indox/data_connectors/google_drive/#readself_file_id","title":"<code>read(self, file_id)</code>","text":"<p>Reads and prints the content of a file from Google Drive based on its MIME type.</p> <p>Parameters: - <code>file_id</code> (str): The ID of the file in Google Drive to be read.</p> <p>Raises: - <code>Exception</code>: For any errors encountered during file reading or processing.</p>"},{"location":"Indox/data_connectors/google_drive/#file_type_handling","title":"File Type Handling","text":"<ul> <li>Plain Text (<code>text/plain</code>): Reads and prints plain text files.</li> <li>Jupyter Notebook (<code>application/x-ipynb+json</code>): Reads and prints the code cells from Jupyter Notebooks.</li> <li>PDF (<code>application/pdf</code>): Extracts and prints text from PDF files.</li> <li>Excel (<code>application/vnd.openxmlformats-officedocument.spreadsheetml.sheet</code>): Reads and prints content from Excel files.</li> <li>CSV (<code>text/csv</code>): Reads and prints content from CSV files.</li> <li>PowerPoint (<code>application/vnd.openxmlformats-officedocument.presentationml.presentation</code>): Extracts and prints text from PowerPoint slides.</li> <li>Word Document (<code>application/vnd.openxmlformats-officedocument.wordprocessingml.document</code>): Reads and prints content from Word documents.</li> <li>JSON (<code>application/json</code>): Reads and prints JSON data.</li> <li>XML (<code>application/xml</code>): Parses and prints XML data.</li> <li>HTML (<code>text/html</code>): Parses and prints HTML content.</li> </ul>"},{"location":"Indox/data_connectors/google_drive/#error_handling","title":"Error Handling","text":"<p>Errors during file reading or processing are caught and reported. Temporary credentials file is removed after processing.</p>"},{"location":"Indox/data_connectors/google_drive/#example_usage","title":"Example Usage","text":"<pre><code>from google_drive import GoogleDrive\n\n# Initialize GoogleDrive object\ndrive = GoogleDrive(creds_file='tokenGoogleDrive.json', credentials_json='credentials.json')\n\n# Read content from a file with a specific file ID\nfile_id = 'YOUR_FILE_ID_HERE'\ndrive.read(file_id)\n</code></pre>"},{"location":"Indox/data_connectors/google_drive/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>credentials.json</code> is properly set up with OAuth2 credentials.</li> <li>The <code>tokenGoogleDrive.json</code> file is automatically created and managed to store the user's access and refresh tokens.</li> </ul>"},{"location":"Indox/data_connectors/google_sheet/","title":"Google Sheet","text":""},{"location":"Indox/data_connectors/google_sheet/#googlesheet","title":"GoogleSheet","text":""},{"location":"Indox/data_connectors/google_sheet/#overview","title":"Overview","text":"<p>The <code>GoogleSheet</code> class provides functionality to authenticate with Google Sheets using OAuth2 and read data from a specific Google Sheet. It supports reading data from a defined range within the sheet and prints the contents to the console.</p>"},{"location":"Indox/data_connectors/google_sheet/#installation","title":"Installation","text":"<p>Ensure you have the following Python libraries installed:</p> <pre><code>pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n</code></pre>"},{"location":"Indox/data_connectors/google_sheet/#quick_start","title":"Quick Start","text":"<ol> <li> <p>Set Up Google API Credentials</p> </li> <li> <p>Go to the Google Cloud Console.</p> </li> <li>Create a new project (if you don't have one).</li> <li>Enable the Google Sheets API for your project.</li> <li> <p>Create OAuth 2.0 Client IDs and download the <code>credentials.json</code> file.</p> </li> <li> <p>Initialize the <code>GoogleSheet</code> Class</p> </li> </ol> <pre><code>from google_sheet import GoogleSheet\n\n# Initialize GoogleSheet object\nsheet = GoogleSheet(creds_file='tokenGoogleSheet.json', credentials_json='credentials.json')\n</code></pre> <ol> <li> <p>Read Data from a Google Sheet</p> </li> <li> <p>Call the <code>read</code> method with the spreadsheet ID and optionally specify a range.</p> </li> </ol> <pre><code>spreadsheet_id = 'YOUR_SPREADSHEET_ID_HERE'\nsheet.read(spreadsheet_id, 'Sheet1!A1:Z')\n</code></pre>"},{"location":"Indox/data_connectors/google_sheet/#class_googlesheet","title":"Class <code>GoogleSheet</code>","text":""},{"location":"Indox/data_connectors/google_sheet/#methods","title":"Methods","text":""},{"location":"Indox/data_connectors/google_sheet/#__init__self_creds_filetokengooglesheetjson_credentials_jsoncredentialsjson","title":"<code>__init__(self, creds_file='tokenGoogleSheet.json', credentials_json='credentials.json')</code>","text":"<p>Initializes the <code>GoogleSheet</code> object and handles OAuth2 authentication.</p> <p>Parameters: - <code>creds_file</code> (str): The path to the file where the credentials are stored. Default is <code>'tokenGoogleSheet.json'</code>. - <code>credentials_json</code> (str): The path to the JSON file containing the client secrets. Default is <code>'credentials.json'</code>.</p>"},{"location":"Indox/data_connectors/google_sheet/#_authenticateself","title":"<code>_authenticate(self)</code>","text":"<p>Authenticates the user and refreshes credentials if necessary.</p>"},{"location":"Indox/data_connectors/google_sheet/#readself_spreadsheet_id_range_namea1z","title":"<code>read(self, spreadsheet_id, range_name='A1:Z')</code>","text":"<p>Reads data from a Google Sheet by its spreadsheet ID and range.</p> <p>Parameters: - <code>spreadsheet_id</code> (str): The ID of the Google Sheet to be read. - <code>range_name</code> (str): The A1 notation of the range to retrieve. Default is <code>'A1:Z'</code>.</p> <p>Raises: - <code>GoogleAPIError</code>: If there is an issue with the Google API request.</p> <p>Notes: - Prints the data to the console. - Removes the temporary credentials file after reading.</p>"},{"location":"Indox/data_connectors/google_sheet/#example_usage","title":"Example Usage","text":"<pre><code>from google_sheet import GoogleSheet\n\n# Initialize GoogleSheet object\nsheet = GoogleSheet(credentials_json='credentials.json')\n\n# Read content from a Google Sheet with a specific ID and range\nspreadsheet_id = '1f2qu3NGL-kU_RLSvN1O3rTXi-NpAZRtQ7B0trI5xH-U'\nsheet.read(spreadsheet_id, 'Sheet1!A1:Z')\n</code></pre>"},{"location":"Indox/data_connectors/google_sheet/#notes","title":"Notes","text":"<ul> <li>Ensure that <code>credentials.json</code> is properly set up with OAuth2 credentials.</li> <li>The <code>tokenGoogleSheet.json</code> file is automatically created and managed to store the user's access and refresh tokens.</li> <li>The <code>read</code> method prints the content of the specified range to the console. Adjust the range as needed for your specific use case.</li> </ul>"},{"location":"Indox/data_connectors/guten/","title":"GutenbergReader","text":"<p>GutenbergReader is a data connector for fetching and searching books from Project Gutenberg. It provides functionality to retrieve full book content by ID and search for books based on a query.</p> <p>Note: To use GutenbergReader, users need to install the <code>requests</code> and <code>beautifulsoup4</code> packages. You can install them using <code>pip install requests beautifulsoup4</code>.</p> <p>To use GutenbergReader:</p> <pre><code>from indox.data_connectors import GutenbergReader\n\nreader = GutenbergReader()\n\n# Fetch a book by ID\nbook = reader.get_book(\"11\")\n\n# Search for books\nsearch_results = reader.search_gutenberg(\"Alice in Wonderland\")\n</code></pre>"},{"location":"Indox/data_connectors/guten/#class_attributes","title":"Class Attributes","text":"<ul> <li>BASE_URL [str]:  The base URL for Project Gutenberg files.</li> </ul>"},{"location":"Indox/data_connectors/guten/#methods","title":"Methods","text":"<p>init() Initializes the <code>GutenbergReader</code> and creates a requests session.</p> <p>get_book(book_id: str) -&gt; Optional[Document]</p> <p>Fetches a book from Project Gutenberg by its ID.</p> <p>Parameters: - book_id [str]: The ID of the book on Project Gutenberg.</p> <p>Returns:   - Optional[Document]: A Document instance containing the book's title, text content, and metadata, or None if the fetch fails.</p> <p>search_gutenberg(query: str) -&gt; List[Document] Searches for books on Project Gutenberg based on a query string. Parameters:</p> <ul> <li>query [str]: Search query string.</li> </ul> <p>Returns:</p> <ul> <li>List[Document]: List of Document instances containing book info (id, title, author) for search results.</li> </ul>"},{"location":"Indox/data_connectors/guten/#private_methods","title":"Private Methods","text":"<p>_extract_title(content: str) -&gt; str Extracts the title of the book from its content. _extract_text(content: str) -&gt; str Extracts the main text content of the book.</p>"},{"location":"Indox/data_connectors/guten/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/guten/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/guten/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/guten/#get_started","title":"Get started","text":""},{"location":"Indox/data_connectors/guten/#import_essential_libraries","title":"Import Essential Libraries","text":"<p><pre><code>from indox.data_connectors import GutenbergReader\n\n# Initialize the reader\nreader = GutenbergReader()\n\n# Fetch a specific book by ID\nbook_id = \"11\"  # Alice's Adventures in Wonderland\nbook = reader.get_book(book_id)\n\nif book:\n    print(f\"Title: {book.metadata['title']}\")\n    print(f\"Content preview: {book.content[:200]}...\")\n    print(\"---\")\n\n# Search for books\nsearch_query = \"Sherlock Holmes\"\nsearch_results = reader.search_gutenberg(search_query)\n\nfor result in search_results[:5]:  # Print first 5 results\n    print(f\"Book ID: {result.metadata['book_id']}\")\n    print(f\"Title: {result.metadata['title']}\")\n    print(f\"Author: {result.metadata['author']}\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use GutenbergReader to fetch a specific book and search for books on Project Gutenberg.</p>"},{"location":"Indox/data_connectors/maps_search/","title":"Maps Search","text":""},{"location":"Indox/data_connectors/maps_search/#mapstextsearch","title":"MapsTextSearch","text":""},{"location":"Indox/data_connectors/maps_search/#overview","title":"Overview","text":"<p>The <code>MapsTextSearch</code> class allows users to search for addresses using the OpenStreetMap Nominatim API. It handles address searching, logging, and printing results. It uses the Geopy library to interact with the Nominatim API.</p>"},{"location":"Indox/data_connectors/maps_search/#installation","title":"Installation","text":"<p>Ensure you have the necessary library installed:</p> <pre><code>pip install geopy\n</code></pre>"},{"location":"Indox/data_connectors/maps_search/#quick_start","title":"Quick Start","text":"<ol> <li>Initialize the <code>MapsTextSearch</code> Class</li> </ol> <p>To use the Nominatim API, you need to provide a user agent string. The user agent should be a meaningful name or identifier that helps Nominatim understand the source of the requests.</p> <p>Creating a User Agent:</p> <ul> <li>Purpose: The user agent helps identify your application to the Nominatim service. It should be unique to your application or project.</li> <li>Format: The user agent string should ideally include your application name and a contact email or website for any potential follow-up.</li> </ul> <p>Example User Agents:    - <code>\"my_application/1.0 (contact@example.com)\"</code>    - <code>\"my_project_name (http://myprojectwebsite.com)\"</code></p> <p>Initialize the Class:</p> <pre><code>from maps_text_search import MapsTextSearch\n\n# Initialize MapsTextSearch with a custom user agent\nsearcher = MapsTextSearch(user_agent=\"my_application/1.0 (contact@example.com)\")\n</code></pre> <ol> <li> <p>Search for an Address</p> </li> <li> <p>Call the <code>search_address</code> method with the address you want to search for.</p> </li> </ol> <pre><code>address = \"Pizza Tower, Via Lecco, 20124 Milan, Italy\"\nsearcher.search_address(address)\n</code></pre>"},{"location":"Indox/data_connectors/maps_search/#class_mapstextsearch","title":"Class <code>MapsTextSearch</code>","text":""},{"location":"Indox/data_connectors/maps_search/#methods","title":"Methods","text":""},{"location":"Indox/data_connectors/maps_search/#__init__self_user_agent_str__osm_search","title":"<code>__init__(self, user_agent: str = \"osm_search\")</code>","text":"<p>Initializes the <code>MapsTextSearch</code> object with the specified user agent.</p> <p>Parameters: - <code>user_agent</code> (str): The user agent for the Nominatim geolocator. Default is <code>\"osm_search\"</code>. It should be a unique identifier for your application or project.</p> <p>Notes: - Configures logging to capture and format log messages. - Handles logging configuration errors by printing a message.</p>"},{"location":"Indox/data_connectors/maps_search/#search_addressself_address_str_-_none","title":"<code>search_address(self, address: str) -&gt; None</code>","text":"<p>Searches for the given address and prints and logs the details.</p> <p>Parameters: - <code>address</code> (str): The address to search for.</p> <p>Notes: - If the address is found, it prints and logs the full address, latitude, and longitude. - If the address is not found, it prints and logs a \"Address not found\" message. - If an error occurs during the search process, it prints and logs the error message, and attempts to log it to a fallback location.</p>"},{"location":"Indox/data_connectors/maps_search/#error_handling","title":"Error Handling","text":"<ul> <li>Logs any issues encountered during the address search or logging processes.</li> <li>Uses a fallback logging mechanism to handle errors during the primary logging process.</li> </ul>"},{"location":"Indox/data_connectors/maps_search/#example_usage","title":"Example Usage","text":"<pre><code>from maps_text_search import MapsTextSearch\n\n# Initialize MapsTextSearch object with a user agent\nsearcher = MapsTextSearch(user_agent=\"my_application/1.0 (contact@example.com)\")\n\n# Search for an address\naddress = \"Pizza Tower, Via Lecco, 20124 Milan, Italy\"\nsearcher.search_address(address)\n</code></pre>"},{"location":"Indox/data_connectors/twitter/","title":"TwitterTweetReader","text":"<p>TwitterTweetReader is a data connector for loading tweets from specified Twitter handles. It retrieves tweets and user metadata using the Twitter API.</p> <p>Note: To use TwitterTweetReader, users need to install the <code>tweepy</code> package and have access to the Twitter API. You can install tweepy using <code>pip install tweepy</code>.</p> <p>For API access, follow the instructions at 'https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api'.</p> <p>To use TwitterTweetReader:</p> <pre><code>from indox.data_connectors import TwitterTweetReader\n\nreader = TwitterTweetReader(bearer_token=\"your_bearer_token\")\ndocuments = reader.load_data(twitterhandles=[\"username1\", \"username2\"])\n</code></pre>"},{"location":"Indox/data_connectors/twitter/#class_attributes","title":"Class Attributes","text":"<ul> <li>bearer_token [str]: Bearer token for Twitter API authentication.</li> <li>num_tweets [Optional[int]]: Default number of tweets to fetch per user.</li> </ul> <p>init(bearer_token: str, num_tweets: Optional[int] = 100):</p> <p>Initializes the TwitterTweetReader with the given bearer token and optional number of tweets.</p> <p>class_name():</p> <p>Returns the name of the class as a string.</p> <p>load_data(twitterhandles: List[str], num_tweets: Optional[int] = None, load_kwargs: Any) -&gt; List[Document]</p> <p>Loads tweets from the specified Twitter handles.</p> <p>Parameters: - twitterhandles [List[str]]: List of Twitter usernames to fetch tweets from. - num_tweets [Optional[int]]: Number of tweets to fetch per user (overrides the default if specified). - load_kwargs [Any]: Additional keyword arguments (not used in current implementation).</p> <p>Returns: - List[Document]: List of Document objects containing tweets and metadata.</p>"},{"location":"Indox/data_connectors/twitter/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/twitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/twitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/twitter/#get_started","title":"Get Started","text":""},{"location":"Indox/data_connectors/twitter/#import_essential_libraries_and_use_twittertweetreader","title":"Import Essential Libraries and Use TwitterTweetReader","text":"<p><pre><code>from indox.data_connectors import TwitterTweetReader\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv('twitter.env')\n# Initialize the reader with your bearer token\ntwitter_token = os.environ['twitter_token']\nreader = TwitterTweetReader(bearer_token=twitter_token)\n\n# Fetch tweets from specific Twitter handles\ntwitter_handles = [\"OpenAI\", \"DeepMind\"]\ndocuments = reader.load_data(twitterhandles=twitter_handles, num_tweets=50)\n\n# Process the retrieved documents\nfor doc in documents:\n    print(f\"Username: {doc.metadata['username']}\")\n    print(f\"User ID: {doc.metadata['user_id']}\")\n    print(f\"Number of tweets: {doc.metadata['num_tweets']}\")\n    print(f\"Tweets preview: {doc.content[:200]}...\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use TwitterTweetReader to retrieve tweets from specific Twitter handles and access their content and metadata.</p>"},{"location":"Indox/data_connectors/wikipedia/","title":"WikipediaReader","text":"<p>WikipediaReader is a data connector for loading content from Wikipedia pages. It retrieves page content, summaries, and metadata for specified Wikipedia pages.</p> <p>Note: To use WikipediaReader, users need to install the <code>wikipedia</code> package. You can install it using <code>pip install wikipedia</code>.</p> <p>To use WikipediaReader:</p> <pre><code>from indox.data_connectors import WikipediaReader\n\nreader = WikipediaReader()\ndocuments = reader.load_data(pages=[\"Python (programming language)\", \"Artificial intelligence\"])\n</code></pre>"},{"location":"Indox/data_connectors/wikipedia/#methods","title":"Methods","text":"<p>init()</p> <p>Initializes the WikipediaReader and checks if the <code>wikipedia</code> package is installed.</p> <p>class_name()</p> <p>Returns the name of the class as a string.</p> <p>load_data(pages: List[str], load_kwargs: Any) -&gt; List[Document]</p> <p>Loads data from the specified Wikipedia pages.</p> <p>Parameters: - pages [List[str]]: List of Wikipedia page titles to retrieve. - load_kwargs [Any]: Additional keyword arguments passed to <code>wikipedia.page()</code>.</p> <p>Returns: - List[Document]: List of Document objects containing page content and metadata.</p>"},{"location":"Indox/data_connectors/wikipedia/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/wikipedia/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/wikipedia/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/wikipedia/#get_started","title":"Get Started","text":""},{"location":"Indox/data_connectors/wikipedia/#import_essential_libraries_and_use_wikipediareader","title":"Import Essential Libraries and Use WikipediaReader","text":"<p><pre><code>from indox.data_connectors import WikipediaReader\n\n# Initialize the reader\nreader = WikipediaReader()\n\n# Fetch content from specific Wikipedia pages\npages = [\"Python (programming language)\", \"Artificial intelligence\"]\ndocuments = reader.load_data(pages)\n\n# Process the retrieved documents\nfor doc in documents:\n    print(f\"Title: {doc.metadata['title']}\")\n    print(f\"URL: {doc.metadata['url']}\")\n    print(f\"Summary: {doc.metadata['summary'][:200]}...\")\n    print(f\"Content preview: {doc.content[:200]}...\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use WikipediaReader to retrieve content from specific Wikipedia pages and access their content and metadata.   </p>"},{"location":"Indox/data_connectors/youtube/","title":"YoutubeTranscriptReader","text":"<p>YoutubeTranscriptReader is a data connector for loading transcripts from YouTube videos. It retrieves transcript text and metadata for specified YouTube video links.</p> <p>Note: To use YoutubeTranscriptReader, users need to install the <code>youtube_transcript_api</code> package. You can install it using <code>pip install youtube-transcript-api</code>.</p> <p>To use YoutubeTranscriptReader:</p> <pre><code>from indox.data_connectors import YoutubeTranscriptReader\n\nreader = YoutubeTranscriptReader()\ndocuments = reader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=dN0lsF2cvm4&amp;t=44s\"])\n</code></pre>"},{"location":"Indox/data_connectors/youtube/#class_attributes","title":"Class Attributes","text":"<ul> <li>languages [tuple]: Tuple of language codes for transcript retrieval (default is (\"en\",) for English).</li> </ul>"},{"location":"Indox/data_connectors/youtube/#methods","title":"Methods","text":"<p>class_name() Returns the name of the class as a string.</p> <p>load_data(ytlinks: List[str], load_kwargs: Any) -&gt; List[Document]</p> <p>Loads transcript data from the specified YouTube video links.</p> <p>Parameters: - ytlinks [List[str]]: List of YouTube video URLs to retrieve transcripts from. - load_kwargs [Any]: Additional keyword arguments (not used in current implementation).</p> <p>Returns: - List[Document]: List of Document objects containing transcript text and metadata.</p>"},{"location":"Indox/data_connectors/youtube/#usage","title":"Usage","text":""},{"location":"Indox/data_connectors/youtube/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":"<p>Windows 1. Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2. Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></p>"},{"location":"Indox/data_connectors/youtube/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/data_connectors/youtube/#get_started","title":"Get Started","text":""},{"location":"Indox/data_connectors/youtube/#import_essential_libraries_and_use_youtubetranscriptreader","title":"Import Essential Libraries and Use YoutubeTranscriptReader","text":"<p><pre><code>from indox.data_connectors import YoutubeTranscriptReader\n\n# Initialize the reader\nreader = YoutubeTranscriptReader()\n\n# Fetch transcripts from specific YouTube videos\nvideo_links = [\"https://www.youtube.com/watch?v=dN0lsF2cvm4&amp;t=44s\"]\n\ndocuments = reader.load_data(ytlinks=video_links)\n\n# Process the retrieved documents\nfor doc in documents:\n    print(f\"Video ID: {doc.metadata['video_id']}\")\n    print(f\"Video Link: {doc.metadata['link']}\")\n    print(f\"Language: {doc.metadata['language']}\")\n    print(f\"Transcript preview: {doc.content[:200]}...\")\n    print(\"---\")\n</code></pre> This example demonstrates how to use YoutubeTranscriptReader to retrieve transcripts from specific YouTube videos and access their content and metadata.</p>"},{"location":"Indox/data_loader/Bs4/","title":"Bs4","text":""},{"location":"Indox/data_loader/Bs4/#html_loader","title":"HTML Loader","text":""},{"location":"Indox/data_loader/Bs4/#overview","title":"Overview","text":"<p>The <code>Bs4</code> function loads an HTML file, extracts its text content, and returns it as a list of <code>Document</code> objects with minimal metadata. This function is useful for processing HTML documents, making the text content easily accessible for further analysis or storage.</p>"},{"location":"Indox/data_loader/Bs4/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install beautifulsoup4 indox\n</code></pre>"},{"location":"Indox/data_loader/Bs4/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Bs4</code> Function</li> </ol> <pre><code>from your_module import Bs4\n</code></pre> <ol> <li> <p>Load and Parse an HTML File</p> </li> <li> <p>Call the <code>Bs4</code> function with the file path to your HTML file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.html'\ndocuments = Bs4(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content and metadata of the HTML file.</p> </li> </ol> <pre><code>document = documents[0]\nprint(document.metadata)\nprint(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Bs4/#function_bs4","title":"Function <code>Bs4</code>","text":""},{"location":"Indox/data_loader/Bs4/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the HTML file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Bs4/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of the HTML file and minimal metadata.</li> </ul>"},{"location":"Indox/data_loader/Bs4/#notes","title":"Notes","text":"<ul> <li>Metadata includes the absolute file path and a default page number of <code>1</code>.</li> <li>The text content is extracted using BeautifulSoup, which handles HTML parsing and text extraction.</li> </ul>"},{"location":"Indox/data_loader/Bs4/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the specified file does not exist.</li> <li>Raises <code>UnicodeDecodeError</code> if there is an error decoding the HTML file.</li> <li>Raises <code>RuntimeError</code> for any other errors encountered during HTML processing or document creation.</li> </ul>"},{"location":"Indox/data_loader/Bs4/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Bs4\n\n# Load and parse the HTML file\nfile_path = 'elements.html'\ndocuments = Bs4(file_path)\n\n# Access the document's metadata and content\ndocument = documents[0]\nprint(document.metadata)  # Prints metadata like file path and page number\nprint(document.page_content)  # Prints the text content of the HTML file\n</code></pre>"},{"location":"Indox/data_loader/Bs4/#notes_1","title":"Notes","text":"<ul> <li>Ensure the HTML file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>page</code> metadata is set to <code>1</code> by default and may need adjustment based on your specific use case.</li> </ul>"},{"location":"Indox/data_loader/Csv/","title":"Csv","text":""},{"location":"Indox/data_loader/Csv/#csv_loader","title":"Csv Loader","text":""},{"location":"Indox/data_loader/Csv/#overview","title":"Overview","text":"<p>The <code>Csv</code> function is designed to load and parse a CSV file, converting its rows into a list of <code>Document</code> objects. This function is particularly useful for situations where CSV data needs to be ingested and analyzed, while maintaining associated metadata for each row.</p>"},{"location":"Indox/data_loader/Csv/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install indox\n</code></pre>"},{"location":"Indox/data_loader/Csv/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Csv</code> Function</li> </ol> <pre><code>from your_module import Csv\n</code></pre> <ol> <li> <p>Load and Parse a CSV File</p> </li> <li> <p>Call the <code>Csv</code> function with the file path to your CSV file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/csv_file.csv'\ndocuments = Csv(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the row content and metadata of the CSV file.</p> </li> </ol> <pre><code>document = documents[0]\nprint(document.metadata)\nprint(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Csv/#function_csv","title":"Function <code>Csv</code>","text":""},{"location":"Indox/data_loader/Csv/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the CSV file to be loaded.</li> <li><code>metadata</code> (Dict[str, Any], optional): Additional metadata to include in each <code>Document</code>. Default is <code>None</code>.</li> </ul>"},{"location":"Indox/data_loader/Csv/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the CSV rows and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Csv/#notes","title":"Notes","text":"<ul> <li>Metadata can be customized via the <code>metadata</code> parameter and will be included in each <code>Document</code> object along with the CSV row content.</li> <li>The metadata dictionary is initialized with the absolute file path and a default page number of <code>1</code>.</li> </ul>"},{"location":"Indox/data_loader/Csv/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the specified file does not exist.</li> <li>Raises <code>UnicodeDecodeError</code> if there is an error decoding the CSV file.</li> <li>Raises <code>RuntimeError</code> for any other errors encountered during CSV processing or document creation.</li> </ul>"},{"location":"Indox/data_loader/Csv/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Csv\n\n# Load and parse the CSV file\nfile_path = 'data.csv'\ndocuments = Csv(file_path)\n\n# Access the document's metadata and content\ndocument = documents[0]\nprint(document.metadata)  # Prints metadata like file path and custom key\nprint(document.page_content)  # Prints the content of the CSV row\n</code></pre>"},{"location":"Indox/data_loader/Csv/#notes_1","title":"Notes","text":"<ul> <li>Ensure the CSV file exists and is accessible at the provided <code>file_path</code>.</li> <li>The metadata includes the absolute file path and a default page number of <code>1</code>.</li> </ul>"},{"location":"Indox/data_loader/Docx/","title":"Docx","text":""},{"location":"Indox/data_loader/Docx/#docx_loader","title":"Docx Loader","text":""},{"location":"Indox/data_loader/Docx/#overview","title":"Overview","text":"<p>The <code>Docx</code> function is designed to load and parse a DOCX file, extracting its text content and estimating page numbers based on paragraph counts. Each page's content is represented as a <code>Document</code> object with associated metadata.</p>"},{"location":"Indox/data_loader/Docx/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install python-docx indox\n</code></pre>"},{"location":"Indox/data_loader/Docx/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Docx</code> Function</li> </ol> <pre><code>from your_module import Docx\n</code></pre> <ol> <li> <p>Load and Parse a DOCX File</p> </li> <li> <p>Call the <code>Docx</code> function with the file path to your DOCX file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/document.docx'\ndocuments = Docx(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a page and metadata.</p> </li> </ol> <pre><code>document = documents[0]\nprint(document.metadata)\nprint(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Docx/#function_docx","title":"Function <code>Docx</code>","text":""},{"location":"Indox/data_loader/Docx/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the DOCX file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Docx/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing text content and associated metadata, including estimated page numbers.</li> </ul>"},{"location":"Indox/data_loader/Docx/#notes","title":"Notes","text":"<ul> <li>The number of pages is estimated based on a fixed number of paragraphs per page (<code>20</code> in this case). Adjust <code>paragraphs_per_page</code> if a different estimate is required.</li> <li>Metadata includes the absolute file path and the estimated page number for each <code>Document</code>.</li> </ul>"},{"location":"Indox/data_loader/Docx/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error in loading or processing the DOCX file.</li> </ul>"},{"location":"Indox/data_loader/Docx/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Docx\n\n# Load and parse the DOCX file\nfile_path = 'document.docx'\ndocuments = Docx(file_path)\n\n# Access the document's metadata and content\ndocument = documents[0]\nprint(document.metadata)  # Prints metadata like file path and page number\nprint(document.page_content)  # Prints the text content of the page\n</code></pre>"},{"location":"Indox/data_loader/Docx/#notes_1","title":"Notes","text":"<ul> <li>Ensure the DOCX file exists and is accessible at the provided <code>file_path</code>.</li> <li>The estimated page numbers are based on paragraph counts and may not reflect actual pagination.</li> </ul>"},{"location":"Indox/data_loader/Joblib/","title":"Joblib","text":""},{"location":"Indox/data_loader/Joblib/#joblib_pkl_loader","title":"Joblib PKL Loader","text":""},{"location":"Indox/data_loader/Joblib/#overview","title":"Overview","text":"<p>The <code>Joblib</code> function loads a Pickle (<code>.pkl</code>) or Joblib file, extracts its content, and returns it as a <code>Document</code> object with associated metadata. This function is useful for processing serialized data files where the content is stored in a Pickle or Joblib format.</p>"},{"location":"Indox/data_loader/Joblib/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install joblib indox\n</code></pre>"},{"location":"Indox/data_loader/Joblib/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Joblib</code> Function</li> </ol> <pre><code>from your_module import Joblib\n</code></pre> <ol> <li> <p>Load and Extract Content from a Pickle or Joblib File</p> </li> <li> <p>Call the <code>Joblib</code> function with the file path to your Pickle or Joblib file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.pkl'\ndocuments = Joblib(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list containing a <code>Document</code> object with the unpickled content and metadata.</p> </li> </ol> <pre><code>document = documents[0]\nprint(document.metadata)\nprint(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Joblib/#function_joblib","title":"Function <code>Joblib</code>","text":""},{"location":"Indox/data_loader/Joblib/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the Pickle or Joblib file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Joblib/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list containing a <code>Document</code> object with the unpickled content and metadata.</li> </ul>"},{"location":"Indox/data_loader/Joblib/#notes","title":"Notes","text":"<ul> <li>Metadata includes the absolute file path and a default page number of <code>1</code>.</li> <li>The content is converted to a string before being stored in the <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Joblib/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error loading the file or processing its content.</li> </ul>"},{"location":"Indox/data_loader/Joblib/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Joblib\n\n# Load and extract content from the Pickle or Joblib file\nfile_path = 'data.pkl'\ndocuments = Joblib(file_path)\n\n# Access the document's metadata and content\ndocument = documents[0]\nprint(document.metadata)  # Prints metadata like file path and page number\nprint(document.page_content)  # Prints the unpickled content of the file\n</code></pre>"},{"location":"Indox/data_loader/Joblib/#notes_1","title":"Notes","text":"<ul> <li>Ensure the Pickle or Joblib file exists and is accessible at the provided <code>file_path</code>.</li> <li>The content is converted to a string representation, which may affect data types depending on the original content.</li> </ul>"},{"location":"Indox/data_loader/Json/","title":"Json","text":""},{"location":"Indox/data_loader/Json/#json_file_loader","title":"Json File Loader","text":""},{"location":"Indox/data_loader/Json/#overview","title":"Overview","text":"<p>The <code>Json</code> function loads a JSON file and converts its key-value pairs into a list of <code>Document</code> objects. Each key-value pair from the JSON data is stored as a separate <code>Document</code> object with associated metadata.</p>"},{"location":"Indox/data_loader/Json/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install indox\n</code></pre>"},{"location":"Indox/data_loader/Json/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Json</code> Function</li> </ol> <pre><code>from your_module import Json\n</code></pre> <ol> <li> <p>Load and Parse a JSON File</p> </li> <li> <p>Call the <code>Json</code> function with the file path to your JSON file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.json'\ndocuments = Json(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing a JSON key-value pair and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Json/#function_json","title":"Function <code>Json</code>","text":""},{"location":"Indox/data_loader/Json/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the JSON file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Json/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing a JSON key-value pair and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Json/#notes","title":"Notes","text":"<ul> <li>Metadata includes the absolute file path (<code>source</code>) and the number of entries in the JSON data (<code>num_entries</code>).</li> <li>Each JSON key-value pair is stored as a string in a separate <code>Document</code> object.</li> <li>The key is also included in the metadata for each <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Json/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error loading the JSON file or processing its content.</li> </ul>"},{"location":"Indox/data_loader/Json/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Json\n\n# Load and parse the JSON file\nfile_path = 'data.json'\ndocuments = Json(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and key\n    print(document.page_content)  # Prints the JSON key-value pair\n</code></pre>"},{"location":"Indox/data_loader/Json/#notes_1","title":"Notes","text":"<ul> <li>Ensure the JSON file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>page_content</code> in each <code>Document</code> object is a string representation of the JSON key-value pair.</li> </ul>"},{"location":"Indox/data_loader/MD/","title":"MD","text":""},{"location":"Indox/data_loader/MD/#md_file_loader","title":"Md File Loader","text":""},{"location":"Indox/data_loader/MD/#overview","title":"Overview","text":"<p>The <code>Md</code> function loads a Markdown file, extracts its text content, and returns it as a <code>Document</code> object with associated metadata. This function is useful for processing Markdown documents, making their content easily accessible for further analysis or storage.</p>"},{"location":"Indox/data_loader/MD/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install indox\n</code></pre>"},{"location":"Indox/data_loader/MD/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Md</code> Function</li> </ol> <pre><code>from your_module import Md\n</code></pre> <ol> <li> <p>Load and Extract Content from a Markdown File</p> </li> <li> <p>Call the <code>Md</code> function with the file path to your Markdown file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.md'\ndocuments = Md(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list containing a <code>Document</code> object with the Markdown file's content and metadata.</p> </li> </ol> <pre><code>document = documents[0]\nprint(document.metadata)\nprint(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/MD/#function_md","title":"Function <code>Md</code>","text":""},{"location":"Indox/data_loader/MD/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the Markdown file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/MD/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list containing a <code>Document</code> object with the text content of the Markdown file and metadata.</li> </ul>"},{"location":"Indox/data_loader/MD/#notes","title":"Notes","text":"<ul> <li>Metadata includes the absolute file path (<code>source</code>) and a fixed page number (<code>1</code>).</li> <li>The text content is read directly from the Markdown file without any processing.</li> </ul>"},{"location":"Indox/data_loader/MD/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error loading the Markdown file or processing its content.</li> </ul>"},{"location":"Indox/data_loader/MD/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Md\n\n# Load and extract content from the Markdown file\nfile_path = 'document.md'\ndocuments = Md(file_path)\n\n# Access and print the document's metadata and content\ndocument = documents[0]\nprint(document.metadata)  # Prints metadata including file path and page number\nprint(document.page_content)  # Prints the text content of the Markdown file\n</code></pre>"},{"location":"Indox/data_loader/MD/#notes_1","title":"Notes","text":"<ul> <li>Ensure the Markdown file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>page</code> metadata is set to <code>1</code> by default and may be adjusted if necessary.</li> </ul>"},{"location":"Indox/data_loader/Openpyxl/","title":"Openpyxl","text":""},{"location":"Indox/data_loader/Openpyxl/#openpyxl_excel_file_loader","title":"OpenPyXl Excel File Loader","text":""},{"location":"Indox/data_loader/Openpyxl/#overview","title":"Overview","text":"<p>The <code>OpenPyXl</code> function loads an Excel file and extracts its data from each sheet, converting it into a list of <code>Document</code> objects. Each sheet's data is stored in a separate <code>Document</code> object with associated metadata. This function is useful for processing and analyzing data stored in Excel files.</p>"},{"location":"Indox/data_loader/Openpyxl/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install pandas openpyxl indox\n</code></pre>"},{"location":"Indox/data_loader/Openpyxl/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>OpenPyXl</code> Function</li> </ol> <pre><code>from your_module import OpenPyXl\n</code></pre> <ol> <li> <p>Load and Extract Content from an Excel File</p> </li> <li> <p>Call the <code>OpenPyXl</code> function with the file path to your Excel file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.xlsx'\ndocuments = OpenPyXl(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing data from an Excel sheet and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Openpyxl/#function_openpyxl","title":"Function <code>OpenPyXl</code>","text":""},{"location":"Indox/data_loader/Openpyxl/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the Excel file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Openpyxl/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of an Excel sheet and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Openpyxl/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the Excel file.</li> <li><code>page</code>: Set to <code>1</code> for all documents (not specific to individual sheets).</li> <li>Each sheet in the Excel file is loaded into a separate <code>Document</code> object.</li> <li>The data in each sheet is converted to a string using <code>pandas.DataFrame.to_string()</code> for storage in the <code>Document</code> object.</li> <li>The <code>sheet_name</code> is included in the metadata of each <code>Document</code>.</li> </ul>"},{"location":"Indox/data_loader/Openpyxl/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error loading the Excel file or processing its content.</li> </ul>"},{"location":"Indox/data_loader/Openpyxl/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import OpenPyXl\n\n# Load and extract content from the Excel file\nfile_path = 'data.xlsx'\ndocuments = OpenPyXl(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and sheet name\n    print(document.page_content)  # Prints the text content of the sheet\n</code></pre>"},{"location":"Indox/data_loader/Openpyxl/#notes_1","title":"Notes","text":"<ul> <li>Ensure the Excel file exists and is accessible at the provided <code>file_path</code>.</li> <li>The metadata <code>page</code> is set to <code>1</code> for all documents, and you might want to modify this if different handling for pages is needed.</li> </ul>"},{"location":"Indox/data_loader/Pdfminer/","title":"Pdfminer","text":""},{"location":"Indox/data_loader/Pdfminer/#pdfminer_pdf_file_loader","title":"PdfMiner PDF File Loader","text":""},{"location":"Indox/data_loader/Pdfminer/#overview","title":"Overview","text":"<p>The <code>PdfMiner</code> function loads a PDF file and extracts its text content from each page using the <code>pdfminer</code> library. Each page's text is stored in a separate <code>Document</code> object with associated metadata. This function is useful for handling and analyzing text data from PDF documents.</p>"},{"location":"Indox/data_loader/Pdfminer/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install pdfminer.six indox\n</code></pre>"},{"location":"Indox/data_loader/Pdfminer/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>PdfMiner</code> Function</li> </ol> <pre><code>from your_module import PdfMiner\n</code></pre> <ol> <li> <p>Load and Extract Content from a PDF File</p> </li> <li> <p>Call the <code>PdfMiner</code> function with the file path to your PDF file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.pdf'\ndocuments = PdfMiner(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a PDF page and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Pdfminer/#function_pdfminer","title":"Function <code>PdfMiner</code>","text":""},{"location":"Indox/data_loader/Pdfminer/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the PDF file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Pdfminer/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of a PDF page and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Pdfminer/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the PDF file.</li> <li><code>page</code>: The page number of the PDF.</li> <li>Each page in the PDF is loaded into a separate <code>Document</code> object.</li> <li>The text content is extracted using <code>pdfminer</code>'s <code>extract_pages</code> function.</li> </ul>"},{"location":"Indox/data_loader/Pdfminer/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the specified file does not exist.</li> <li>Raises <code>PDFSyntaxError</code> if there is a syntax-related issue with the PDF file.</li> <li>Raises <code>RuntimeError</code> for any other errors encountered during PDF processing.</li> </ul>"},{"location":"Indox/data_loader/Pdfminer/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import PdfMiner\n\n# Load and extract content from the PDF file\nfile_path = 'document.pdf'\ndocuments = PdfMiner(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and page number\n    print(document.page_content)  # Prints the text content of the page\n</code></pre>"},{"location":"Indox/data_loader/Pdfminer/#notes_1","title":"Notes","text":"<ul> <li>Ensure the PDF file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>page</code> metadata is set according to the page number in the PDF.</li> </ul>"},{"location":"Indox/data_loader/Pdfplumber/","title":"Pdfplumber","text":""},{"location":"Indox/data_loader/Pdfplumber/#pdfplumber_pdf_file_loader","title":"PdfPlumber PDF File Loader","text":""},{"location":"Indox/data_loader/Pdfplumber/#overview","title":"Overview","text":"<p>The <code>PdfPlumber</code> function loads a PDF file and extracts its text content from each page using the <code>pdfplumber</code> library. Each page's text is stored in a separate <code>Document</code> object with associated metadata. This function is useful for handling and analyzing text data from PDF documents.</p>"},{"location":"Indox/data_loader/Pdfplumber/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install pdfplumber indox\n</code></pre>"},{"location":"Indox/data_loader/Pdfplumber/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>PdfPlumber</code> Function</li> </ol> <pre><code>from your_module import PdfPlumber\n</code></pre> <ol> <li> <p>Load and Extract Content from a PDF File</p> </li> <li> <p>Call the <code>PdfPlumber</code> function with the file path to your PDF file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.pdf'\ndocuments = PdfPlumber(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a PDF page and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Pdfplumber/#function_pdfplumber","title":"Function <code>PdfPlumber</code>","text":""},{"location":"Indox/data_loader/Pdfplumber/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the PDF file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Pdfplumber/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of a PDF page and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Pdfplumber/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the PDF file.</li> <li><code>page</code>: The page number of the PDF.</li> <li>Each page in the PDF is loaded into a separate <code>Document</code> object.</li> <li>The text content is extracted using <code>pdfplumber</code>'s <code>extract_text</code> function.</li> </ul>"},{"location":"Indox/data_loader/Pdfplumber/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the specified file does not exist.</li> <li>Raises <code>RuntimeError</code> if there is an error reading the PDF file or extracting text.</li> </ul>"},{"location":"Indox/data_loader/Pdfplumber/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import PdfPlumber\n\n# Load and extract content from the PDF file\nfile_path = 'document.pdf'\ndocuments = PdfPlumber(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and page number\n    print(document.page_content)  # Prints the text content of the page\n</code></pre>"},{"location":"Indox/data_loader/Pdfplumber/#notes_1","title":"Notes","text":"<ul> <li>Ensure the PDF file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>page</code> metadata is set according to the page number in the PDF.</li> </ul>"},{"location":"Indox/data_loader/Pptx/","title":"Pptx","text":""},{"location":"Indox/data_loader/Pptx/#pptx_powerpoint_file_loader","title":"Pptx PowerPoint File Loader","text":""},{"location":"Indox/data_loader/Pptx/#overview","title":"Overview","text":"<p>The <code>Pptx</code> function loads a PowerPoint (.pptx) file and extracts its text content from each slide. Each slide's text is stored in a separate <code>Document</code> object with associated metadata. This function is useful for analyzing and processing text data from PowerPoint presentations.</p>"},{"location":"Indox/data_loader/Pptx/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install python-pptx indox\n</code></pre>"},{"location":"Indox/data_loader/Pptx/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Pptx</code> Function</li> </ol> <pre><code>from your_module import Pptx\n</code></pre> <ol> <li> <p>Load and Extract Content from a PowerPoint File</p> </li> <li> <p>Call the <code>Pptx</code> function with the file path to your PowerPoint file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/presentation.pptx'\ndocuments = Pptx(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a slide and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Pptx/#function_pptx","title":"Function <code>Pptx</code>","text":""},{"location":"Indox/data_loader/Pptx/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the PowerPoint file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Pptx/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of a slide and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Pptx/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the PowerPoint file.</li> <li><code>num_slides</code>: The total number of slides in the presentation.</li> <li><code>slide_number</code>: The slide number (1-based) within the presentation.</li> <li>Each slide's text is extracted from its shapes and stored in a separate <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Pptx/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an error loading the PowerPoint file.</li> </ul>"},{"location":"Indox/data_loader/Pptx/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Pptx\n\n# Load and extract content from the PowerPoint file\nfile_path = 'presentation.pptx'\ndocuments = Pptx(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and slide number\n    print(document.page_content)  # Prints the text content of the slide\n</code></pre>"},{"location":"Indox/data_loader/Pptx/#notes_1","title":"Notes","text":"<ul> <li>Ensure the PowerPoint file exists and is accessible at the provided <code>file_path</code>.</li> <li>Metadata is generated for each slide, including the slide number and total number of slides.</li> </ul>"},{"location":"Indox/data_loader/Pypdf2/","title":"Pypdf2","text":""},{"location":"Indox/data_loader/Pypdf2/#pypdf2_pdf_loader","title":"PyPdf2 PDF Loader","text":""},{"location":"Indox/data_loader/Pypdf2/#overview","title":"Overview","text":"<p>The <code>PyPdf2</code> function loads a PDF file and extracts text content from each page using the <code>PyPDF2</code> library. Each page's text is stored in a separate <code>Document</code> object along with relevant metadata. This function is useful for extracting and processing text data from PDF documents.</p>"},{"location":"Indox/data_loader/Pypdf2/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install PyPDF2 indox\n</code></pre>"},{"location":"Indox/data_loader/Pypdf2/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>PyPdf2</code> Function</li> </ol> <pre><code>from your_module import PyPdf2\n</code></pre> <ol> <li> <p>Load and Extract Content from a PDF File</p> </li> <li> <p>Call the <code>PyPdf2</code> function with the path to your PDF file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/document.pdf'\ndocuments = PyPdf2(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a page and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Pypdf2/#function_pypdf2","title":"Function <code>PyPdf2</code>","text":""},{"location":"Indox/data_loader/Pypdf2/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the PDF file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Pypdf2/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of a page and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Pypdf2/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the PDF file.</li> <li><code>page</code>: The page number (0-based index) within the PDF file.</li> <li>Text is extracted using <code>PyPDF2.PdfReader</code>, and each page's text is stored in a separate <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Pypdf2/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the file does not exist at the specified path.</li> <li>Raises <code>RuntimeError</code> if there is an issue reading the PDF file or extracting text from any page.</li> </ul>"},{"location":"Indox/data_loader/Pypdf2/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import PyPdf2\n\n# Load and extract content from the PDF file\nfile_path = 'document.pdf'\ndocuments = PyPdf2(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and page number\n    print(document.page_content)  # Prints the text content of the page\n</code></pre>"},{"location":"Indox/data_loader/Pypdf2/#notes_1","title":"Notes","text":"<ul> <li>Ensure that the PDF file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>PyPdf2</code> function handles the extraction of text from each page of the PDF and collects metadata related to the page and source file.</li> </ul>"},{"location":"Indox/data_loader/Pypdf4/","title":"Pypdf4","text":""},{"location":"Indox/data_loader/Pypdf4/#pypdf4_pdf_loader","title":"PyPdf4 PDF Loader","text":""},{"location":"Indox/data_loader/Pypdf4/#overview","title":"Overview","text":"<p>The <code>PyPdf4</code> function uses the <code>PyPDF4</code> library to read and extract text from a PDF file. It processes each page of the PDF, constructs a <code>Document</code> object for each page, and includes relevant metadata. This function is useful for text extraction and processing from PDF documents.</p>"},{"location":"Indox/data_loader/Pypdf4/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install PyPDF4 indox\n</code></pre>"},{"location":"Indox/data_loader/Pypdf4/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>PyPdf4</code> Function</li> </ol> <pre><code>from your_module import PyPdf4\n</code></pre> <ol> <li> <p>Load and Extract Content from a PDF File</p> </li> <li> <p>Call the <code>PyPdf4</code> function with the path to your PDF file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/document.pdf'\ndocuments = PyPdf4(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects, each containing the text content of a page and associated metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Pypdf4/#function_pypdf4","title":"Function <code>PyPdf4</code>","text":""},{"location":"Indox/data_loader/Pypdf4/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the PDF file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Pypdf4/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of a page and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Pypdf4/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the PDF file.</li> <li><code>page</code>: The page number (0-based index) within the PDF file.</li> <li>Text is extracted using <code>PyPDF4.PdfFileReader</code>, and each page's text is stored in a separate <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Pypdf4/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>FileNotFoundError</code> if the file does not exist at the specified path.</li> <li>Raises <code>RuntimeError</code> if there is an issue reading the PDF file or extracting text from any page.</li> </ul>"},{"location":"Indox/data_loader/Pypdf4/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import PyPdf4\n\n# Load and extract content from the PDF file\nfile_path = 'document.pdf'\ndocuments = PyPdf4(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and page number\n    print(document.page_content)  # Prints the text content of the page\n</code></pre>"},{"location":"Indox/data_loader/Pypdf4/#notes_1","title":"Notes","text":"<ul> <li>Ensure that the PDF file exists and is accessible at the provided <code>file_path</code>.</li> <li>The <code>PyPdf4</code> function handles the extraction of text from each page of the PDF and collects metadata related to the page and source file.</li> </ul>"},{"location":"Indox/data_loader/Rtf/","title":"Rtf","text":""},{"location":"Indox/data_loader/Rtf/#rtf_file_loader","title":"Rtf File Loader","text":""},{"location":"Indox/data_loader/Rtf/#overview","title":"Overview","text":"<p>The <code>Rtf</code> function utilizes the <code>pyth</code> library to process and extract text from RTF files. It reads the RTF file, converts its content into plain text, and packages this text along with metadata into <code>Document</code> objects.</p>"},{"location":"Indox/data_loader/Rtf/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install pyth indox\n</code></pre>"},{"location":"Indox/data_loader/Rtf/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Rtf</code> Function</li> </ol> <pre><code>from your_module import Rtf\n</code></pre> <ol> <li> <p>Load and Extract Content from an RTF File</p> </li> <li> <p>Call the <code>Rtf</code> function with the path to your RTF file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/document.rtf'\ndocuments = Rtf(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects containing the text content and metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Rtf/#function_rtf","title":"Function <code>Rtf</code>","text":""},{"location":"Indox/data_loader/Rtf/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the RTF file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Rtf/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list containing a <code>Document</code> object with the text content of the RTF file and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Rtf/#notes","title":"Notes","text":"<ul> <li>Metadata includes:</li> <li><code>source</code>: The absolute file path of the RTF file.</li> <li><code>page</code>: A fixed page number set to <code>1</code> as RTF files do not inherently have page numbers.</li> <li>The text is extracted using <code>pyth.plugins.rtf15.reader</code> and converted to plain text using <code>pyth.plugins.plaintext.writer</code>.</li> </ul>"},{"location":"Indox/data_loader/Rtf/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Rtf\n\n# Load and extract content from the RTF file\nfile_path = 'document.rtf'\ndocuments = Rtf(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and page number\n    print(document.page_content)  # Prints the text content of the RTF file\n</code></pre>"},{"location":"Indox/data_loader/Rtf/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an issue reading the RTF file or extracting text from it.</li> </ul>"},{"location":"Indox/data_loader/Scipy/","title":"Scipy","text":""},{"location":"Indox/data_loader/Scipy/#scipy_file_loader","title":"Scipy File Loader","text":""},{"location":"Indox/data_loader/Scipy/#overview","title":"Overview","text":"<p>The <code>Scipy</code> function reads a MATLAB <code>.mat</code> file, extracts its data, and stores it in <code>Document</code> objects. The MATLAB-specific metadata is excluded, and the data variables are included as text content.</p>"},{"location":"Indox/data_loader/Scipy/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install scipy indox\n</code></pre>"},{"location":"Indox/data_loader/Scipy/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Scipy</code> Function</li> </ol> <pre><code>from your_module import Scipy\n</code></pre> <ol> <li> <p>Load and Extract Content from a <code>.mat</code> File</p> </li> <li> <p>Call the <code>Scipy</code> function with the path to your <code>.mat</code> file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/document.mat'\ndocuments = Scipy(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects containing the data variables and metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Scipy/#function_scipy","title":"Function <code>Scipy</code>","text":""},{"location":"Indox/data_loader/Scipy/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the <code>.mat</code> file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Scipy/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the data variables from the <code>.mat</code> file as text content and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Scipy/#notes","title":"Notes","text":"<ul> <li>Metadata:</li> <li><code>source</code>: The absolute file path of the <code>.mat</code> file.</li> <li> <p><code>page</code>: An index corresponding to each variable in the <code>.mat</code> file, starting from <code>0</code>.</p> </li> <li> <p>MATLAB-Specific Metadata:</p> </li> <li> <p>The <code>__header__</code>, <code>__version__</code>, and <code>__globals__</code> fields are removed from the data before processing.</p> </li> <li> <p>Data Handling:</p> </li> <li>Data variables are converted to strings and included as text content in <code>Document</code> objects.</li> </ul>"},{"location":"Indox/data_loader/Scipy/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Scipy\n\n# Load and extract content from the .mat file\nfile_path = 'document.mat'\ndocuments = Scipy(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including file path and variable index\n    print(document.page_content)  # Prints the content of the variable from the .mat file\n</code></pre>"},{"location":"Indox/data_loader/Scipy/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an issue loading the <code>.mat</code> file or processing its contents.</li> </ul>"},{"location":"Indox/data_loader/Sql/","title":"Sql","text":""},{"location":"Indox/data_loader/Sql/#sql_file_loader","title":"SQL File Loader","text":""},{"location":"Indox/data_loader/Sql/#overview","title":"Overview","text":"<p>The <code>Sql</code> function reads an SQL file, extracts its text content, and stores it in <code>Document</code> objects. Metadata includes basic file information and content statistics.</p>"},{"location":"Indox/data_loader/Sql/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install indox\n</code></pre>"},{"location":"Indox/data_loader/Sql/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Sql</code> Function</li> </ol> <pre><code>from your_module import Sql\n</code></pre> <ol> <li> <p>Load and Extract Content from an SQL File</p> </li> <li> <p>Call the <code>Sql</code> function with the path to your SQL file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.sql'\ndocuments = Sql(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects containing the SQL content and metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Sql/#function_sql","title":"Function <code>Sql</code>","text":""},{"location":"Indox/data_loader/Sql/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the SQL file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Sql/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list of <code>Document</code> objects, each containing the text content of the SQL file and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Sql/#notes","title":"Notes","text":"<ul> <li>Metadata:</li> <li><code>source</code>: The base name of the SQL file (i.e., the file name without directory path).</li> <li> <p><code>page</code>: A fixed value of <code>1</code>, indicating a single document representing the entire file.</p> </li> <li> <p>Content Handling:</p> </li> <li>The entire SQL file is read as a single string and included as text content in the <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Sql/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Sql\n\n# Load and extract content from the SQL file\nfile_path = 'file.sql'\ndocuments = Sql(file_path)\n\n# Access and print each document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including the file name\n    print(document.page_content)  # Prints the entire SQL file content\n</code></pre>"},{"location":"Indox/data_loader/Sql/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an issue loading the SQL file or processing its contents.</li> </ul>"},{"location":"Indox/data_loader/Txt/","title":"Txt","text":""},{"location":"Indox/data_loader/Txt/#text_file_loader","title":"Text File Loader","text":""},{"location":"Indox/data_loader/Txt/#overview","title":"Overview","text":"<p>The <code>Txt</code> function reads a text file, extracts its content, and stores it in a <code>Document</code> object with relevant metadata.</p>"},{"location":"Indox/data_loader/Txt/#installation","title":"Installation","text":"<p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install indox\n</code></pre>"},{"location":"Indox/data_loader/Txt/#quick_start","title":"Quick Start","text":"<ol> <li>Import the <code>Txt</code> Function</li> </ol> <pre><code>from your_module import Txt\n</code></pre> <ol> <li> <p>Load and Extract Content from a Text File</p> </li> <li> <p>Call the <code>Txt</code> function with the path to your text file.</p> </li> </ol> <pre><code>file_path = 'path/to/your/file.txt'\ndocuments = Txt(file_path)\n</code></pre> <ol> <li> <p>Access the Document Content and Metadata</p> </li> <li> <p>The function returns a list of <code>Document</code> objects containing the text content and metadata.</p> </li> </ol> <pre><code>for document in documents:\n    print(document.metadata)\n    print(document.page_content)\n</code></pre>"},{"location":"Indox/data_loader/Txt/#function_txt","title":"Function <code>Txt</code>","text":""},{"location":"Indox/data_loader/Txt/#parameters","title":"Parameters","text":"<ul> <li><code>file_path</code> (str): The path to the text file to be loaded.</li> </ul>"},{"location":"Indox/data_loader/Txt/#returns","title":"Returns","text":"<ul> <li><code>List[Document]</code>: A list containing a single <code>Document</code> object with the text content of the file and associated metadata.</li> </ul>"},{"location":"Indox/data_loader/Txt/#notes","title":"Notes","text":"<ul> <li>Metadata:</li> <li><code>source</code>: The base name of the text file (i.e., the file name without directory path).</li> <li> <p><code>page</code>: A fixed value of <code>1</code>, indicating a single document representing the entire file.</p> </li> <li> <p>Content Handling:</p> </li> <li>The entire text file is read as a single string and included as text content in the <code>Document</code> object.</li> </ul>"},{"location":"Indox/data_loader/Txt/#example_usage","title":"Example Usage","text":"<pre><code>from your_module import Txt\n\n# Load and extract content from the text file\nfile_path = 'file.txt'\ndocuments = Txt(file_path)\n\n# Access and print the document's metadata and content\nfor document in documents:\n    print(document.metadata)  # Prints metadata including the file name\n    print(document.page_content)  # Prints the entire text file content\n</code></pre>"},{"location":"Indox/data_loader/Txt/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>RuntimeError</code> if there is an issue loading the text file or processing its contents.</li> </ul>"},{"location":"Indox/data_loader_and_splitter/clustered_split/","title":"ClusteredSplit","text":"<p>The <code>ClusteredSplit</code> function creates leaf chunks from the text and adds extra clustered chunks to these leaf chunks. The clustering continues until no new clusters are available, growing like a tree: starting from leaf chunks, then clustering between the last clustered chunks, and so on.</p> <pre><code>def __init__(self, file_path: str, embeddings, re_chunk: bool = False, remove_sword: bool = False,\n             chunk_size: Optional[int] = 100, overlap: Optional[int] = 0, threshold: float = 0.1, dim: int = 10,\n             use_openai_summary: bool = False, max_len_summary: int = 100, min_len_summary: int = 30)\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/clustered_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the plain text file or PDF file to be processed.</li> <li>embeddings: The embeddings to be used for clustering.</li> <li>re_chunk (bool): If True, re-chunk the text after initial chunking. Default is False.</li> <li>remove_sword (bool): If True, remove stopwords during the chunking process. Default is False.</li> <li>chunk_size (Optional[int]): The size of each chunk in characters. Default is 100.</li> <li>overlap (Optional[int]): The number of characters to overlap between chunks. Default is 0.</li> <li>threshold (float): The similarity threshold for creating clusters. Default is 0.1.</li> <li>dim (int): The dimensionality of the embeddings. Default is 10.</li> <li>use_openai_summary (bool, optional): Whether to use OpenAI summary for summarizing the chunks. Default is False.</li> <li>max_len_summary (int, optional): The maximum length of the summary. Default is 100.</li> <li>min_len_summary (int, optional): The minimum length of the summary. Default is 30.</li> </ul>"},{"location":"Indox/data_loader_and_splitter/clustered_split/#usage","title":"Usage","text":"<p>To use the ClusteredSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize Indox and QA models:</p> <pre><code>from indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom Indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the clustered split on the text file or PDF file:</p> <pre><code>from indox.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/clustered_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the ClusteredSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indox.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path,\n                        embeddings=openai_qa.embeddings,\n                        re_chunk=False,\n                        remove_sword=False,\n                        chunk_size=100,\n                        overlap=0,\n                        threshold=0.1,\n                        dim=10)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <p>This will process the specified file and return all chunks with the extra clustered layers, forming a hierarchical structure of text chunks.</p>"},{"location":"Indox/data_loader_and_splitter/simple_load_split/","title":"SimpleLoadAndSplit","text":"<p>The <code>SimpleLoadAndSplit</code> function accept both PDF and text files and create chunks based on a semantic text splitter</p> <pre><code>def __init__(self, file_path: str, remove_sword: bool = False,\n             max_chunk_size: Optional[int] = 500, )\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/simple_load_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the plain text file or PDF file to be processed.</li> <li>remove_sword (bool): If True, remove stopwords during the chunking process. Default is False.</li> <li>max_chunk_size (Optional[int]): The maximum size of each chunk in characters. Default is 500.</li> </ul>"},{"location":"Indox/data_loader_and_splitter/simple_load_split/#usage","title":"Usage","text":"<p>To use the SimpleLoadAndSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize Indox and QA models:</p> <pre><code>from indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom Indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the clustered split on the text file or PDF file:</p> <pre><code>from indox.data_loader_splitter import SimpleLoadAndSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = SimpleLoadAndSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/simple_load_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the ClusteredSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indox.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <p>This will process the specified file and return all chunks with the extra clustered layers, forming a hierarchical structure of text chunks.</p>"},{"location":"Indox/data_loader_and_splitter/unstructured_load_and_split/","title":"Unstructured Load and Split","text":""},{"location":"Indox/data_loader_and_splitter/unstructured_load_and_split/#unstructuredloadandsplit","title":"UnstructuredLoadAndSplit","text":"<p>The UnstructuredLoadAndSplit function uses the unstructured library to import various file types and split them into chunks. By default, it uses the \u201csplit by title\u201d method from the unstructured library, but users can also choose the semantic_text_splitter.</p> <pre><code>def UnstructuredLoadAndSplit(file_path: str,\n                             remove_sword: bool = False,\n                             max_chunk_size: int = 500,\n                             splitter=None)\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/unstructured_load_and_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the file to be processed. Various file   types are supported.</li> <li>remove_sword (bool): If True, remove stop words during the chunking   process. Default is False.</li> <li>max_chunk_size (int): The maximum size of each chunk in characters.   Default is 500.</li> <li>splitter: The method used to split the text. The default is \u201csplit   by title\u201d from the unstructured library. Users can also choose   semantic_text_splitter.</li> </ul>"},{"location":"Indox/data_loader_and_splitter/unstructured_load_and_split/#usage","title":"Usage","text":"<p>To use the UnstructuredLoadAndSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize Indox and QA models:</p> <pre><code>from indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom Indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the unstructured load and split on the file:</p> <pre><code>from indox.data_loader_splitter import UnstructuredLoadAndSplit\nfrom Indox.splitter import semantic_text_splitter\n\nfile_path = \"path/to/your/file.pdf\"  # Specify the file path\nloader_splitter = UnstructuredLoadAndSplit(file_path=file_path,\n                                remove_sword=False,\n                                max_chunk_size=500,\n                                splitter=semantic_text_splitter)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"Indox/data_loader_and_splitter/unstructured_load_and_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the UnstructuredLoadAndSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indox import IndoxRetrievalAugmentation\nIndox = IndoxRetrievalAugmentation()\n\nfrom Indox.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indox.data_loader_splitter import UnstructuredLoadAndSplit\nfrom indox.splitter import semantic_text_splitter\n\nfile_path = \"path/to/your/file.pdf\"  # Specify the file path\nloader_splitter = UnstructuredLoadAndSplit(file_path=file_path,\n                                remove_sword=False,\n                                max_chunk_size=500,\n                                splitter=semantic_text_splitter)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"Indox/examples/clusterSplit/","title":"Load And Split With Clustering","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n</code></pre>"},{"location":"Indox/examples/clusterSplit/#initial_setup","title":"Initial Setup","text":"<p>The following imports are essential for setting up the Indox application. These imports include the main Indox retrieval augmentation module, question-answering models, embeddings, and data loader splitter.</p> <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import OpenAiQA\nfrom indox.embeddings import OpenAiEmbedding\nfrom indox.data_loader_splitter import ClusteredSplit\n</code></pre> <p>In this step, we initialize the Indox Retrieval Augmentation, the QA model, and the embedding model. Note that the models used for QA and embedding can vary depending on the specific requirements.</p> <pre><code>Indox = IndoxRetrievalAugmentation()\nqa_model = OpenAiQA(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo-0125\")\nembed = OpenAiEmbedding(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n</code></pre>"},{"location":"Indox/examples/clusterSplit/#data_loader_setup","title":"Data Loader Setup","text":"<p>We set up the data loader using the <code>ClusteredSplit</code> class. This step involves loading documents, configuring embeddings, and setting options for processing the text.</p> <pre><code>loader_splitter = ClusteredSplit(file_path=\"sample.txt\",embeddings=embed,remove_sword=False,re_chunk=False,chunk_size=300,use_openai_summary=True)\n</code></pre> <pre><code>docs = loader_splitter.load_and_chunk()\n</code></pre> <pre><code>--Generated 1 clusters--\n</code></pre>"},{"location":"Indox/examples/clusterSplit/#vector_store_connection_and_document_storage","title":"Vector Store Connection and Document Storage","text":"<p>In this step, we connect the Indox application to the vector store and store the processed documents.</p> <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed)\n</code></pre> <pre><code>Indox.connect_to_vectorstore(db)\n</code></pre> <pre><code>&lt;indox.vector_stores.Chroma.ChromaVectorStore at 0x215d56a6c00&gt;\n</code></pre> <pre><code>Indox.store_in_vectorstore(docs)\n</code></pre> <pre><code>&lt;indox.vector_stores.Chroma.ChromaVectorStore at 0x215d56a6c00&gt;\n</code></pre>"},{"location":"Indox/examples/clusterSplit/#querying_and_interpreting_the_response","title":"Querying and Interpreting the Response","text":"<p>In this step, we query the Indox application with a specific question and use the QA model to get the response. The response is a tuple where the first element is the answer and the second element contains the retrieved context with their cosine scores. response[0] contains the answer response[1] contains the retrieved context with their cosine scores</p> <pre><code>retriever = Indox.QuestionAnswer(vector_database=db,llm=qa_model,top_k=5)\n</code></pre> <pre><code>retriever.invoke(query=\"How cinderella reach happy ending?\")\n</code></pre> <pre><code>\"Cinderella reached her happy ending through her kindness, perseverance, and the magical assistance she received. Despite being mistreated by her stepmother and stepsisters, Cinderella remained kind and pure of heart. With the help of a little bird and her mother's grave, she was able to attend the royal ball where the prince fell in love with her. Even though she had to escape from the prince, he searched for her and eventually found her with the help of the golden slipper she left behind. The prince then declared that he would only marry the woman whose foot fit the golden slipper, leading to Cinderella's ultimate happy ending as she was the only one whose foot fit the slipper.\"\n</code></pre> <pre><code>retriever.context\n</code></pre> <pre><code>[\"They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes   The prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the pigeon-house   The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house   The old man thought, can it be cinderella   And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it   And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown\",\n \"The documentation provided describes the classic fairy tale of Cinderella. It tells the story of a young girl, Cinderella, who is mistreated by her stepmother and stepsisters after her mother's death. Despite their cruelty, Cinderella remains kind and pure of heart. Through magical assistance from a little bird and her mother's grave, Cinderella is able to attend the royal ball where the prince falls in love with her. When she escapes, the prince searches for her and finally finds her with the\",\n \"had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree -      shiver and quiver, my little tree,      silver and gold throw down over me And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden   And when she went to the festival in the dress, no one knew how to speak for astonishment   The king's son danced with her only, and if any one invited her to dance, he said this is my partner When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad,\",\n \"and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again   In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders   And as on that account she always looked dusty and dirty, they called her cinderella It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them Beautiful dresses, said one, pearls and jewels, said the second And you, cinderella, said he, what will you have   Father break off for me the first branch which knocks against your hat on your way home   So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat   Then he broke off the branch and took it with him   When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush   Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it   And it grew and became a handsome tree  Thrice a day cinderella went and sat beneath it, and wept and\",\n \"prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride   When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so   You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival   You have no clothes and shoes, and yet would dance   As, however, cinderella went on asking, the step-mother said at last, I have emptied a dish of lentils into the ashes for you, if you have picked them out again in two hours, you shall go with us   The maiden went through the back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick\"]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Indox/examples/googleai/","title":"google AI","text":""},{"location":"Indox/examples/googleai/#retrieval_augmentation_using_googleai","title":"Retrieval Augmentation Using GoogleAi","text":"<p>Here, we will explore how to work with Indox Retrieval Augmentation. We are using GoogleAi from Indox , we should set our GOOGLE_API_KEY as an environment variable.</p> <pre><code>!pip install -q -U google-generativeai\n!pip install chromadb\n!pip install indox\n</code></pre> <p>:::</p> <p>::: {#initial_id .cell .code execution_count=\"1\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:37:58.399097Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:37:58.386931Z\\\"}\"}</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\nGOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n</code></pre> <p>:::</p> <p>::: {#f5bf0c429718d8a2 .cell .code execution_count=\"13\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:41:10.371425Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:41:10.368186Z\\\"}\"}</p> <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import GoogleAi\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter import ClusteredSplit\n</code></pre> <p>:::</p> <p>::: {#fbd579f0c46d666 .cell .markdown}</p>"},{"location":"Indox/examples/googleai/#creating_an_instance_of_indoxtetrivalaugmentation","title":"Creating an instance of IndoxTetrivalAugmentation","text":"<p>To effectively utilize the Indox Retrieval Augmentation capabilities, you must first create an instance of the IndoxRetrievalAugmentation class. This instance will allow you to access the methods and properties defined within the class, enabling the augmentation and retrieval functionalities. :::</p> <p>::: {#a0df2482140a045f .cell .code}</p> <pre><code>indox = IndoxRetrievalAugmentation()\n</code></pre> <p>:::</p> <p>::: {#522c45523919543e .cell .code execution_count=\"3\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:11.458148Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:07.828305Z\\\"}\"}</p> <pre><code>google_qa = GoogleAi(api_key=GOOGLE_API_KEY,model=\"gemini-1.5-flash-latest\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\")\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:08:07,830 INFO:IndoxRetrievalAugmentation initialized 2024-06-30 18:08:07,831 INFO:Initializing HuggingFaceModel with model: deepset/roberta-base-squad2 2024-06-30 18:08:07,831 INFO:HuggingFaceModel initialized successfully 2024-06-30 18:08:07,976 INFO:Initializing GoogleAi with model: gemini-1.5-flash-latest 2024-06-30 18:08:07,977 INFO:GoogleAi initialized successfully 2024-06-30 18:08:09,994 INFO:Load pretrained SentenceTransformer: multi-qa-mpnet-base-cos-v1 2024-06-30 18:08:11,455 INFO:Use pytorch device: cpu 2024-06-30 18:08:11,456 INFO:Initialized HuggingFace embeddings with model: multi-qa-mpnet-base-cos-v1 ::: :::</p> <p>::: {#faec85a50b0102a3 .cell .code}</p> <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n</code></pre> <p>:::</p> <p>::: {#5de8165e3423511e .cell .code execution_count=\"4\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:11.461983Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:11.459151Z\\\"}\"}</p> <pre><code>file_path = \"sample.txt\"\n</code></pre> <p>:::</p> <p>::: {#f793d71798ad25e6 .cell .markdown}</p>"},{"location":"Indox/examples/googleai/#data_loader_setup","title":"Data Loader Setup","text":"<p>We set up the data loader using the <code>ClusteredSplit</code> class. This step involves loading documents, configuring embeddings, and setting options for processing the text. :::</p> <p>::: {#be71fc01b5102508 .cell .code execution_count=\"14\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:42:48.499278Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:41:37.373778Z\\\"}\"}</p> <pre><code>load_splitter = ClusteredSplit(file_path=file_path,summary_model=google_qa,embeddings=embed)\ndocs = load_splitter.load_and_chunk()\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:11:37,375 INFO:Initializing ClusteredSplit 2024-06-30 18:11:37,375 INFO:ClusteredSplit initialized successfully 2024-06-30 18:11:37,376 INFO:Getting all documents 2024-06-30 18:11:37,376 INFO:Starting processing for documents 2024-06-30 18:11:47,798 INFO:Generating summary for documentation 2024-06-30 18:11:47,799 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.</p> <pre><code>Documentation:\n [\"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you   Thereupon she closed her eyes and departed   Every day the maiden went out to her mother's grave, and wept, and she remained pious and good   When winter came\", 'the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart Now began a bad time for the poor step-child   Is the stupid goose to sit in the parlor with us, they said   He who wants to eat bread must earn it   Out with the kitchen-wench', 'and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again   In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders   And as on that account she always looked dusty and dirty, they called her cinderella It happened that the father was once going to the fair, and he', 'a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown Next day when the festival began afresh, and her parents and', \"kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree -      shiver and quiver, my little tree,      silver and gold throw down over me\"] in maximum 150 tokens\n</code></pre> <p>:::</p> <p>::: {.output .stream .stdout} --Generated 6 clusters-- :::</p> <p>::: {.output .stream .stderr} 2024-06-30 18:11:51,695 INFO:Response generated successfully 2024-06-30 18:11:51,696 INFO:Generating summary for documentation 2024-06-30 18:11:51,697 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.</p> <pre><code>Documentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:11:53,745 INFO:Response generated successfully\n2024-06-30 18:11:53,746 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:11:54,378 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:11:56,256 INFO:Response generated successfully\n2024-06-30 18:11:56,256 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:11:56,335 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:11:58,269 INFO:Response generated successfully\n2024-06-30 18:11:58,270 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:01,198 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:12:02,975 INFO:Response generated successfully\n2024-06-30 18:12:02,976 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:08,093 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:12:09,883 INFO:Response generated successfully\n2024-06-30 18:12:09,884 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:21,580 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes   Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash   Besides this, the sisters did her every imaginable injury - they mocked her', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"and danced with no one but her   When others came and invited her, he said, this is my partner   When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went   But she sprang away from him, and into the garden behind the house   Therein stood a beautiful tall tree on which hung the most magnificent pears   She clambered so nimbly\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to\", \"when you are queen you will have no more need to go on foot   The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son   Then he took her on his his horse as his bride and rode away with her   They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,\", \"  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot   The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son   He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \"before the king's son, who gave her the golden shoe   Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove   And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride   The step-mother and the two sisters were horrified and became pale with rage, he,\"] in maximum 150 tokens\n2024-06-30 18:12:23,355 INFO:Response generated successfully\n2024-06-30 18:12:23,356 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:23,356 ERROR:Error generating summary: RetryError[&lt;Future at 0x1d6ab0fde80 state=finished raised ValueError&gt;]\n2024-06-30 18:12:23,358 INFO:Generating summary for documentation\n2024-06-30 18:12:23,358 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['asked his two step-daughters what he should bring back for them Beautiful dresses, said one, pearls and jewels, said the second And you, cinderella, said he, what will you have   Father break off for me the first branch which knocks against your hat on your way home   So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and', \"knocked off his hat   Then he broke off the branch and took it with him   When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush   Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it   And it grew and became a handsome tree\", \"  When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so\", '  You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival   You have no clothes and shoes, and yet would dance   As, however, cinderella went on asking, the step-mother said at last, I have emptied a dish of lentils into the ashes for you, if you have picked them out again in two hours, you shall go with us   The maiden went through the', 'But the step-mother said, no, cinderella, you have no clothes and you can not dance   You would only be laughed at   And as cinderella wept at this, the step-mother said, if you can pick two dishes of lentils out of the ashes for me in one hour, you shall go with us   And she thought to herself, that she most certainly cannot do again   When the step-mother had emptied the two', \"  But the step-mother said, all this will not help   You cannot go with us, for you have no clothes and can not dance   We should be ashamed of you   On this she turned her back on cinderella, and hurried away with her two proud daughters As no one was now at home, cinderella went to her mother's grave beneath the hazel-tree, and cried -      shiver and quiver, little tree,      silver and gold throw down over me\", \"the step-sisters had gone once more, cinderella went to the hazel-tree and said -      shiver and quiver, my little tree,      silver and gold throw down over me Then the bird threw down a much more beautiful dress than on the preceding day  And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty   The king's son had waited until she came, and instantly took her by the hand\", \"the other on the left, and remained sitting there When the wedding with the king's son was to be celebrated, the two false sisters came and wanted to get into favor with cinderella and share her good fortune   When the betrothed couple went to church, the elder was at the right side and the younger at the left, and the pigeons pecked out one eye from each of them   Afterwards as they came back the elder was at\"] in maximum 150 tokens\n2024-06-30 18:12:25,536 INFO:Response generated successfully\n2024-06-30 18:12:25,538 INFO:Generating summary for documentation\n2024-06-30 18:12:25,538 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n [' Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride', 'Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver   She put on the dress with all speed, and went to the wedding   Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes   The', \"pigeon-house   The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house   The old man thought, can it be cinderella   And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it   And when they got home cinderella lay in her dirty clothes among the ashes, and\", \"between the branches like a squirrel that the king's son did not know where she was gone   He waited until her father came, and said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree   The father thought, can it be cinderella   And had an axe brought and cut the tree down, but no one was on it   And when they got into the\", \"And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden   And when she went to the festival in the dress, no one knew how to speak for astonishment   The king's son danced with her only, and if any one invited her to dance, he said this is my partner When evening came, cinderella wished to leave, and the king's\", \"  No, said the man, there is still a little stunted kitchen-wench which my late wife left behind her, but she cannot possibly be the bride   The king's son said he was to send her up to him, but the mother answered, oh, no, she is much too dirty, she cannot show herself   But he absolutely insisted on it, and cinderella had to be called   She first washed her hands and face clean, and then went and bowed down\"] in maximum 150 tokens\n2024-06-30 18:12:27,785 INFO:Response generated successfully\n2024-06-30 18:12:27,787 INFO:Generating summary for documentation\n2024-06-30 18:12:27,787 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,      the bad into the crop Then two white pigeons came in by the kitchen window, and afterwards the turtle-doves, and at last all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes', 'And the pigeons nodded with their heads and began pick, pick, pick, pick, and the rest began also pick, pick, pick, pick, and gathered all the good grains into the dish   Hardly had one hour passed before they had finished, and all flew out again   Then the girl took the dish to her step-mother, and was glad, and believed that now she would be allowed to go with them to the festival', 'dishes of lentils amongst the ashes, the maiden went through the back-door into the garden and cried, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,      the bad into the crop Then two white pigeons came in by the kitchen-window, and afterwards the turtle-doves, and at length all the birds beneath the', 'sky, came whirring and crowding in, and alighted amongst the ashes   And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again Then the maiden was delighted, and believed that she might now go with them to the wedding', 'the left, and the younger at the right, and then the pigeons pecked out the other eye from each   And thus, for their wickedness and falsehood, they were punished with blindness all their days'] in maximum 150 tokens\n2024-06-30 18:12:29,375 INFO:Response generated successfully\n2024-06-30 18:12:29,376 INFO:Generating summary for documentation\n2024-06-30 18:12:29,376 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:31,255 INFO:Response generated successfully\n2024-06-30 18:12:31,256 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:32,119 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:33,896 INFO:Response generated successfully\n2024-06-30 18:12:33,897 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:34,407 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:36,095 INFO:Response generated successfully\n2024-06-30 18:12:36,095 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:38,977 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:40,747 INFO:Response generated successfully\n2024-06-30 18:12:40,748 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:41,355 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:42,776 INFO:Response generated successfully\n2024-06-30 18:12:42,777 ERROR:Error generating response: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n2024-06-30 18:12:43,837 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad, for they had pretty feet   The eldest went with the shoe into her room and wanted to try it on, and her mother stood by   But she could not get her big toe into it, and the shoe was too small for her   Then her mother gave her a knife and said, cut the toe off,', \"     there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it   He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on   Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \"     turn and peep, turn and peep,      there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red   Then he turned his horse and took the false bride home again   This also is not the right one, said he, have you no other daughter\", \"however, took cinderella on his horse and rode away with her   As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\"] in maximum 150 tokens\n2024-06-30 18:12:45,470 INFO:Response generated successfully\n2024-06-30 18:12:46,466 INFO:Generating summary for documentation\n2024-06-30 18:12:46,467 INFO:Generating response for prompt: You are a helpful assistant. Give a detailed summary of the documentation provided.\n\nDocumentation:\n ['This story recounts the tragic tale of Cinderella, a young girl who loses her mother and is subjected to cruel treatment by her stepmother and stepsisters. Despite her misfortune, Cinderella remains kind and pious, seeking solace at her mother\\'s grave. She possesses a magical connection to a little hazel tree, which allows her to change into beautiful clothes and attend a fair with her family. However, her transformation is temporary, and she returns to her life of hardship in the kitchen, earning the name \"Cinderella\" due to her ash-covered appearance.', 'RetryError[&lt;Future at 0x1d6ab0fde80 state=finished raised ValueError&gt;]', \"This documentation tells the story of Cinderella. Cinderella's father returns from a trip and brings gifts for his step-daughters but only a hazel twig for Cinderella. Cinderella plants the twig on her mother's grave and it grows into a magical tree. When the king's son is holding a wedding, Cinderella's step-sisters refuse to let her go because she has no nice clothes. Despite their obstacles, Cinderella makes a wish at the magical tree and is gifted with a beautiful dress. She attends the wedding and the king's son instantly falls in love with her. The step-sisters, hoping to share in Cinderella's fortune, attend the wedding but are punished for their cruelty when pigeons peck out their eyes.\", \"This documentation appears to be snippets from a Cinderella story. It details Cinderella's interactions with a magical bird that grants her wishes. The bird provides her with beautiful dresses and slippers, allowing her to attend a three-day festival where the king's son is looking for a bride. Cinderella's step-family doesn't recognize her and believe she's a foreign princess. The story also describes the king's son's attempts to find Cinderella, first by searching a pigeon-house and then by cutting down a pear tree. Finally, the king's son insists on seeing Cinderella, despite her step-mother's initial objections.\", 'This documentation tells the story of a young maiden who was tasked with separating lentils from ashes.  She called upon pigeons and turtle-doves to help her, and they swiftly sorted the lentils.  This success allowed the maiden to attend a festival or wedding.  In contrast, a step-mother and her daughter were punished for their deceit and wickedness by being blinded.', 'The documentation describes a scene from a fairy tale, likely Cinderella. A prince is searching for the woman whose foot fits a golden slipper.  Two of his sisters try to deceive him by cutting off their toes and heel to fit the shoe.  However, blood betrays them, and the prince realizes they are not the true bride.  Finally, Cinderella, the true bride, tries on the slipper and it fits perfectly. Two white doves confirm her identity and the prince rides away with Cinderella.'] in maximum 150 tokens\n</code></pre> <p>:::</p> <p>::: {.output .stream .stdout} --Generated 1 clusters-- :::</p> <p>::: {.output .stream .stderr} 2024-06-30 18:12:48,494 INFO:Response generated successfully 2024-06-30 18:12:48,496 INFO:Completed chunking &amp; clustering process 2024-06-30 18:12:48,496 INFO:Successfully obtained all documents ::: :::</p> <p>::: {#9c6b76fbf6c244b5 .cell .code execution_count=\"15\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:43:01.727597Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:43:01.722922Z\\\"}\"}</p> <pre><code>docs\n</code></pre> <p>::: {.output .execute_result execution_count=\"15\"} [\"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you Thereupon she closed her eyes and departed Every day the maiden went out to her mother's grave, and wept, and she remained pious and good When winter came\", 'the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart Now began a bad time for the poor step-child Is the stupid goose to sit in the parlor with us, they said He who wants to eat bread must earn it Out with the kitchen-wench', ' They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash Besides this, the sisters did her every imaginable injury - they mocked her', 'and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders And as on that account she always looked dusty and dirty, they called her cinderella It happened that the father was once going to the fair, and he', 'asked his two step-daughters what he should bring back for them Beautiful dresses, said one, pearls and jewels, said the second And you, cinderella, said he, what will you have Father break off for me the first branch which knocks against your hat on your way home So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and', \"knocked off his hat Then he broke off the branch and took it with him When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it And it grew and became a handsome tree\", ' Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride', \" When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so\", ' You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival You have no clothes and shoes, and yet would dance As, however, cinderella went on asking, the step-mother said at last, I have emptied a dish of lentils into the ashes for you, if you have picked them out again in two hours, you shall go with us The maiden went through the', 'back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick the good into the pot, the bad into the crop Then two white pigeons came in by the kitchen window, and afterwards the turtle-doves, and at last all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes', 'And the pigeons nodded with their heads and began pick, pick, pick, pick, and the rest began also pick, pick, pick, pick, and gathered all the good grains into the dish Hardly had one hour passed before they had finished, and all flew out again Then the girl took the dish to her step-mother, and was glad, and believed that now she would be allowed to go with them to the festival', 'But the step-mother said, no, cinderella, you have no clothes and you can not dance You would only be laughed at And as cinderella wept at this, the step-mother said, if you can pick two dishes of lentils out of the ashes for me in one hour, you shall go with us And she thought to herself, that she most certainly cannot do again When the step-mother had emptied the two', 'dishes of lentils amongst the ashes, the maiden went through the back-door into the garden and cried, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick the good into the pot, the bad into the crop Then two white pigeons came in by the kitchen-window, and afterwards the turtle-doves, and at length all the birds beneath the', 'sky, came whirring and crowding in, and alighted amongst the ashes And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again Then the maiden was delighted, and believed that she might now go with them to the wedding', \" But the step-mother said, all this will not help You cannot go with us, for you have no clothes and can not dance We should be ashamed of you On this she turned her back on cinderella, and hurried away with her two proud daughters As no one was now at home, cinderella went to her mother's grave beneath the hazel-tree, and cried - shiver and quiver, little tree, silver and gold throw down over me\", 'Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver She put on the dress with all speed, and went to the wedding Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes The', \"prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the\", \"pigeon-house The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house The old man thought, can it be cinderella And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it And when they got home cinderella lay in her dirty clothes among the ashes, and\", 'a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown Next day when the festival began afresh, and her parents and', \"the step-sisters had gone once more, cinderella went to the hazel-tree and said - shiver and quiver, my little tree, silver and gold throw down over me Then the bird threw down a much more beautiful dress than on the preceding day And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty The king's son had waited until she came, and instantly took her by the hand\", \"and danced with no one but her When others came and invited her, he said, this is my partner When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went But she sprang away from him, and into the garden behind the house Therein stood a beautiful tall tree on which hung the most magnificent pears She clambered so nimbly\", \"between the branches like a squirrel that the king's son did not know where she was gone He waited until her father came, and said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree The father thought, can it be cinderella And had an axe brought and cut the tree down, but no one was on it And when they got into the\", \"kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree - shiver and quiver, my little tree, silver and gold throw down over me\", \"And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden And when she went to the festival in the dress, no one knew how to speak for astonishment The king's son danced with her only, and if any one invited her to dance, he said this is my partner When evening came, cinderella wished to leave, and the king's\", \"son was anxious to go with her, but she escaped from him so quickly that he could not follow her The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck The king's son picked it up, and it was small and dainty, and all golden Next morning, he went with it to\", 'the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits Then were the two sisters glad, for they had pretty feet The eldest went with the shoe into her room and wanted to try it on, and her mother stood by But she could not get her big toe into it, and the shoe was too small for her Then her mother gave her a knife and said, cut the toe off,', \"when you are queen you will have no more need to go on foot The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son Then he took her on his his horse as his bride and rode away with her They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried - turn and peep, turn and peep,\", \" there's blood within the shoe, the shoe it is too small for her, the true bride waits for you Then he looked at her foot and saw how the blood was trickling from it He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large\", \" So her mother gave her a knife and said, cut a bit off your heel, when you are queen you will have no more need to go on foot The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -\", \" turn and peep, turn and peep, there's blood within the shoe, the shoe it is too small for her, the true bride waits for you He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red Then he turned his horse and took the false bride home again This also is not the right one, said he, have you no other daughter\", \" No, said the man, there is still a little stunted kitchen-wench which my late wife left behind her, but she cannot possibly be the bride The king's son said he was to send her up to him, but the mother answered, oh, no, she is much too dirty, she cannot show herself But he absolutely insisted on it, and cinderella had to be called She first washed her hands and face clean, and then went and bowed down\", \"before the king's son, who gave her the golden shoe Then she seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride The step-mother and the two sisters were horrified and became pale with rage, he,\", \"however, took cinderella on his horse and rode away with her As they passed by the hazel-tree, the two white doves cried - turn and peep, turn and peep, no blood is in the shoe, the shoe is not too small for her, the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right,\", \"the other on the left, and remained sitting there When the wedding with the king's son was to be celebrated, the two false sisters came and wanted to get into favor with cinderella and share her good fortune When the betrothed couple went to church, the elder was at the right side and the younger at the left, and the pigeons pecked out one eye from each of them Afterwards as they came back the elder was at\", 'the left, and the younger at the right, and then the pigeons pecked out the other eye from each And thus, for their wickedness and falsehood, they were punished with blindness all their days', 'This story recounts the tragic tale of Cinderella, a young girl who loses her mother and is subjected to cruel treatment by her stepmother and stepsisters. Despite her misfortune, Cinderella remains kind and pious, seeking solace at her mother\\'s grave. She possesses a magical connection to a little hazel tree, which allows her to change into beautiful clothes and attend a fair with her family. However, her transformation is temporary, and she returns to her life of hardship in the kitchen, earning the name \"Cinderella\" due to her ash-covered appearance.', 'RetryError[]', \"This documentation tells the story of Cinderella. Cinderella's father returns from a trip and brings gifts for his step-daughters but only a hazel twig for Cinderella. Cinderella plants the twig on her mother's grave and it grows into a magical tree. When the king's son is holding a wedding, Cinderella's step-sisters refuse to let her go because she has no nice clothes. Despite their obstacles, Cinderella makes a wish at the magical tree and is gifted with a beautiful dress. She attends the wedding and the king's son instantly falls in love with her. The step-sisters, hoping to share in Cinderella's fortune, attend the wedding but are punished for their cruelty when pigeons peck out their eyes.\", \"This documentation appears to be snippets from a Cinderella story. It details Cinderella's interactions with a magical bird that grants her wishes. The bird provides her with beautiful dresses and slippers, allowing her to attend a three-day festival where the king's son is looking for a bride. Cinderella's step-family doesn't recognize her and believe she's a foreign princess. The story also describes the king's son's attempts to find Cinderella, first by searching a pigeon-house and then by cutting down a pear tree. Finally, the king's son insists on seeing Cinderella, despite her step-mother's initial objections.\", 'This documentation tells the story of a young maiden who was tasked with separating lentils from ashes. She called upon pigeons and turtle-doves to help her, and they swiftly sorted the lentils. This success allowed the maiden to attend a festival or wedding. In contrast, a step-mother and her daughter were punished for their deceit and wickedness by being blinded.', 'The documentation describes a scene from a fairy tale, likely Cinderella. A prince is searching for the woman whose foot fits a golden slipper. Two of his sisters try to deceive him by cutting off their toes and heel to fit the shoe. However, blood betrays them, and the prince realizes they are not the true bride. Finally, Cinderella, the true bride, tries on the slipper and it fits perfectly. Two white doves confirm her identity and the prince rides away with Cinderella.', \"This documentation presents variations of the Cinderella story. It details Cinderella's hardships with her stepmother and stepsisters, her connection to a magical hazel tree granting her beautiful clothes, and her attendance at a royal event where she meets the king's son. The stories also highlight Cinderella's kindness, the punishment of her stepfamily for their cruelty, and the use of magical elements like birds and a golden slipper to identify the true bride. There are mentions of Cinderella's wish-granting tree, magical birds, and the prince's search for the woman who fits the golden slipper.\"] ::: ::: <p>::: {#1fc4947b39af7a2b .cell .markdown}</p>"},{"location":"Indox/examples/googleai/#vector_store_connection_and_document_storage","title":"Vector Store Connection and Document Storage","text":"<p>In this step, we connect the Indox application to the vector store and store the processed documents. :::</p> <p>::: {#e759f83f18e54ad0 .cell .code execution_count=\"6\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:15.146212Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:14.853347Z\\\"}\"}</p> <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed)\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:08:15,026 INFO:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information. ::: :::</p> <p>::: {#b0b38ae389dac2cf .cell .code execution_count=\"7\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:15.926353Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:15.919425Z\\\"}\"}</p> <pre><code>indox.connect_to_vectorstore(vectorstore_database=db)\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:08:15,920 INFO:Attempting to connect to the vector store database 2024-06-30 18:08:15,921 INFO:Connection to the vector store database established successfully :::</p> <p>::: {.output .execute_result execution_count=\"7\"}  ::: ::: <p>::: {#53662ae9061b4503 .cell .code execution_count=\"8\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:22.413794Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:16.495368Z\\\"}\"}</p> <pre><code>indox.store_in_vectorstore(docs)\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:08:16,497 INFO:Storing documents in the vector store 2024-06-30 18:08:22,409 INFO:Document added successfully to the vector store. 2024-06-30 18:08:22,410 INFO:Documents stored successfully :::</p> <p>::: {.output .execute_result execution_count=\"8\"}  ::: ::: <p>::: {#406ea4c69d522590 .cell .markdown}</p>"},{"location":"Indox/examples/googleai/#querying_and_interpreting_the_response","title":"Querying and Interpreting the Response","text":"<p>In this step, we query the Indox application with a specific question and use the QA model to get the response. :::</p> <p>::: {#803f7d211e83d807 .cell .code execution_count=\"9\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:22.417573Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:22.414798Z\\\"}\"}</p> <pre><code>query = \"How cinderella reach her happy ending?\"\n</code></pre> <p>:::</p> <p>::: {#978d7cea54f08f19 .cell .code execution_count=\"10\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:22.421544Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:22.418578Z\\\"}\"}</p> <pre><code>retriever = indox.QuestionAnswer(vector_database=db,llm=google_qa,top_k=5)\n</code></pre> <p>:::</p> <p>::: {#b7a6fd77009af7a6 .cell .code execution_count=\"11\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:31.950429Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:24.450022Z\\\"}\"}</p> <pre><code>answer = retriever.invoke(query=query)\n</code></pre> <p>::: {.output .stream .stderr} 2024-06-30 18:08:24,452 INFO:Retrieving context and scores from the vector database 2024-06-30 18:08:24,522 INFO:Generating answer without document relevancy filter 2024-06-30 18:08:24,523 INFO:Answering question: How cinderella reach her happy ending? 2024-06-30 18:08:24,523 INFO:Generating response for prompt: Given Context: [\"Beautiful dresses, said one, pearls and jewels, said the second. And you, cinderella, said he, what will you have. Father break off for me the first branch which knocks against your hat on your way home. So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat. Then he broke off the branch and took it with him. When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush. Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it. And it grew and became a handsome tree. Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for. It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride. When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace. Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so. You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival. You have no clothes and shoes, and yet would dance. As, however, cinderella went on\", \"said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree. The father thought, can it be cinderella. And had an axe brought and cut the tree down, but no one was on it. And when they got into the kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown. On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree - shiver and quiver, my little tree, silver and gold throw down over me. And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden. And when she went to the festival in the dress, no one knew how to speak for astonishment. The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her. The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck. The king's son picked it up, and it was small and dainty, and all golden. Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits. Then were the two sisters glad, for they had pretty feet. The eldest went with the shoe into her room and wanted to try it on, and her mother stood by. But she\", \"afterwards the turtle-doves, and at length all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes. And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again. Then the maiden was delighted, and believed that she might now go with them to the wedding. But the step-mother said, all this will not help. You cannot go with us, for you have no clothes and can not dance. We should be ashamed of you. On this she turned her back on cinderella, and hurried away with her two proud daughters. As no one was now at home, cinderella went to her mother's grave beneath the hazel-tree, and cried - shiver and quiver, little tree, silver and gold throw down over me. Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver. She put on the dress with all speed, and went to the wedding. Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress. They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes. The prince approached her, took her by the hand and danced with her. He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner. She danced till it was evening, and then she wanted to go home. But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged.\", \"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you. Thereupon she closed her eyes and departed. Every day the maiden went out to her mother's grave, and wept, and she remained pious and good. When winter came the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife. The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart. Now began a bad time for the poor step-child. Is the stupid goose to sit in the parlor with us, they said. He who wants to eat bread must earn it. Out with the kitchen-wench. They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes. Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen. There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash. Besides this, the sisters did her every imaginable injury - they mocked her and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again. In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders. And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them.\", \"She escaped from him, however, and sprang into the pigeon-house. The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house. The old man thought, can it be cinderella. And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it. And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown. Next day when the festival began afresh, and her parents and the step-sisters had gone once more, cinderella went to the hazel-tree and said - shiver and quiver, my little tree, silver and gold throw down over me. Then the bird threw down a much more beautiful dress than on the preceding day. And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty. The king's son had waited until she came, and instantly took her by the hand and danced with no one but her. When others came and invited her, he said, this is my partner. When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went. But she sprang away from him, and into the garden behind the house. Therein stood a beautiful tall tree on which hung the most magnificent pears. She clambered so nimbly between the branches like a squirrel that the king's son did not know where she was gone. He waited until her father came, and\"] Give the best full answer amongst the option to question How cinderella reach her happy ending? in maximum 200 tokens 2024-06-30 18:08:28,621 INFO:Response generated successfully 2024-06-30 18:08:28,622 ERROR:Error generating response: Invalid operation: The <code>response.text</code> quick accessor requires the response to contain a valid <code>Part</code>, but none were returned. Please check the <code>candidate.safety_ratings</code> to determine if the response was blocked. 2024-06-30 18:08:29,489 INFO:Generating response for prompt: Given Context: [\"Beautiful dresses, said one, pearls and jewels, said the second. And you, cinderella, said he, what will you have. Father break off for me the first branch which knocks against your hat on your way home. So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat. Then he broke off the branch and took it with him. When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush. Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it. And it grew and became a handsome tree. Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for. It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride. When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace. Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so. You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival. You have no clothes and shoes, and yet would dance. As, however, cinderella went on\", \"said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree. The father thought, can it be cinderella. And had an axe brought and cut the tree down, but no one was on it. And when they got into the kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown. On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree - shiver and quiver, my little tree, silver and gold throw down over me. And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden. And when she went to the festival in the dress, no one knew how to speak for astonishment. The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her. The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck. The king's son picked it up, and it was small and dainty, and all golden. Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits. Then were the two sisters glad, for they had pretty feet. The eldest went with the shoe into her room and wanted to try it on, and her mother stood by. But she\", \"afterwards the turtle-doves, and at length all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes. And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again. Then the maiden was delighted, and believed that she might now go with them to the wedding. But the step-mother said, all this will not help. You cannot go with us, for you have no clothes and can not dance. We should be ashamed of you. On this she turned her back on cinderella, and hurried away with her two proud daughters. As no one was now at home, cinderella went to her mother's grave beneath the hazel-tree, and cried - shiver and quiver, little tree, silver and gold throw down over me. Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver. She put on the dress with all speed, and went to the wedding. Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress. They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes. The prince approached her, took her by the hand and danced with her. He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner. She danced till it was evening, and then she wanted to go home. But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged.\", \"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you. Thereupon she closed her eyes and departed. Every day the maiden went out to her mother's grave, and wept, and she remained pious and good. When winter came the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife. The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart. Now began a bad time for the poor step-child. Is the stupid goose to sit in the parlor with us, they said. He who wants to eat bread must earn it. Out with the kitchen-wench. They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes. Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen. There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash. Besides this, the sisters did her every imaginable injury - they mocked her and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again. In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders. And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them.\", \"She escaped from him, however, and sprang into the pigeon-house. The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house. The old man thought, can it be cinderella. And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it. And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown. Next day when the festival began afresh, and her parents and the step-sisters had gone once more, cinderella went to the hazel-tree and said - shiver and quiver, my little tree, silver and gold throw down over me. Then the bird threw down a much more beautiful dress than on the preceding day. And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty. The king's son had waited until she came, and instantly took her by the hand and danced with no one but her. When others came and invited her, he said, this is my partner. When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went. But she sprang away from him, and into the garden behind the house. Therein stood a beautiful tall tree on which hung the most magnificent pears. She clambered so nimbly between the branches like a squirrel that the king's son did not know where she was gone. He waited until her father came, and\"] Give the best full answer amongst the option to question How cinderella reach her happy ending? in maximum 200 tokens 2024-06-30 18:08:31,947 INFO:Response generated successfully 2024-06-30 18:08:31,948 INFO:Query answered successfully ::: :::</p> <p>::: {#bd99cb2b27580ebb .cell .code execution_count=\"12\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:38:43.120287Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:38:43.115576Z\\\"}\"}</p> <pre><code>answer\n</code></pre> <p>::: {.output .execute_result execution_count=\"12\"} \"Cinderella reaches her happy ending through a combination of kindness, resourcefulness, and a little bit of magic. She uses the hazel-tree gifted by her father to communicate with a helpful bird that grants her wishes. This bird provides her with beautiful dresses and golden slippers, allowing her to attend the king's ball and capture the prince's attention. Despite her step-family's attempts to sabotage her, Cinderella's kindness and good nature shine through, and the prince, recognizing her true beauty, seeks her out. The golden slipper, a symbol of her grace and charm, ultimately leads the prince to her, proving that despite adversity, true love and kindness prevail.\" ::: :::</p> <p>::: {#40f2b50b5cb304ca .cell .code execution_count=\"15\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-30T14:34:29.902514Z\\\",\\\"start_time\\\":\\\"2024-06-30T14:34:29.897796Z\\\"}\"}</p> <pre><code>retriever.context\n</code></pre> <p>::: {.output .execute_result execution_count=\"15\"} [\"Beautiful dresses, said one, pearls and jewels, said the second. And you, cinderella, said he, what will you have. Father break off for me the first branch which knocks against your hat on your way home. So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat. Then he broke off the branch and took it with him. When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush. Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it. And it grew and became a handsome tree. Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for. It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride. When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace. Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so. You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival. You have no clothes and shoes, and yet would dance. As, however, cinderella went on\", \"said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree. The father thought, can it be cinderella. And had an axe brought and cut the tree down, but no one was on it. And when they got into the kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown. On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree - shiver and quiver, my little tree, silver and gold throw down over me. And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden. And when she went to the festival in the dress, no one knew how to speak for astonishment. The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her. The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck. The king's son picked it up, and it was small and dainty, and all golden. Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits. Then were the two sisters glad, for they had pretty feet. The eldest went with the shoe into her room and wanted to try it on, and her mother stood by. But she\", \"afterwards the turtle-doves, and at length all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes. And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again. Then the maiden was delighted, and believed that she might now go with them to the wedding. But the step-mother said, all this will not help. You cannot go with us, for you have no clothes and can not dance. We should be ashamed of you. On this she turned her back on cinderella, and hurried away with her two proud daughters. As no one was now at home, cinderella went to her mother's grave beneath the hazel-tree, and cried - shiver and quiver, little tree, silver and gold throw down over me. Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver. She put on the dress with all speed, and went to the wedding. Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress. They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes. The prince approached her, took her by the hand and danced with her. He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner. She danced till it was evening, and then she wanted to go home. But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged.\", \"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you. Thereupon she closed her eyes and departed. Every day the maiden went out to her mother's grave, and wept, and she remained pious and good. When winter came the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife. The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart. Now began a bad time for the poor step-child. Is the stupid goose to sit in the parlor with us, they said. He who wants to eat bread must earn it. Out with the kitchen-wench. They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes. Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen. There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash. Besides this, the sisters did her every imaginable injury - they mocked her and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again. In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders. And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them.\", \"She escaped from him, however, and sprang into the pigeon-house. The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house. The old man thought, can it be cinderella. And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it. And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown. Next day when the festival began afresh, and her parents and the step-sisters had gone once more, cinderella went to the hazel-tree and said - shiver and quiver, my little tree, silver and gold throw down over me. Then the bird threw down a much more beautiful dress than on the preceding day. And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty. The king's son had waited until she came, and instantly took her by the hand and danced with no one but her. When others came and invited her, he said, this is my partner. When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went. But she sprang away from him, and into the garden behind the house. Therein stood a beautiful tall tree on which hung the most magnificent pears. She clambered so nimbly between the branches like a squirrel that the king's son did not know where she was gone. He waited until her father came, and\"] ::: :::</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/","title":"HF mistral SimpleReader","text":""},{"location":"Indox/examples/hf_mistral_SimpleReader/#how_to_use_indox_retrieval_augmentation_for_pdf_files","title":"How to use Indox Retrieval Augmentation for PDF files","text":"<p>In this notebook, we will demonstrate how to handle <code>inDox</code> as system for question answering system with open source models which are available on internet like <code>Mistral</code>. so firstly you should buil environment variables and API keys in Python using the <code>dotenv</code> library.</p> <p>Note: Because we are using HuggingFace models you need to define your <code>HUGGINGFACE_API_KEY</code> in <code>.env</code> file. This allows us to keep our API keys and other sensitive information out of our codebase, enhancing security and maintainability. :::</p> <p>::: {#4deb2abc71a048be .cell .code execution_count=\"8\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"4deb2abc71a048be\" outputId=\"7d64d807-ed35-4ea1-fe15-fe5479ca796d\"} <pre><code>!pip install indox\n!pip install chromadb\n!pip install semantic_text_splitter\n!pip install sentence-transformers\n</code></pre></p> <p>::: {#2d6948cfe8ce7b88 .cell .code execution_count=\"2\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"2d6948cfe8ce7b88\" outputId=\"e4e00368-40e9-46e3-b200-14b8bbc229f0\"} <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n</code></pre></p> <p>::: {.output .stream .stdout}     --2024-07-02 09:10:03--  https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt     Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...     Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.     HTTP request sent, awaiting response... 200 OK     Length: 14025 (14K) [text/plain]     Saving to: \u2018sample.txt\u2019</p> <p>sample.txt            0%[                    ]       0  --.-KB/s              sample.txt          100%[===================&gt;]  13.70K  --.-KB/s    in 0s      </p> <pre><code>2024-07-02 09:10:03 (72.4 MB/s) - \u2018sample.txt\u2019 saved [14025/14025]\n</code></pre> <p>::: :::</p> <p>::: {#initial_id .cell .code execution_count=\"3\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:38.098709Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:38.086571Z\\\"}\" collapsed=\"true\" id=\"initial_id\"} <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n</code></pre> :::</p> <p>::: {#d97666fa4a136fa4 .cell .markdown id=\"d97666fa4a136fa4\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#import_essential_libraries","title":"Import Essential Libraries","text":"<p>Then, we import essential libraries for our <code>Indox</code> question answering system:</p> <ul> <li><code>IndoxRetrievalAugmentation</code>: Enhances the retrieval process for     better QA performance.</li> <li><code>MistralQA</code>: A powerful QA model from Indox, built on top of the     Hugging Face model.</li> <li><code>HuggingFaceEmbedding</code>: Utilizes Hugging Face embeddings for     improved semantic understanding.</li> <li><code>SimpleLoadAndSplit</code>: A utility for loading and splitting PDF files. :::</li> </ul> <p>::: {#71cea6a5876fa5fe .cell .code execution_count=\"6\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:47.832652Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:38.872557Z\\\"}\" id=\"71cea6a5876fa5fe\"} <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter.SimpleLoadAndSplit import SimpleLoadAndSplit\n</code></pre> :::</p> <p>::: {#dfce2b023a435935 .cell .markdown id=\"dfce2b023a435935\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#building_the_indox_system_and_initializing_models","title":"Building the Indox System and Initializing Models","text":"<p>Next, we will build our <code>inDox</code> system and initialize the Mistral question answering model along with the embedding model. This setup will allow us to leverage the advanced capabilities of Indox for our question answering tasks. :::</p> <p>::: {#be7f2eb137ea4f1b .cell .code execution_count=\"9\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:51.558601Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:47.833775Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\",\\\"height\\\":369,\\\"referenced_widgets\\\":[\\\"f5a5036392c9462c903364eab1388cca\\\",\\\"2c9c482359c74bba8d35bd5a99b0b272\\\",\\\"fe107afe6cc74673afce75ad1fb61206\\\",\\\"4aa9bf7557d547f6a39fa93b96a8233d\\\",\\\"f3c97aebd0ce416f858a4abc8533bc1c\\\",\\\"a688174866694716a3f833a05761509f\\\",\\\"f2ef47b784a34c218cb5346d96183da6\\\",\\\"e7a1b752ee564ed18dafb4928bbad940\\\",\\\"a0ae506462fd4de9a831f3545cb4682e\\\",\\\"f33f1beca1f54fdd99a3d1ea7d03a0f1\\\",\\\"9b8f564b15da453ea811acde3f1b5eaa\\\",\\\"010ed74be3944660a12891dc607ca1a8\\\",\\\"6dbe57883b3a422abcc78cdc86faee0e\\\",\\\"adb60c0a86384134a8b0e9d1fb93dae6\\\",\\\"d759c9ae84284362ac1c197be51ef8fd\\\",\\\"823759359e8041a298e51ddeb9e718f8\\\",\\\"1f098f05988e4336b1e05c7bdf0530d7\\\",\\\"070514ed43e44b16ad0e5851deacafed\\\",\\\"b1237265df6b40f3837c4732cafee7fb\\\",\\\"5297f72091294256bd5f19263873827e\\\",\\\"3177fadb9d1a43bcba2c077f906b64ff\\\",\\\"2c724a13f5c041e693bf57d5740b43b2\\\",\\\"946464dc055e45f9bb9c1ead93247206\\\",\\\"44e038a01938429da9a44133590f5bfd\\\",\\\"8c83104d6ef24e5ca7a4838231ffece7\\\",\\\"b8047170243e429bbe35fcddff4697ce\\\",\\\"3136aac028444d09a16a3acdb05928b5\\\",\\\"527bc850f45b42a2a9ae6993d50ecd79\\\",\\\"7834cb453fd043f38b7e540bc70c90de\\\",\\\"86c85a928644439a9e6ded6a1ad86ebf\\\",\\\"ea89fbfff4e943eba0b769edabb9ffd8\\\",\\\"4b2f7cbd45e14c068d77aad2341b672c\\\",\\\"08cb878535274af8a7d771dbb71b58e7\\\",\\\"4112edbd6b044b569660dd38ffa3f27a\\\",\\\"0af588eacff84cbc997482f0878fa780\\\",\\\"8b4a84a48fc84cfb82cdfb244c5b4f05\\\",\\\"6cb894c218e5484ca128ff284f46aeec\\\",\\\"2302727f502e4cf5a8884d2f8d423183\\\",\\\"4c6896f5a2224f3090426ccfdf55f5ab\\\",\\\"e68009bb33394e6198f1109bacae6d67\\\",\\\"9ce868b2cc944e4b86eb97765acf7745\\\",\\\"c340eac0ba674bc9bf3736115cf0d7d4\\\",\\\"c1b7d9b59d184a37abb6e0c4f01f7469\\\",\\\"3d32d7acbdbc4de0ac73d0fbcd014acd\\\",\\\"c3dd1b712c324778a91a7c1750323ca2\\\",\\\"a7c53cf410054ad580f12d7f96fb7586\\\",\\\"e937971144b9448a80e76cc58224b174\\\",\\\"e2ff74c989f642d1b3f2d4e3e9043bc2\\\",\\\"a18a05fde8e146d5a2f8f66b9c5fe1d5\\\",\\\"ba9140e635c44689a717e6f146b40aa9\\\",\\\"072dac0b06a8452dbfcdfbc142beeab8\\\",\\\"98335a7da2b545db8098e2ace1f3ac1b\\\",\\\"3ce716123194429b9c444e2182d6093a\\\",\\\"46923b21aab24aba9253cd88e09656b8\\\",\\\"99ac55283f734f3cbc598e5b5ee228b7\\\",\\\"b7b98fd3242748a3941ab86879edb1d4\\\",\\\"a6306fd2f3564bdb88b25c4aee746bd3\\\",\\\"38c6199398824a4c880ef2e02374942c\\\",\\\"51198d2b64724ebda6b9ab58419b3cf9\\\",\\\"1d5ee00b1f334307b665043b88ec78cf\\\",\\\"33af708692764a40965ff40cc1679ae7\\\",\\\"9f3ce45cbe9f4db78bc5bee85ef3fbde\\\",\\\"fcd421736ea640d397175dab615d7131\\\",\\\"d077450e52b04d9e9a58e34c66459fcb\\\",\\\"ae47d975cc42451da6ad9f67051c0690\\\",\\\"e8bd35bd61c64d3f8f4646a9f51cba4b\\\",\\\"70ca761c554c49788f39ac022a1e0a0c\\\",\\\"c2d1474fc76040b7a783baac323b72eb\\\",\\\"260eda9b2c284046b3b13eec5fa3e681\\\",\\\"15a1950ddec644a0b4dbaf6fbd2d2f2c\\\",\\\"92592fda49944fd7a71e20cf988e0736\\\",\\\"01cce2aa951a4efab6b1de0e7c434841\\\",\\\"463a01c2f5d54f718bc7acb06322b410\\\",\\\"53fb6a95bd11498d99da873bcbc351ec\\\",\\\"b91a4d38baff4c7292eda4e41e306a8e\\\",\\\"762b71dde71740d3b266b30ee9cf7d0c\\\",\\\"77c48755a5584564b6d69ee6e59742f2\\\",\\\"3c540818f4fb4c34a3e1bb92d623e356\\\",\\\"b4d20af60de343c8bd7a559146f33288\\\",\\\"62a96756539a4c9c8b37a3880cf60856\\\",\\\"3ed60db66bfd4b7a9b6ea838bbf7cde0\\\",\\\"b229d81476d04f5a9a8b208ee50932e9\\\",\\\"9d7db8511d2043368f72994dcddca546\\\",\\\"585fc10a270144899f32c66f61884c25\\\",\\\"935e798098c74c0bb7b20a400c72a604\\\",\\\"a97b422055da4cf0b801e214a9c1d615\\\",\\\"115c6e6bc812404ead523830594419b0\\\",\\\"30ef7de6651e4fb19792027e9c5560f4\\\",\\\"2782e5da9ae14a2286373625261c9ddf\\\",\\\"cdd792528ab6407b9cb3504114d0b990\\\",\\\"1c64f40e0bfd4bcba077c0488f87d799\\\",\\\"985a161dcc784e748f757f7e7b0f4b72\\\",\\\"c309ce7dc72b4229bebe1c24932f2509\\\",\\\"671ef87d01514bf19c85827e6a963c03\\\",\\\"d125bf45f8054f0298ce128ac7c80f6f\\\",\\\"f28e3776dda74ce5a99a4f44bcad7fc4\\\",\\\"eb02f73ebda64b05aba04532c70fbfe2\\\",\\\"11b857b5dfb647eb90b9b3bb75089819\\\",\\\"fcb60e61fd8046f498aac8ff4bc6ed90\\\",\\\"8ec1f824a2d6489ba28811b1ebc233db\\\",\\\"c611f11954d94f2ea08bf3f5e251513b\\\",\\\"aef9e06b527b4c09a39c92613bda68e2\\\",\\\"4110c8d535dc4e5db696827274175581\\\",\\\"458a16a9cb864cfeb67d4fd8caa15b7c\\\",\\\"549c647ee0144ec1b99264eb7b6c5b5e\\\",\\\"e410b6dde03e4c9582b14fcf86351eab\\\",\\\"ea9de2ed992f45a58b3f9774d4a27fe5\\\",\\\"5c87e1522e8043bab149034a0849118c\\\",\\\"c2ab2050de7e4c6d95ce0209b7704853\\\",\\\"1677b83b6f7941288ffea7e26f1839a1\\\",\\\"5482f2e93f464564957f3260daed3bd2\\\",\\\"1d97c86d31b34be7ac7590d28858a81f\\\",\\\"ac4c0e5652de415f9887ff795bded3ac\\\",\\\"e3f7757502a144158e9b3bb8f0f8cd4c\\\",\\\"4fa1d4de5e794d38b2f1ef1362bc0056\\\",\\\"35a7be45599b46a0aecb73ee79b6bef0\\\",\\\"74f50b06501c4bcc8209b0541eaaad02\\\",\\\"f04994abe48b44c8ae1d40b2000032b5\\\",\\\"e6c0b3f713a2412db68aca62bee3124c\\\",\\\"0a01dec7cd7042e3806fe5db88fe68ae\\\",\\\"1b367c3e496948909341c2f15a9e146d\\\"]}\" id=\"be7f2eb137ea4f1b\" outputId=\"f774310a-e877-4983-b8f0-b3c97269f97f\"} <pre><code>indox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\")\n</code></pre></p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"f5a5036392c9462c903364eab1388cca\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"010ed74be3944660a12891dc607ca1a8\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"946464dc055e45f9bb9c1ead93247206\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"4112edbd6b044b569660dd38ffa3f27a\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"c3dd1b712c324778a91a7c1750323ca2\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"b7b98fd3242748a3941ab86879edb1d4\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"70ca761c554c49788f39ac022a1e0a0c\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"3c540818f4fb4c34a3e1bb92d623e356\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"2782e5da9ae14a2286373625261c9ddf\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"8ec1f824a2d6489ba28811b1ebc233db\",\"version_major\":2,\"version_minor\":0}\n</code></pre> :::</p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"5482f2e93f464564957f3260daed3bd2\",\"version_major\":2,\"version_minor\":0}\n</code></pre> ::: :::</p> <p>::: {#70a4fe8c3d39b341 .cell .markdown id=\"70a4fe8c3d39b341\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#setting_up_reference_directory_and_file_path","title":"Setting Up Reference Directory and File Path","text":"<p>To demonstrate the capabilities of our Indox question answering system, we will use a sample directory. This directory will contain our reference data, which we will use for testing and evaluation.</p> <p>First, we specify the path to our sample file. In this case, we are using a file named <code>sample.txt</code> located in our working directory. This file will serve as our reference data for the subsequent steps.</p> <p>Let\\'s define the file path for our reference data. :::</p> <p>::: {#de47eb24481ec6f0 .cell .code} <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n</code></pre> :::</p> <p>::: {#7d72d5ab31985758 .cell .code execution_count=\"10\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:51.563008Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:51.559605Z\\\"}\" id=\"7d72d5ab31985758\"} <pre><code>file_path = \"sample.txt\"\n</code></pre> :::</p> <p>::: {#5c474890b4eb337 .cell .markdown id=\"5c474890b4eb337\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#chunking_reference_data_with_unstructuredloadandsplit","title":"Chunking Reference Data with UnstructuredLoadAndSplit","text":"<p>To effectively utilize our reference data, we need to process and chunk it into manageable parts. This ensures that our question answering system can efficiently handle and retrieve relevant information.</p> <p>We use the <code>SimpleLoadAndSplit</code> utility for this task. This tool allows us to load the PDF files and split it into smaller chunks. This process enhances the performance of our retrieval and QA models by making the data more accessible and easier to process. We are using \\'bert-base-uncased\\' model for splitting data.</p> <p>In this step, we define the file path for our reference data and use <code>SimpleLoadAndSplit</code> to chunk the data with a maximum chunk size of 200 characters. Also we can handle to remove stop words or not by initializing <code>remove-sword</code> parameter.</p> <p>Let\\'s proceed with chunking our reference data. :::</p> <p>::: {#f45144aba717b77d .cell .code execution_count=\"11\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:52.866553Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:51.564014Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\",\\\"height\\\":49,\\\"referenced_widgets\\\":[\\\"35a00d66d49f49b491287c686dfd6d9c\\\",\\\"0fb451900f6c49f3813a0708760d8db5\\\",\\\"6108a37bb56d4406947badb9e972bf35\\\",\\\"4198edcd7c654199b74deb5aa56ff2e6\\\",\\\"6b9eb0e710a646f28db2dcac8ad61820\\\",\\\"79f2fdb553be4edb9a9f3e82006f9355\\\",\\\"54caa58d86a84fb6b292a0dc7c1a6164\\\",\\\"237c618eeac047679c10714d37126997\\\",\\\"1ccdfdb963fb49d8bfe522545d198d7f\\\",\\\"889105b138bf413fb6658cf96e86ad38\\\",\\\"39503b49c47f4d268cd3fca8a41a82f3\\\"]}\" id=\"f45144aba717b77d\" outputId=\"5f16b724-f1bf-4f25-d10d-315c87dcc18b\"} <pre><code>simpleLoadAndSplit = SimpleLoadAndSplit(file_path=\"sample.txt\",remove_sword=False,max_chunk_size=200)\ndocs = simpleLoadAndSplit.load_and_chunk()\n</code></pre></p> <p>::: {.output .display_data} <pre><code>{\"model_id\":\"35a00d66d49f49b491287c686dfd6d9c\",\"version_major\":2,\"version_minor\":0}\n</code></pre> ::: :::</p> <p>::: {#449dd09782582fa7 .cell .code execution_count=\"12\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:52.872717Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:52.867559Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"449dd09782582fa7\" outputId=\"e7b1e18b-9d3f-4464-c25f-c62e66db24d6\"} <pre><code>docs\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"12\"}     [\"The wife of a rich man fell sick, and as she felt that her end was drawing near, she called her only daughter to her bedside and said, dear child, be good and pious, and then the good God will always protect you, and I will look down on you from heaven and be near you.  Thereupon she closed her eyes and departed.  Every day the maiden went out to her mother's grave, and wept, and she remained pious and good.  When winter came the snow spread a white sheet over the grave, and by the time the spring sun had drawn it off again, the man had taken another wife. The woman had brought with her into the house two daughters, who were beautiful and fair of face, but vile and black of heart. Now began a bad time for the poor step-child.  Is the stupid goose to sit in the parlor with us, they said.  He who wants to eat bread\",      'must earn it.  Out with the kitchen-wench.  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes.  Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen. There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash.  Besides this, the sisters did her every imaginable injury - they mocked her and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again.  In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders.  And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he',      \"asked his two step-daughters what he should bring back for them. Beautiful dresses, said one, pearls and jewels, said the second. And you, cinderella, said he, what will you have.  Father break off for me the first branch which knocks against your hat on your way home.  So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat.  Then he broke off the branch and took it with him.  When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush.  Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it.  And it grew and became a handsome\",      \"tree. Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for. It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride.  When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace. Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so.  You go, cinderella, said she, covered in dust and\",      'dirt as you are, and would go to the festival.  You have no clothes and shoes, and yet would dance.  As, however, cinderella went on asking, the step-mother said at last, I have emptied a dish of lentils into the ashes for you, if you have picked them out again in two hours, you shall go with us.  The maiden went through the back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,      the bad into the crop. Then two white pigeons came in by the kitchen window, and afterwards the turtle-doves, and at last all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes. And the pigeons nodded with their heads and began pick, pick, pick, pick, and the rest began also pick, pick, pick, pick, and',      'gathered all the good grains into the dish.  Hardly had one hour passed before they had finished, and all flew out again.  Then the girl took the dish to her step-mother, and was glad, and believed that now she would be allowed to go with them to the festival. But the step-mother said, no, cinderella, you have no clothes and you can not dance.  You would only be laughed at.  And as cinderella wept at this, the step-mother said, if you can pick two dishes of lentils out of the ashes for me in one hour, you shall go with us.  And she thought to herself, that she most certainly cannot do again.  When the step-mother had emptied the two dishes of lentils amongst the ashes, the maiden went through the back-door into the garden and cried, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick      the good into the pot,',      \"the bad into the crop. Then two white pigeons came in by the kitchen-window, and afterwards the turtle-doves, and at length all the birds beneath the sky, came whirring and crowding in, and alighted amongst the ashes.  And the doves nodded with their heads and began pick, pick, pick, pick, and the others began also pick, pick, pick, pick, and gathered all the good seeds into the dishes, and before half an hour was over they had already finished, and all flew out again. Then the maiden was delighted, and believed that she might now go with them to the wedding.  But the step-mother said, all this will not help.  You cannot go with us, for you have no clothes and can not dance.  We should be ashamed of you.  On this she turned her back on cinderella, and hurried away with her two proud daughters. As no one was now at home, cinderella went to her mother's\",      'grave beneath the hazel-tree, and cried -      shiver and quiver, little tree,      silver and gold throw down over me. Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver.  She put on the dress with all speed, and went to the wedding.  Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress. They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes.  The prince approached her, took her by the hand and danced with her. He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner. She danced till it was evening, and then she wanted to go home.',      \"But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged. She escaped from him, however, and sprang into the pigeon-house.  The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house.  The old man thought, can it be cinderella.  And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it.  And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had\",      \"taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown. Next day when the festival began afresh, and her parents and the step-sisters had gone once more, cinderella went to the hazel-tree and said -      shiver and quiver, my little tree,      silver and gold throw down over me. Then the bird threw down a much more beautiful dress than on the preceding day. And when cinderella appeared at the wedding in this dress, every one was astonished at her beauty.  The king's son had waited until she came, and instantly took her by the hand and danced with no one but her.  When others came and invited her, he said, this is my partner.  When evening came she wished to leave, and the king's son followed her and wanted to see into which house she went.  But she sprang away from him, and into the garden behind the house.  Therein stood a beautiful tall tree on\",      \"which hung the most magnificent pears.  She clambered so nimbly between the branches like a squirrel that the king's son did not know where she was gone.  He waited until her father came, and said to him, the unknown maiden has escaped from me, and I believe she has climbed up the pear-tree.  The father thought, can it be cinderella.  And had an axe brought and cut the tree down, but no one was on it.  And when they got into the kitchen, cinderella lay there among the ashes, as usual, for she had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown. On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree -      shiver and quiver, my little tree,      silver and gold throw down over me.\",      \"And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden.  And when she went to the festival in the dress, no one knew how to speak for astonishment.  The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her.  The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck.  The king's son picked it up, and it was small and dainty, and all golden.  Next morning, he went with it to\",      \"the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits.  Then were the two sisters glad, for they had pretty feet.  The eldest went with the shoe into her room and wanted to try it on, and her mother stood by.  But she could not get her big toe into it, and the shoe was too small for her.  Then her mother gave her a knife and said, cut the toe off, when you are queen you will have no more need to go on foot.  The maiden cut the toe off, forced the foot into the shoe, swallowed the pain, and went out to the king's son.  Then he took her on his his horse as his bride and rode away with her.  They were obliged, however, to pass the grave, and there, on the hazel-tree, sat the two pigeons and cried -      turn and peep, turn and peep,      there's blood within the shoe,\",      \"the shoe it is too small for her,      the true bride waits for you. Then he looked at her foot and saw how the blood was trickling from it.  He turned his horse round and took the false bride home again, and said she was not the true one, and that the other sister was to put the shoe on.  Then this one went into her chamber and got her toes safely into the shoe, but her heel was too large.  So her mother gave her a knife and said,  cut a bit off your heel, when you are queen you will have no more need to go on foot.  The maiden cut a bit off her heel, forced her foot into the shoe, swallowed the pain, and went out to the king's son.  He took her on his horse as his bride, and rode away with her, but when they passed by the hazel-tree, the two pigeons sat on it and cried -      turn and peep, turn and peep,\",      \"there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you. He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red.  Then he turned his horse and took the false bride home again.  This also is not the right one, said he, have you no other daughter.  No, said the man, there is still a little stunted kitchen-wench which my late wife left behind her, but she cannot possibly be the bride.  The king's son said he was to send her up to him, but the mother answered, oh, no, she is much too dirty, she cannot show herself.  But he absolutely insisted on it, and cinderella had to be called.  She first washed her hands and face clean, and then went and bowed down before the king's son, who gave her the golden shoe.  Then she\",      \"seated herself on a stool, drew her foot out of the heavy wooden shoe, and put it into the slipper, which fitted like a glove.  And when she rose up and the king's son looked at her face he recognized the beautiful maiden who had danced with him and cried, that is the true bride.  The step-mother and the two sisters were horrified and became pale with rage, he, however, took cinderella on his horse and rode away with her.  As they passed by the hazel-tree, the two white doves cried -      turn and peep, turn and peep,      no blood is in the shoe,      the shoe is not too small for her,      the true bride rides with you, and when they had cried that, the two came flying down and placed themselves on cinderella's shoulders, one on the right, the other on the left, and remained sitting there. When the wedding with the king's son was to be celebrated, the\",      'two false sisters came and wanted to get into favor with cinderella and share her good fortune.  When the betrothed couple went to church, the elder was at the right side and the younger at the left, and the pigeons pecked out one eye from each of them.  Afterwards as they came back the elder was at the left, and the younger at the right, and then the pigeons pecked out the other eye from each.  And thus, for their wickedness and falsehood, they were punished with blindness all their days.'] ::: :::</p> <p>::: {#28ba43c70a109fe4 .cell .markdown id=\"28ba43c70a109fe4\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#connecting_embedding_model_to_indox","title":"Connecting Embedding Model to Indox","text":"<p>With our reference data chunked and ready, the next step is to connect our embedding model to the Indox system. This connection enables the system to leverage the embeddings for better semantic understanding and retrieval performance.</p> <p>We use the <code>connect_to_vectorstore</code> method to link the <code>HuggingFaceEmbedding</code> model with our Indox system. By specifying the embeddings and a collection name, we ensure that our reference data is appropriately indexed and stored, facilitating efficient retrieval during the question-answering process.</p> <p>Let\\'s connect the embedding model to Indox. :::</p> <p>::: {#fb0c5eb8341d52d3 .cell .code execution_count=\"13\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:53.236317Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:52.873727Z\\\"}\" id=\"fb0c5eb8341d52d3\"} <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed)\n</code></pre> :::</p> <p>::: {#bbd77c136e780844 .cell .code execution_count=\"14\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:44:53.242353Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:44:53.237325Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"bbd77c136e780844\" outputId=\"3dc71f1c-96a0-4c0f-cb44-611f2db058c0\"} <pre><code>indox.connect_to_vectorstore(vectorstore_database=db)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"14\"}      ::: ::: <p>::: {#7a2de351e4cb5e24 .cell .markdown id=\"7a2de351e4cb5e24\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#storing_data_in_the_vector_store","title":"Storing Data in the Vector Store","text":"<p>After connecting our embedding model to the Indox system, the next step is to store our chunked reference data in the vector store. This process ensures that our data is indexed and readily available for retrieval during the question-answering process.</p> <p>We use the <code>store_in_vectorstore</code> method to store the processed data in the vector store. By doing this, we enhance the system\\'s ability to quickly access and retrieve relevant information based on the embeddings generated earlier.</p> <p>Let\\'s proceed with storing the data in the vector store. :::</p> <p>::: {#8b53fee18caed89c .cell .code execution_count=\"15\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:07.044107Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:03.390792Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"8b53fee18caed89c\" outputId=\"ab39b1ac-03de-44e1-977b-b2157538f9a9\"} <pre><code>indox.store_in_vectorstore(docs)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"15\"}      ::: ::: <p>::: {#a2ac994fc0fb7ca0 .cell .markdown id=\"a2ac994fc0fb7ca0\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#query_from_rag_system_with_indox","title":"Query from RAG System with Indox","text":"<p>With our Retrieval-Augmented Generation (RAG) system built using Indox, we are now ready to test it with a sample question. This test will demonstrate how effectively our system can retrieve and generate accurate answers based on the reference data stored in the vector store.</p> <p>We\\'ll use a sample query to test our system:</p> <ul> <li>Query: \\\"How did Cinderella reach her happy ending?\\\"</li> </ul> <p>This question will be processed by our Indox system to retrieve relevant information and generate an appropriate response.</p> <p>Let\\'s test our RAG system with the sample question :::</p> <p>::: {#6b9fcd8f902257ad .cell .code execution_count=\"16\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:08.545059Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:08.541085Z\\\"}\" id=\"6b9fcd8f902257ad\"} <pre><code>query = \"How cinderella reach her happy ending?\"\n</code></pre> :::</p> <p>::: {#905f5aeb288a9ea3 .cell .markdown id=\"905f5aeb288a9ea3\"} Now that our Retrieval-Augmented Generation (RAG) system with Indox is fully set up, we can test it with a sample question. We\\'ll use the <code>invoke</code> submethod to get a response from the system.</p> <p>The <code>invoke</code> method processes the query using the connected QA model and retrieves relevant information from the vector store. It returns a list where:</p> <ul> <li>The first index contains the answer.</li> <li>The second index contains the contexts and their respective scores.</li> </ul> <p>We\\'ll pass this query to the <code>invoke</code> method and print the response. :::</p> <p>::: {#5a87158e90e6cb32 .cell .code execution_count=\"17\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:09.503644Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:09.500273Z\\\"}\" id=\"5a87158e90e6cb32\"} <pre><code>retriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)\n</code></pre> :::</p> <p>::: {#6fe8b6eff074cee .cell .code execution_count=\"18\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:16.862965Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:10.367688Z\\\"}\" id=\"6fe8b6eff074cee\"} <pre><code>answer = retriever.invoke(query=query)\n</code></pre> :::</p> <p>::: {#8a5500901a37dcf4 .cell .code execution_count=\"19\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:18.009646Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:18.005532Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\",\\\"height\\\":70}\" id=\"8a5500901a37dcf4\" outputId=\"db8b708c-4512-4350-d279-11d831804c8d\"} <pre><code>answer\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"19\"} <pre><code>{\"type\":\"string\"}\n</code></pre> ::: :::</p> <p>::: {#4a0b652a31bba343 .cell .code execution_count=\"20\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:30.087337Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:30.081831Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"4a0b652a31bba343\" outputId=\"8e7742d1-19f2-449a-e455-15259515e760\"} <pre><code>context = retriever.context\ncontext\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"20\"}     ['must earn it.  Out with the kitchen-wench.  They took her pretty clothes away from her, put an old grey bedgown on her, and gave her wooden shoes.  Just look at the proud princess, how decked out she is, they cried, and laughed, and led her into the kitchen. There she had to do hard work from morning till night, get up before daybreak, carry water, light fires, cook and wash.  Besides this, the sisters did her every imaginable injury - they mocked her and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again.  In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders.  And as on that account she always looked dusty and dirty, they called her cinderella. It happened that the father was once going to the fair, and he',      \"And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden.  And when she went to the festival in the dress, no one knew how to speak for astonishment.  The king's son danced with her only, and if any one invited her to dance, he said this is my partner. When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her.  The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck.  The king's son picked it up, and it was small and dainty, and all golden.  Next morning, he went with it to\",      \"there's blood within the shoe,      the shoe it is too small for her,      the true bride waits for you. He looked down at her foot and saw how the blood was running out of her shoe, and how it had stained her white stocking quite red.  Then he turned his horse and took the false bride home again.  This also is not the right one, said he, have you no other daughter.  No, said the man, there is still a little stunted kitchen-wench which my late wife left behind her, but she cannot possibly be the bride.  The king's son said he was to send her up to him, but the mother answered, oh, no, she is much too dirty, she cannot show herself.  But he absolutely insisted on it, and cinderella had to be called.  She first washed her hands and face clean, and then went and bowed down before the king's son, who gave her the golden shoe.  Then she\",      \"tree. Thrice a day cinderella went and sat beneath it, and wept and prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for. It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride.  When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace. Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so.  You go, cinderella, said she, covered in dust and\",      'grave beneath the hazel-tree, and cried -      shiver and quiver, little tree,      silver and gold throw down over me. Then the bird threw a gold and silver dress down to her, and slippers embroidered with silk and silver.  She put on the dress with all speed, and went to the wedding.  Her step-sisters and the step-mother however did not know her, and thought she must be a foreign princess, for she looked so beautiful in the golden dress. They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes.  The prince approached her, took her by the hand and danced with her. He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner. She danced till it was evening, and then she wanted to go home.'] ::: :::</p> <p>::: {#ef42aa27b31243a .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:45:34.018349Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:45:34.014766Z\\\"}\" id=\"ef42aa27b31243a\"} <pre><code>\n</code></pre> :::</p> <p>::: {#f32928ee1157509f .cell .markdown id=\"f32928ee1157509f\"}</p>"},{"location":"Indox/examples/hf_mistral_SimpleReader/#evaluation","title":"Evaluation","text":"<p>:::</p> <p>::: {#d6362d3145cb9f05 .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:46:07.503919Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:46:05.038476Z\\\"}\" id=\"d6362d3145cb9f05\"} <pre><code>from indox.evaluation import Evaluation\nevaluator = Evaluation([\"BertScore\"])\n</code></pre> :::</p> <p>::: {#717b701e7958a0c .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:46:14.343498Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:46:13.455591Z\\\"}\" id=\"717b701e7958a0c\"} <pre><code>inputs = {\n    \"question\" : query,\n    \"answer\" : answer,\n    \"context\" : context\n}\nresult = evaluator(inputs)\n</code></pre> :::</p> <p>::: {#73f30dd1c899a55d .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-07-02T07:46:19.019465Z\\\",\\\"start_time\\\":\\\"2024-07-02T07:46:19.011378Z\\\"}\" id=\"73f30dd1c899a55d\" outputId=\"f1f3f407-5ada-45f8-9a30-60a1b3542554\"} <pre><code>result\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"18\"} <pre><code>&lt;div&gt;\n&lt;style scoped&gt;\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n&lt;/style&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;0&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Precision&lt;/th&gt;\n      &lt;td&gt;0.547425&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Recall&lt;/th&gt;\n      &lt;td&gt;0.482575&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;F1-score&lt;/th&gt;\n      &lt;td&gt;0.507377&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/div&gt;\n</code></pre> ::: :::</p> <p>::: {#6e2160b7c2e28cb1 .cell .code id=\"6e2160b7c2e28cb1\"} <pre><code>\n</code></pre> :::</p>"},{"location":"Indox/examples/mistral_unstructured/","title":"Mistral Unstructured","text":""},{"location":"Indox/examples/mistral_unstructured/#mistral_as_question_answer_model","title":"Mistral As Question Answer Model","text":""},{"location":"Indox/examples/mistral_unstructured/#introduction","title":"Introduction","text":"<p>In this notebook, we will demonstrate how to securely handle <code>inDox</code> as system for question answering system with open source models which are available on internet like <code>Mistral</code>. so firstly you should buil environment variables and API keys in Python using the <code>dotenv</code> library. Environment variables are a crucial part of configuring your applications, especially when dealing with sensitive information like API keys.</p> <p>Let\\'s start by importing the required libraries and loading our environment variables.</p> <pre><code>!pip install mistralai\n!pip install indox\n!pip install chromadb\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nMISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY')\n</code></pre>"},{"location":"Indox/examples/mistral_unstructured/#import_essential_libraries","title":"Import Essential Libraries","text":"<p>Then, we import essential libraries for our <code>Indox</code> question answering system:</p> <ul> <li><code>IndoxRetrievalAugmentation</code>: Enhances the retrieval process for     better QA performance.</li> <li><code>Mistral</code>: A powerful QA model from Indox, built on top of the     semantic understanding.</li> <li><code>UnstructuredLoadAndSplit</code>: A utility for loading and splitting     unstructured data.</li> </ul> <p><pre><code>from indox import IndoxRetrievalAugmentation\nindox = IndoxRetrievalAugmentation()\n</code></pre> :::</p> <p>::: {#449eb2a7ca2e5bce .cell .markdown id=\"449eb2a7ca2e5bce\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#building_the_indox_system_and_initializing_models","title":"Building the Indox System and Initializing Models","text":"<p>Next, we will build our <code>inDox</code> system and initialize the Mistral question answering model along with the embedding model. This setup will allow us to leverage the advanced capabilities of Indox for our question answering tasks. :::</p> <p>::: {#ac5ff6002e2497b3 .cell .code execution_count=\"4\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:40:56.456996Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:40:51.790145Z\\\"}\" id=\"ac5ff6002e2497b3\"} <pre><code>from indox.llms import Mistral\nfrom indox.embeddings import MistralEmbedding\nmistral_qa = Mistral(api_key=MISTRAL_API_KEY)\nembed_mistral = MistralEmbedding(MISTRAL_API_KEY)\n</code></pre> :::</p> <p>::: {#fd23f48af26265ca .cell .markdown id=\"fd23f48af26265ca\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#setting_up_reference_directory_and_file_path","title":"Setting Up Reference Directory and File Path","text":"<p>To demonstrate the capabilities of our Indox question answering system, we will use a sample directory. This directory will contain our reference data, which we will use for testing and evaluation.</p> <p>First, we specify the path to our sample file. In this case, we are using a file named <code>sample.txt</code> located in our working directory. This file will serve as our reference data for the subsequent steps.</p> <p>Let\\'s define the file path for our reference data. :::</p> <p>::: {#9706a7ba1cc8deff .cell .code} <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n</code></pre> :::</p> <p>::: {#b38c913b696a2642 .cell .code execution_count=\"5\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:40:59.074020Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:40:59.071498Z\\\"}\" id=\"b38c913b696a2642\"} <pre><code>file_path = \"sample.txt\"\n</code></pre> :::</p> <p>::: {#e88dd6c433fc600c .cell .markdown id=\"e88dd6c433fc600c\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#chunking_reference_data_with_unstructuredloadandsplit","title":"Chunking Reference Data with UnstructuredLoadAndSplit","text":"<p>To effectively utilize our reference data, we need to process and chunk it into manageable parts. This ensures that our question answering system can efficiently handle and retrieve relevant information.</p> <p>We use the <code>UnstructuredLoadAndSplit</code> utility for this task. This tool allows us to load the unstructured data from our specified file and split it into smaller chunks. This process enhances the performance of our retrieval and QA models by making the data more accessible and easier to process.</p> <p>In this step, we define the file path for our reference data and use <code>UnstructuredLoadAndSplit</code> to chunk the data with a maximum chunk size of 400 characters.</p> <p>Let\\'s proceed with chunking our reference data. :::</p> <p>::: {#4dcc52c1d0416383 .cell .code execution_count=\"6\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:41:58.334662Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:41:58.008246Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"4dcc52c1d0416383\" outputId=\"c43a25f4-7c29-470c-8f82-6cfbb83be6d1\"} <pre><code>from indox.data_loader_splitter import UnstructuredLoadAndSplit\nload_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\ndocs = load_splitter.load_and_chunk()\n</code></pre></p> <p>::: {.output .stream .stderr}     [nltk_data] Downloading package punkt to /root/nltk_data...     [nltk_data]   Unzipping tokenizers/punkt.zip.     [nltk_data] Downloading package averaged_perceptron_tagger to     [nltk_data]     /root/nltk_data...     [nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip. ::: :::</p> <p>::: {#72d312cf4791f60f .cell .markdown id=\"72d312cf4791f60f\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#connecting_embedding_model_to_indox","title":"Connecting Embedding Model to Indox","text":"<p>With our reference data chunked and ready, the next step is to connect our embedding model to the Indox system. This connection enables the system to leverage the embeddings for better semantic understanding and retrieval performance.</p> <p>We use the <code>connect_to_vectorstore</code> method to link the <code>embed_mistral</code> model with our Indox system. By specifying the embeddings and a collection name, we ensure that our reference data is appropriately indexed and stored, facilitating efficient retrieval during the question-answering process.</p> <p>Let\\'s connect the embedding model to Indox. :::</p> <p>::: {#ebc33cc4fb58a305 .cell .code execution_count=\"7\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:03.601445Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:03.594047Z\\\"}\" id=\"ebc33cc4fb58a305\"} <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed_mistral)\n</code></pre> :::</p> <p>::: {#943f965096e65197 .cell .code execution_count=\"8\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:05.061630Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:05.055595Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"943f965096e65197\" outputId=\"0704ed8d-ace4-4112-c9dc-dbef9eab48b0\"} <pre><code>indox.connect_to_vectorstore(vectorstore_database=db)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"8\"}      ::: ::: <p>::: {#250da1a633bef038 .cell .markdown id=\"250da1a633bef038\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#storing_data_in_the_vector_store","title":"Storing Data in the Vector Store","text":"<p>After connecting our embedding model to the Indox system, the next step is to store our chunked reference data in the vector store. This process ensures that our data is indexed and readily available for retrieval during the question-answering process.</p> <p>We use the <code>store_in_vectorstore</code> method to store the processed data in the vector store. By doing this, we enhance the system\\'s ability to quickly access and retrieve relevant information based on the embeddings generated earlier.</p> <p>Let\\'s proceed with storing the data in the vector store. :::</p> <p>::: {#83b2f51f1a359477 .cell .code execution_count=\"9\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:15.228086Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:07.249961Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"83b2f51f1a359477\" outputId=\"c2d4c310-d550-4e15-9776-24b7c23a7ee8\"} <pre><code>indox.store_in_vectorstore(docs)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"9\"}      ::: ::: <p>::: {#7766ed35249fef6e .cell .markdown id=\"7766ed35249fef6e\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#query_from_rag_system_with_indox","title":"Query from RAG System with Indox","text":"<p>With our Retrieval-Augmented Generation (RAG) system built using Indox, we are now ready to test it with a sample question. This test will demonstrate how effectively our system can retrieve and generate accurate answers based on the reference data stored in the vector store.</p> <p>We\\'ll use a sample query to test our system:</p> <ul> <li>Query: \\\"How did Cinderella reach her happy ending?\\\"</li> </ul> <p>This question will be processed by our Indox system to retrieve relevant information and generate an appropriate response.</p> <p>Let\\'s test our RAG system with the sample question :::</p> <p>::: {#c30a41f4d7293b39 .cell .code execution_count=\"10\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:17.310350Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:17.306685Z\\\"}\" id=\"c30a41f4d7293b39\"} <pre><code>query = \"How cinderella reach her happy ending?\"\n</code></pre> :::</p> <p>::: {#58639a3d46eb327f .cell .markdown id=\"58639a3d46eb327f\"} Now that our Retrieval-Augmented Generation (RAG) system with Indox is fully set up, we can test it with a sample question. We\\'ll use the <code>invoke</code> submethod to get a response from the system.</p> <p>The <code>invoke</code> method processes the query using the connected QA model and retrieves relevant information from the vector store. It returns a list where:</p> <ul> <li>The first index contains the answer.</li> <li>The second index contains the contexts and their respective scores.</li> </ul> <p>We\\'ll pass this query to the <code>invoke</code> method and print the response. :::</p> <p>::: {#66ecb3768f04d326 .cell .code execution_count=\"11\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:18.376680Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:18.373295Z\\\"}\" id=\"66ecb3768f04d326\"} <pre><code>retriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5)\n</code></pre> :::</p> <p>::: {#9adbffdb7d5427bd .cell .code execution_count=\"12\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:27.798069Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:19.041579Z\\\"}\" id=\"9adbffdb7d5427bd\"} <pre><code>answer = retriever.invoke(query=query)\n</code></pre> :::</p> <p>::: {#f905f84906433aab .cell .code execution_count=\"13\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-26T08:42:32.700407Z\\\",\\\"start_time\\\":\\\"2024-06-26T08:42:32.696411Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\",\\\"height\\\":140}\" id=\"f905f84906433aab\" outputId=\"da7808e4-2408-4ff6-b185-a97413f08713\"} <pre><code>answer\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"13\"} <pre><code>{\"type\":\"string\"}\n</code></pre> ::: :::</p> <p>::: {#db289e0dae276aee .cell .code execution_count=\"14\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:23:16.751306Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:23:16.746109Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"db289e0dae276aee\" outputId=\"cafedfc6-1137-48ae-c095-b0052f393dcf\"} <pre><code>context = retriever.context\ncontext\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"14\"}     ['by the hearth in the cinders. And as on that account she always\\n\\nlooked dusty and dirty, they called her cinderella.\\n\\nIt happened that the father was once going to the fair, and he\\n\\nasked his two step-daughters what he should bring back for them.\\n\\nBeautiful dresses, said one, pearls and jewels, said the second.\\n\\nAnd you, cinderella, said he, what will you have. Father',      \"to appear among the number, they were delighted, called cinderella\\n\\nand said, comb our hair for us, brush our shoes and fasten our\\n\\nbuckles, for we are going to the wedding at the king's palace.\\n\\nCinderella obeyed, but wept, because she too would have liked to\\n\\ngo with them to the dance, and begged her step-mother to allow\\n\\nher to do so. You go, cinderella, said she, covered in dust and\",      \"danced with her only, and if any one invited her to dance, he said\\n\\nthis is my partner.\\n\\nWhen evening came, cinderella wished to leave, and the king's\\n\\nson was anxious to go with her, but she escaped from him so quickly\\n\\nthat he could not follow her. The king's son, however, had\\n\\nemployed a ruse, and had caused the whole staircase to be smeared\",      'cinderella expressed a wish, the bird threw down to her what she\\n\\nhad wished for.\\n\\nIt happened, however, that the king gave orders for a festival\\n\\nwhich was to last three days, and to which all the beautiful young\\n\\ngirls in the country were invited, in order that his son might choose\\n\\nhimself a bride. When the two step-sisters heard that they too were',      \"Then the maiden was delighted, and believed that she might now go\\n\\nwith them to the wedding. But the step-mother said, all this will\\n\\nnot help. You cannot go with us, for you have no clothes and can\\n\\nnot dance. We should be ashamed of you. On this she turned her\\n\\nback on cinderella, and hurried away with her two proud daughters.\\n\\nAs no one was now at home, cinderella went to her mother's\"] ::: :::</p> <p>::: {#ea002e2ab9469d7b .cell .markdown id=\"ea002e2ab9469d7b\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#evaluation","title":"Evaluation","text":"<p>Evaluating the performance of your question-answering system is crucial to ensure the quality and reliability of the responses. In this section, we will use the <code>Evaluation</code> module from Indox to assess our system\\'s outputs. :::</p> <p>::: {#48d4718f798523c8 .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:23:21.759373Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:23:16.751306Z\\\"}\" id=\"48d4718f798523c8\"} <pre><code>from indox.evaluation import Evaluation\nevaluator = Evaluation([\"BertScore\", \"Toxicity\"])\n</code></pre> :::</p> <p>::: {#dca587b000e0f3d5 .cell .markdown id=\"dca587b000e0f3d5\"}</p>"},{"location":"Indox/examples/mistral_unstructured/#preparing_inputs_for_evaluation","title":"Preparing Inputs for Evaluation","text":"<p>Next, we need to format the inputs according to the Indox evaluator\\'s requirements. This involves creating a dictionary that includes the question, the generated answer, and the context from which the answer was derived. :::</p> <p>::: {#26d130c534ed349f .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:23:22.516004Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:23:21.759373Z\\\"}\" id=\"26d130c534ed349f\"} <pre><code>inputs = {\n    \"question\" : query,\n    \"answer\" : answer,\n    \"context\" : context\n}\nresult = evaluator(inputs)\n</code></pre> :::</p> <p>::: {#da14c97311ae1028 .cell .code ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:23:22.534495Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:23:22.516004Z\\\"}\" id=\"da14c97311ae1028\" outputId=\"e982f515-31c3-4b31-89c2-7351a34a67e2\"} <pre><code>result\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"15\"} <pre><code>&lt;div&gt;\n&lt;style scoped&gt;\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n&lt;/style&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;0&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Precision&lt;/th&gt;\n      &lt;td&gt;0.524382&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Recall&lt;/th&gt;\n      &lt;td&gt;0.537209&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;F1-score&lt;/th&gt;\n      &lt;td&gt;0.530718&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;Toxicity&lt;/th&gt;\n      &lt;td&gt;0.074495&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/div&gt;\n</code></pre> ::: :::</p> <p>::: {#1c0e58d968847693 .cell .code id=\"1c0e58d968847693\"} <pre><code>\n</code></pre> :::</p>"},{"location":"Indox/examples/openai_clusterSplit/","title":"open AI clusterSplit","text":""},{"location":"Indox/examples/openai_clusterSplit/#load_and_split_with_clustering","title":"Load And Split With Clustering","text":"<pre><code>!pip install indox\n!pip install openai\n!pip install chromadb\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n</code></pre>"},{"location":"Indox/examples/openai_clusterSplit/#initial_setup","title":"Initial Setup","text":"<p>The following imports are essential for setting up the Indox application. These imports include the main Indox retrieval augmentation module, question-answering models, embeddings, and data loader splitter. :::</p> <p>::: {#506326bc .cell .code execution_count=\"3\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:01.771108Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:26:51.269942Z\\\"}\" id=\"506326bc\"} <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import OpenAi\nfrom indox.embeddings import OpenAiEmbedding\nfrom indox.data_loader_splitter import ClusteredSplit\n</code></pre> :::</p> <p>::: {#d8c124de .cell .markdown id=\"d8c124de\"} In this step, we initialize the Indox Retrieval Augmentation, the QA model, and the embedding model. Note that the models used for QA and embedding can vary depending on the specific requirements. :::</p> <p>::: {#8da2931c .cell .code execution_count=\"4\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:03.779477Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:01.772413Z\\\"}\" id=\"8da2931c\"} <pre><code>Indox = IndoxRetrievalAugmentation()\nqa_model = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo-0125\")\nembed = OpenAiEmbedding(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n</code></pre> :::</p> <p>::: {#7ddc88c0 .cell .markdown id=\"7ddc88c0\"}</p>"},{"location":"Indox/examples/openai_clusterSplit/#data_loader_setup","title":"Data Loader Setup","text":"<p>We set up the data loader using the <code>ClusteredSplit</code> class. This step involves loading documents, configuring embeddings, and setting options for processing the text. :::</p> <p>::: {#4f0280aa44ef805b .cell .code} <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\n</code></pre> :::</p> <p>::: {#8c5de9dc .cell .code execution_count=\"5\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:03.785039Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:03.779883Z\\\"}\" id=\"8c5de9dc\"} <pre><code>loader_splitter = ClusteredSplit(file_path=\"sample.txt\",embeddings=embed,remove_sword=False,re_chunk=False,chunk_size=300,summary_model=qa_model)\n</code></pre> :::</p> <p>::: {#f95f29ed .cell .code execution_count=\"6\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:09.779603Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:03.785039Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"f95f29ed\" outputId=\"60771a97-425e-47bb-af05-f78f49ede7c3\"} <pre><code>docs = loader_splitter.load_and_chunk()\n</code></pre></p> <p>::: {.output .stream .stdout}     --Generated 1 clusters-- ::: :::</p> <p>::: {#b8963612 .cell .markdown id=\"b8963612\"}</p>"},{"location":"Indox/examples/openai_clusterSplit/#vector_store_connection_and_document_storage","title":"Vector Store Connection and Document Storage","text":"<p>In this step, we connect the Indox application to the vector store and store the processed documents. :::</p> <p>::: {#28db7399 .cell .code execution_count=\"7\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:10.152713Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:09.787673Z\\\"}\" id=\"28db7399\"} <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed)\n</code></pre> :::</p> <p>::: {#74fda1aa .cell .code execution_count=\"8\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:10.157286Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:10.152713Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"74fda1aa\" outputId=\"63b0ebd8-9ef4-4166-f523-c6ccc253644e\"} <pre><code>Indox.connect_to_vectorstore(db)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"8\"}      ::: ::: <p>::: {#f0554a96 .cell .code execution_count=\"9\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:11.742575Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:10.157286Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"f0554a96\" outputId=\"dbdcd424-b293-488d-b049-52ba525b75fa\"} <pre><code>Indox.store_in_vectorstore(docs)\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"9\"}      ::: ::: <p>::: {#84dceb32 .cell .markdown id=\"84dceb32\"}</p>"},{"location":"Indox/examples/openai_clusterSplit/#querying_and_interpreting_the_response","title":"Querying and Interpreting the Response","text":"<p>In this step, we query the Indox application with a specific question and use the QA model to get the response. :::</p> <p>::: {#e9e2a586 .cell .code execution_count=\"10\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:22.888584Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:22.879723Z\\\"}\" id=\"e9e2a586\"} <pre><code>retriever = Indox.QuestionAnswer(vector_database=db,llm=qa_model,top_k=5)\n</code></pre> :::</p> <p>::: {#c89e2597 .cell .code execution_count=\"11\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:27.610041Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:23.181547Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\",\\\"height\\\":122}\" id=\"c89e2597\" outputId=\"66e536cc-ebc9-4cbc-860c-161232c9c3ec\"} <pre><code>retriever.invoke(query=\"How cinderella reach happy ending?\")\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"11\"} <pre><code>{\"type\":\"string\"}\n</code></pre> ::: :::</p> <p>::: {#7b766b26 .cell .code execution_count=\"12\" ExecuteTime=\"{\\\"end_time\\\":\\\"2024-06-09T10:27:27.615330Z\\\",\\\"start_time\\\":\\\"2024-06-09T10:27:27.610041Z\\\"}\" colab=\"{\\\"base_uri\\\":\\\"https://localhost:8080/\\\"}\" id=\"7b766b26\" outputId=\"687c96d3-2363-4355-fec2-af0e7aa43098\"} <pre><code>retriever.context\n</code></pre></p> <p>::: {.output .execute_result execution_count=\"12\"}     [\"They never once thought of cinderella, and believed that she was sitting at home in the dirt, picking lentils out of the ashes   The prince approached her, took her by the hand and danced with her He would dance with no other maiden, and never let loose of her hand, and if any one else came to invite her, he said, this is my partner She danced till it was evening, and then she wanted to go home But the king's son said, I will go with you and bear you company, for he wished to see to whom the beautiful maiden belonged She escaped from him, however, and sprang into the pigeon-house   The king's son waited until her father came, and then he told him that the unknown maiden had leapt into the pigeon-house   The old man thought, can it be cinderella   And they had to bring him an axe and a pickaxe that he might hew the pigeon-house to pieces, but no one was inside it   And when they got home cinderella lay in her dirty clothes among the ashes, and a dim little oil-lamp was burning on the mantle-piece, for cinderella had jumped quickly down from the back of the pigeon-house and had run to the little hazel-tree, and there she had taken off her beautiful clothes and laid them on the grave, and the bird had taken them away again, and then she had seated herself in the kitchen amongst the ashes in her grey gown\",      \"The documentation provided is a detailed retelling of the classic fairy tale of Cinderella. It starts with the story of a kind and pious girl who is mistreated by her stepmother and stepsisters after her mother's death. Despite their cruelty, Cinderella remains good and pious. Through the help of a magical hazel tree and a little white bird, Cinderella is able to attend a royal festival where she captures the attention of the prince.\\n\\nThe story unfolds with Cinderella attending the festival on three consecutive days, each time receiving a more splendid dress and accessories from the hazel tree. The prince is captivated by her beauty and dances only with her. However, her stepmother and stepsisters try to deceive the prince by mutilating\",      \"had jumped down on the other side of the tree, had taken the beautiful dress to the bird on the little hazel-tree, and put on her grey gown On the third day, when the parents and sisters had gone away, cinderella went once more to her mother's grave and said to the little tree -      shiver and quiver, my little tree,      silver and gold throw down over me And now the bird threw down to her a dress which was more splendid and magnificent than any she had yet had, and the slippers were golden   And when she went to the festival in the dress, no one knew how to speak for astonishment   The king's son danced with her only, and if any one invited her to dance, he said this is my partner When evening came, cinderella wished to leave, and the king's son was anxious to go with her, but she escaped from him so quickly that he could not follow her   The king's son, however, had employed a ruse, and had caused the whole staircase to be smeared with pitch, and there, when she ran down, had the maiden's left slipper remained stuck   The king's son picked it up, and it was small and dainty, and all golden   Next morning, he went with it to the father, and said to him, no one shall be my wife but she whose foot this golden slipper fits   Then were the two sisters glad,\",      \"and emptied her peas and lentils into the ashes, so that she was forced to sit and pick them out again   In the evening when she had worked till she was weary she had no bed to go to, but had to sleep by the hearth in the cinders   And as on that account she always looked dusty and dirty, they called her cinderella It happened that the father was once going to the fair, and he asked his two step-daughters what he should bring back for them Beautiful dresses, said one, pearls and jewels, said the second And you, cinderella, said he, what will you have   Father break off for me the first branch which knocks against your hat on your way home   So he bought beautiful dresses, pearls and jewels for his two step-daughters, and on his way home, as he was riding through a green thicket, a hazel twig brushed against him and knocked off his hat   Then he broke off the branch and took it with him   When he reached home he gave his step-daughters the things which they had wished for, and to cinderella he gave the branch from the hazel-bush   Cinderella thanked him, went to her mother's grave and planted the branch on it, and wept so much that the tears fell down on it and watered it   And it grew and became a handsome tree  Thrice a day cinderella went and sat beneath it, and wept and\",      \"prayed, and a little white bird always came on the tree, and if cinderella expressed a wish, the bird threw down to her what she had wished for It happened, however, that the king gave orders for a festival which was to last three days, and to which all the beautiful young girls in the country were invited, in order that his son might choose himself a bride   When the two step-sisters heard that they too were to appear among the number, they were delighted, called cinderella and said, comb our hair for us, brush our shoes and fasten our buckles, for we are going to the wedding at the king's palace Cinderella obeyed, but wept, because she too would have liked to go with them to the dance, and begged her step-mother to allow her to do so   You go, cinderella, said she, covered in dust and dirt as you are, and would go to the festival   You have no clothes and shoes, and yet would dance   As, however, cinderella went on asking, the step-mother said at last, I have emptied a dish of lentils into the ashes for you, if you have picked them out again in two hours, you shall go with us   The maiden went through the back-door into the garden, and called, you tame pigeons, you turtle-doves, and all you birds beneath the sky, come and help me to pick\"] ::: :::</p>"},{"location":"Indox/examples/unstructuredSplit/","title":"Unstructured Load And Split","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"Indox/examples/unstructuredSplit/#initial_setup","title":"Initial Setup","text":"<p>The following imports are essential for setting up the Indox application. These imports include the main Indox retrieval augmentation module, question-answering models, embeddings, and data loader splitter.</p> <pre><code>from indox import IndoxRetrievalAugmentation\nindox = IndoxRetrievalAugmentation()\n</code></pre>"},{"location":"Indox/examples/unstructuredSplit/#generating_response_using_openais_language_models","title":"Generating response using OpenAI's language models","text":"<p>OpenAIQA class is used to handle question-answering task using OpenAI's language models. This instance creates OpenAiEmbedding class to specifying embedding model. Here ChromaVectorStore handles the storage and retrieval of vector embeddings by specifying a collection name and sets up a vector store where text embeddings can be stored and queried.</p> <pre><code>from indox.llms import OpenAiQA\nfrom indox.embeddings import OpenAiEmbedding\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\nembed_openai = OpenAiEmbedding(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n\nfrom indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed_openai)\nindox.connect_to_vectorstore(vectorstore_database=db)\n</code></pre> <pre><code>&lt;indox.vector_stores.Chroma.ChromaVectorStore at 0x1d0407cb440&gt;\n</code></pre>"},{"location":"Indox/examples/unstructuredSplit/#load_and_preprocess_data","title":"load and preprocess data","text":"<p>This part of code demonstrates how to load and preprocess text data from a file, split it into chunks, and store these chunks in the vector store that was set up previously.</p> <pre><code>file_path = \"sample.txt\"\n</code></pre> <pre><code>from indox.data_loader_splitter import UnstructuredLoadAndSplit\nloader_splitter = UnstructuredLoadAndSplit(file_path=file_path,max_chunk_size=400)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <pre><code>indox.store_in_vectorstore(docs=docs)\n</code></pre> <pre><code>&lt;indox.vector_stores.Chroma.ChromaVectorStore at 0x1d0407cb440&gt;\n</code></pre>"},{"location":"Indox/examples/unstructuredSplit/#retrieve_relevant_information_and_generate_an_answer","title":"Retrieve relevant information and generate an answer","text":"<p>The main purpose of these lines is to perform a query on the vector store to retrieve the most relevant information (top_k=5) and generate an answer using the language model.</p> <pre><code>query = \"How Cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa, top_k=5)\n</code></pre> <p>invoke(query) method sends the query to the retriever, which searches the vector store for relevant text chunks and uses the language model to generate a response based on the retrieved information. Context property retrieves the context or the detailed information that the retriever used to generate the answer to the query. It provides insight into how the query was answered by showing the relevant text chunks and any additional information used.</p> <pre><code>retriever.invoke(query)\nretriever.context\n</code></pre> <pre><code>[\"to appear among the number, they were delighted, called cinderella\\n\\nand said, comb our hair for us, brush our shoes and fasten our\\n\\nbuckles, for we are going to the wedding at the king's palace.\\n\\nCinderella obeyed, but wept, because she too would have liked to\\n\\ngo with them to the dance, and begged her step-mother to allow\\n\\nher to do so. You go, cinderella, said she, covered in dust and\",\n \"which they had wished for, and to cinderella he gave the branch\\n\\nfrom the hazel-bush. Cinderella thanked him, went to her mother's\\n\\ngrave and planted the branch on it, and wept so much that the tears\\n\\nfell down on it and watered it. And it grew and became a handsome\\n\\ntree. Thrice a day cinderella went and sat beneath it, and wept and\\n\\nprayed, and a little white bird always came on the tree, and if\",\n 'by the hearth in the cinders. And as on that account she always\\n\\nlooked dusty and dirty, they called her cinderella.\\n\\nIt happened that the father was once going to the fair, and he\\n\\nasked his two step-daughters what he should bring back for them.\\n\\nBeautiful dresses, said one, pearls and jewels, said the second.\\n\\nAnd you, cinderella, said he, what will you have. Father',\n \"Then the maiden was delighted, and believed that she might now go\\n\\nwith them to the wedding. But the step-mother said, all this will\\n\\nnot help. You cannot go with us, for you have no clothes and can\\n\\nnot dance. We should be ashamed of you. On this she turned her\\n\\nback on cinderella, and hurried away with her two proud daughters.\\n\\nAs no one was now at home, cinderella went to her mother's\",\n \"danced with her only, and if any one invited her to dance, he said\\n\\nthis is my partner.\\n\\nWhen evening came, cinderella wished to leave, and the king's\\n\\nson was anxious to go with her, but she escaped from him so quickly\\n\\nthat he could not follow her. The king's son, however, had\\n\\nemployed a ruse, and had caused the whole staircase to be smeared\"]\n</code></pre>"},{"location":"Indox/examples/unstructuredSplit/#with_agenticrag","title":"With AgenticRag","text":"<p>AgenticRag stands for Agentic Retrieval-Augmented Generation. This concept combines retrieval-based methods and generation-based methods in natural language processing (NLP). The key idea is to enhance the generative capabilities of a language model by incorporating relevant information retrieved from a database or a vector store.   AgenticRag is designed to provide more contextually rich and accurate responses by utilizing external knowledge sources. It retrieves relevant pieces of information (chunks) from a vector store based on a query and then uses a language model to generate a comprehensive response that incorporates this retrieved information.</p> <pre><code>agent = indox.AgenticRag(llm=openai_qa,vector_database=db,top_k=5)\nagent.run(query)\n</code></pre> <pre><code>Not Relevant doc\nRelevant doc\nNot Relevant doc\nNot Relevant doc\nNot Relevant doc\n\n\n\n\n\n\"Cinderella reached her happy ending by receiving a branch from the hazel-bush from the prince, planting it on her mother's grave, and weeping and praying beneath it. The branch grew into a handsome tree, and a little white bird always came to the tree, bringing her comfort and assistance.\"\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Indox/graphs/memgraph/","title":"MemgraphDB","text":"<p>The <code>MemgraphDB</code> class allows you to use Memgraph as a graph database to store and query graph-structured data. This can be used in a Retrieval-Augmented Generation (RAG) system or other knowledge graph-related applications.</p>"},{"location":"Indox/graphs/memgraph/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>uri (str): The URI of the Memgraph instance (e.g., \"bolt://localhost:7687\").</li> <li>username (Optional[str]): The username for authenticating to the Memgraph database (optional, defaults to an empty string).</li> <li>password (Optional[str]): The password for authenticating to the Memgraph database (optional, defaults to an empty string).</li> </ul>"},{"location":"Indox/graphs/memgraph/#installation","title":"Installation","text":"<p>To use Memgraph with this package, you need to install the <code>neo4j</code> driver, as it works with Memgraph. You can install it using pip:</p> <pre><code>pip install neo4j\n</code></pre> <p>You will also need a running Memgraph instance. Refer to the official Memgraph installation guide for instructions on setting up a Memgraph instance locally or in the cloud.</p>"},{"location":"Indox/graphs/memgraph/#example_usage","title":"Example Usage","text":""},{"location":"Indox/graphs/memgraph/#initialize_the_memgraphdb","title":"Initialize the MemgraphDB","text":"<p>You can initialize the <code>MemgraphDB</code> class by passing the Memgraph connection details.</p> <pre><code>from indox.graph import MemgraphDB\n\nmemgraph_db = MemgraphDB(uri=\"bolt://localhost:7687\", username=\"username\", password=\"password\")\n</code></pre>"},{"location":"Indox/graphs/memgraph/#add_graph_documents_to_memgraph","title":"Add Graph Documents to Memgraph","text":"<p>To add graph documents (which contain nodes and relationships) to the Memgraph database, you can use the <code>add_graph_documents</code> method. This method allows you to store a list of graph documents with options to include source metadata and entity labels.</p> <pre><code># Assuming `graph_documents` is a list of GraphDocument objects you want to store\nmemgraph_db.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n</code></pre> <ul> <li>graph_documents (List[GraphDocument]): A list of <code>GraphDocument</code> objects to store in the Memgraph database.</li> <li>base_entity_label (bool, optional): If True, adds a base \"Entity\" label to all nodes. Defaults to True.</li> <li>include_source (bool, optional): If True, includes the source document as part of the graph. Defaults to True.</li> </ul>"},{"location":"Indox/graphs/memgraph/#query_relationships_by_entity","title":"Query Relationships by Entity","text":"<p>To query relationships in Memgraph, you can use the <code>search_relationships_by_entity</code> method. This method allows you to search for relationships by specifying the entity ID and the relationship type.</p> <pre><code># Search for parent relationships for the entity with ID \"Elizabeth_I\"\nrelationships = memgraph_db.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Print the found relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n</code></pre> <ul> <li>entity_id (str): The ID of the entity for which you want to find relationships.</li> <li>relationship_type (str): The type of relationship to search for (e.g., \"PARENT\").</li> </ul>"},{"location":"Indox/graphs/memgraph/#close_the_connection","title":"Close the Connection","text":"<p>After performing your operations, always remember to close the Memgraph connection:</p> <pre><code>memgraph_db.close()\n</code></pre>"},{"location":"Indox/graphs/memgraph/#example_workflow","title":"Example Workflow","text":"<p>Here's an example of how to use <code>MemgraphDB</code> to store and query relationships in Memgraph:</p> <pre><code>from indox.graph import MemgraphDB\n\n# Initialize MemgraphDB\nmemgraph_db = MemgraphDB(uri=\"bolt://localhost:7687\", username=\"username\", password=\"password\")\n\n# Assuming you have a list of GraphDocument objects ready to store\n# graph_documents = List of GraphDocument objects\n\n# Add the graph documents to Memgraph\nmemgraph_db.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n\n# Query relationships of an entity\nrelationships = memgraph_db.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Output the relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n\n# Close the Memgraph connection\nmemgraph_db.close()\n</code></pre>"},{"location":"Indox/splitters/AI21semantic_splitter/","title":"AI21SemanticTextSplitter","text":"<p>AI21SemanticTextSplitter is a Python class that utilizes the AI21 API for semantic text segmentation. It splits input text into meaningful segments and optionally merges these segments into larger chunks, maintaining semantic coherence. This tool is particularly useful for processing large texts while preserving context and meaning.</p> <p>Note: To use AI21SemanticTextSplitter, users need to have an AI21 API key and the <code>requests</code> library installed. The AI21 API key should be provided either as an environment variable or when initializing the class.</p> <p>To use AI21SemanticTextSplitter:</p> <pre><code>from indox.splitter import AI21SemanticTextSplitter\n\nsplitter = AI21SemanticTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50,\n    api_key=\"your_ai21_api_key\"\n)\n</code></pre>"},{"location":"Indox/splitters/AI21semantic_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: Maximum size of each chunk (default: 400). Set to 0 to disable merging.</li> <li>chunk_overlap [int]: Number of characters to overlap between chunks (default: 50).</li> <li>api_key [Optional[str]]: AI21 API key (optional if set as an environment variable).</li> <li>api_host [str]: Base URL for the AI21 API (default: \"https://api.ai21.com/studio/v1\").</li> <li>timeout_sec [Optional[float]]: Timeout for API requests in seconds (optional).</li> <li>num_retries [int]: Number of times to retry failed API calls (default: 3).</li> </ul>"},{"location":"Indox/splitters/AI21semantic_splitter/#usage","title":"Usage","text":""},{"location":"Indox/splitters/AI21semantic_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/splitters/AI21semantic_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/AI21semantic_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/AI21semantic_splitter/#get_started","title":"Get Started","text":""},{"location":"Indox/splitters/AI21semantic_splitter/#set_ai21_api_key_as_environment_variable","title":"Set AI21 API Key as Environment Variable","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv('api.env')\nAI21_API_KEY = os.getenv('AI21_API_KEY')\n</code></pre></p>"},{"location":"Indox/splitters/AI21semantic_splitter/#import_essential_libraries","title":"Import Essential Libraries","text":"<pre><code>from indox.splitter import AI21SemanticTextSplitter\n</code></pre>"},{"location":"Indox/splitters/AI21semantic_splitter/#initialize_ai21semantictextsplitter","title":"Initialize AI21SemanticTextSplitter","text":"<pre><code>splitter = AI21SemanticTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50\n)\n</code></pre>"},{"location":"Indox/splitters/AI21semantic_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>TEXT = (\n    \"We\u2019ve all experienced reading long, tedious, and boring pieces of text - financial reports, \"\n    \"legal documents, or terms and conditions (though, who actually reads those terms and conditions to be honest?).\\n\"\n    \"Imagine a company that employs hundreds of thousands of employees. In today's information \"\n    \"overload age, nearly 30% of the workday is spent dealing with documents. There's no surprise \"\n    \"here, given that some of these documents are long and convoluted on purpose (did you know that \"\n    \"reading through all your privacy policies would take almost a quarter of a year?). Aside from \"\n    \"inefficiency, workers may simply refrain from reading some documents (for example, Only 16% of \"\n    \"Employees Read Their Employment Contracts Entirely Before Signing!).\\nThis is where AI-driven summarization \"\n    \"tools can be helpful: instead of reading entire documents, which is tedious and time-consuming, \"\n    \"users can (ideally) quickly extract relevant information from a text. With large language models, \"\n    \"the development of those tools is easier than ever, and you can offer your users a summary that is \"\n    \"specifically tailored to their preferences.\\nLarge language models naturally follow patterns in input \"\n    \"(prompt), and provide coherent completion that follows the same patterns. For that, we want to feed \"\n    'them with several examples in the input (\"few-shot prompt\"), so they can follow through. '\n    \"The process of creating the correct prompt for your problem is called prompt engineering, \"\n    \"and you can read more about it here.\"\n)\n\nsemantic_text_splitter = AI21SemanticTextSplitter()\nchunks = semantic_text_splitter.split_text(TEXT)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor chunk in chunks:\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"Indox/splitters/Charachter_splitter/","title":"CharacterTextSplitter","text":"<p>CharacterTextSplitter is a Python class designed for splitting text into chunks based on a specified separator. It implements an algorithm to split text into chunks of a specified size, with an optional overlap between chunks.</p> <p>Note: This class is part of a text splitting module. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use CharacterTextSplitter:</p> <pre><code>from indox.splitter import CharacterTextSplitter\n\nsplitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Charachter_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>separator [str]: The string used to split the text (default: \"\\n\\n\").</li> <li>chunk_size [int]: The maximum size of each chunk (default: 400).</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks (default: 50).</li> <li>length_function [callable]: A function used to calculate the length of text (default: len).</li> </ul>"},{"location":"Indox/splitters/Charachter_splitter/#usage","title":"Usage","text":""},{"location":"Indox/splitters/Charachter_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/splitters/Charachter_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Charachter_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Charachter_splitter/#import_the_charactertextsplitter","title":"Import the CharacterTextSplitter","text":"<pre><code>from indox.splitter import CharacterTextSplitter\n</code></pre>"},{"location":"Indox/splitters/Charachter_splitter/#initialize_charactertextsplitter","title":"Initialize CharacterTextSplitter","text":"<pre><code>splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Charachter_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"Indox/splitters/Markdown_text_splitter/","title":"MarkdownTextSplitter","text":"<p>MarkdownTextSplitter is a Python class designed for splitting Markdown-formatted text into chunks. It extends the RecursiveCharacterTextSplitter to specifically handle Markdown-formatted text, splitting along headings and other Markdown-specific separators.</p> <p>Note: This class is part of the <code>indox.splitter</code> module and requires the <code>RecursiveCharacterTextSplitter</code> as a base class. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use MarkdownTextSplitter:</p> <pre><code>from indox.splitter import MarkdownTextSplitter\n\nsplitter = MarkdownTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Markdown_text_splitter/#hyperparameters","title":"Hyperparameters","text":"<p>MarkdownTextSplitter inherits all parameters from RecursiveCharacterTextSplitter. Key parameters include:</p> <ul> <li>chunk_size [int]: The maximum size of each text chunk.</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks.</li> <li>length_function [Callable[[str],int]]: Function to measure the length of given text.</li> </ul>"},{"location":"Indox/splitters/Markdown_text_splitter/#usage","title":"Usage","text":""},{"location":"Indox/splitters/Markdown_text_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/splitters/Markdown_text_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Markdown_text_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Markdown_text_splitter/#import_the_markdowntextsplitter","title":"Import the MarkdownTextSplitter","text":"<pre><code>from indox.splitter import MarkdownTextSplitter\n</code></pre>"},{"location":"Indox/splitters/Markdown_text_splitter/#initialize_markdowntextsplitter","title":"Initialize MarkdownTextSplitter","text":"<pre><code>splitter = MarkdownTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Markdown_text_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>markdown_text = \"\"\"\n# Main Heading\n\n## Subheading 1\nContent for subheading 1...\n\n## Subheading 2\nContent for subheading 2...\n\n***\n\nSome more content with horizontal rule above.\n\"\"\"\n\nchunks = splitter.split_text(markdown_text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor chunk in chunks:\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"Indox/splitters/Recursively_splitter/","title":"RecursiveCharacterTextSplitter","text":"<p>RecursiveCharacterTextSplitter is a Python class designed for splitting text into chunks recursively based on specified separators. It implements a recursive algorithm to split text into chunks of a specified size, with an optional overlap between chunks.</p> <p>Note: This class is part of the <code>indox.splitter</code> module. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use RecursiveCharacterTextSplitter:</p> <pre><code>from indox.splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Recursively_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: The maximum size of each chunk (default: 400).</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks (default: 50).</li> <li>separators [Optional[List[str]]]: A list of separators to use for splitting the text (default: [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]).</li> </ul>"},{"location":"Indox/splitters/Recursively_splitter/#usage","title":"Usage","text":""},{"location":"Indox/splitters/Recursively_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/splitters/Recursively_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Recursively_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Recursively_splitter/#import_the_recursivecharactertextsplitter","title":"Import the RecursiveCharacterTextSplitter","text":"<pre><code>from indox.splitter import RecursiveCharacterTextSplitter\n</code></pre>"},{"location":"Indox/splitters/Recursively_splitter/#initialize_recursivecharactertextsplitter","title":"Initialize RecursiveCharacterTextSplitter","text":"<pre><code>splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"Indox/splitters/Recursively_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"Indox/splitters/Semantic_splitter/","title":"SemanticTextSplitter","text":"<p>SemanticTextSplitter is a Python class designed for splitting text into semantically meaningful chunks using a BERT tokenizer. It utilizes the semantic_text_splitter library to split input text into chunks that preserve semantic meaning while ensuring each chunk does not exceed a specified maximum number of tokens.</p> <p>Note: This class requires the semantic_text_splitter and tokenizers libraries. Ensure you have these dependencies installed before using this class.</p> <p>To use SemanticTextSplitter:</p> <pre><code>from indox.splitter import SemanticTextSplitter\n\nsplitter = SemanticTextSplitter(chunk_size=400, model_name=\"bert-base-uncased\")\n</code></pre>"},{"location":"Indox/splitters/Semantic_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: The maximum number of tokens allowed in each chunk (default: 400).</li> <li>model_name [str]: The name of the pre-trained model to use for the tokenizer (default: \"bert-base-uncased\").</li> </ul>"},{"location":"Indox/splitters/Semantic_splitter/#usage","title":"Usage","text":""},{"location":"Indox/splitters/Semantic_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/splitters/Semantic_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Semantic_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/splitters/Semantic_splitter/#install_required_libraries","title":"Install Required Libraries","text":"<pre><code>pip install semantic-text-splitter tokenizers\n</code></pre>"},{"location":"Indox/splitters/Semantic_splitter/#import_the_semantictextsplitter","title":"Import the SemanticTextSplitter","text":"<pre><code>from indox.splitter import SemanticTextSplitter\n</code></pre>"},{"location":"Indox/splitters/Semantic_splitter/#initialize_semantictextsplitter","title":"Initialize SemanticTextSplitter","text":"<pre><code>splitter = SemanticTextSplitter(chunk_size=400, model_name=\"bert-base-uncased\")\n</code></pre>"},{"location":"Indox/splitters/Semantic_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"Indox/tools/multiquery/","title":"MultiQueryRetrieval Documentation","text":""},{"location":"Indox/tools/multiquery/#overview","title":"Overview","text":"<p>The <code>MultiQueryRetrieval</code> class implements an advanced information retrieval technique that generates multiple queries from an original query, retrieves relevant information for each generated query, and combines the results to produce a comprehensive final response.</p>"},{"location":"Indox/tools/multiquery/#class_multiqueryretrieval","title":"Class: MultiQueryRetrieval","text":""},{"location":"Indox/tools/multiquery/#initialization","title":"Initialization","text":"<pre><code>def __init__(self, llm, vector_database, top_k: int = 3):\n</code></pre> <ul> <li>llm: The language model used for query generation and response synthesis.</li> <li>vector_database: The vector database used for information retrieval.</li> <li>top_k: The number of top results to retrieve for each query (default: 3).</li> </ul>"},{"location":"Indox/tools/multiquery/#methods","title":"Methods","text":"<p>generate_queries <pre><code>def generate_queries(self, original_query: str) -&gt; List[str]:\n</code></pre></p> <p>Generates multiple queries from the original query.</p> <ul> <li>original_query: The original user query.</li> <li>Returns: A list of generated queries.</li> </ul> <p>retrieve_information <pre><code>def retrieve_information(self, queries: List[str]) -&gt; List[str]:\n</code></pre></p> <p>Retrieves relevant information for each generated query.</p> <ul> <li>queries: A list of queries to use for information retrieval.</li> <li>Returns: A list of relevant passages retrieved from the vector database.</li> </ul> <p>generate_response <pre><code>def generate_response(self, original_query: str, context: List[str]) -&gt; str:\n</code></pre> Generates a final response based on the original query and retrieved context.</p> <ul> <li>original_query: The original user query.</li> <li>context: A list of relevant passages to use as context.</li> <li>Returns: The generated response.</li> </ul> <p>run <pre><code>def run(self, query: str) -&gt; str:\n</code></pre> Executes the full multi-query retrieval process.</p> <ul> <li>query: The original user query.</li> <li>Returns: The final generated response.</li> </ul>"},{"location":"Indox/tools/multiquery/#usage","title":"Usage","text":"<p>To use the MultiQueryRetrieval functionality within the <code>IndoxRetrievalAugmentation</code> class:</p> <ol> <li>Initialize the IndoxRetrievalAugmentation class:</li> </ol> <p><pre><code>ira = IndoxRetrievalAugmentation()\n</code></pre> 2. Create a <code>QuestionAnswer</code> instance: <pre><code>qa = ira.QuestionAnswer(llm=lm, vector_database=vector_db, top_k=5)\n</code></pre> 3. Use the <code>invoke</code> method to perform multi-query retrieval: <pre><code>response = qa.invoke(\"Your complex query here\",multi_query=True)\n</code></pre></p>"},{"location":"Indox/tools/multiquery/#example","title":"Example","text":"<pre><code>from indox import IndoxRetrievalAugmentation\n\n# Initialize the retrieval augmentation\nira = IndoxRetrievalAugmentation()\n\n# Create a QuestionAnswer instance\nqa = ira.QuestionAnswer(\n    llm=llm,\n    vector_database=vector_db,\n    top_k=5\n)\n\n# Perform multi-query retrieval\nquery = \"What are the main differences between renewable and non-renewable energy sources?\"\nresponse = qa.invoke(query,multi_query=True)\n\nprint(f\"Query: {query}\")\nprint(f\"Response: {response}\")\n</code></pre>"},{"location":"Indox/tools/multivector/","title":"<code>MultiVectorRetriever</code>","text":""},{"location":"Indox/tools/multivector/#overview","title":"Overview","text":"<p>The <code>MultiVectorRetriever</code> class allows users to perform similarity searches across multiple vector stores in parallel and retrieve results based on their similarity score. This class is particularly useful for projects that need to integrate several vector stores and combine search results, such as building a multivector hybrid search system.</p>"},{"location":"Indox/tools/multivector/#key_features","title":"Key Features","text":"<ul> <li>Perform similarity searches on multiple vector stores simultaneously.</li> <li>Combine and sort the results from all vector stores based on similarity scores.</li> <li>Handle exceptions during the search process and log any issues.</li> </ul>"},{"location":"Indox/tools/multivector/#constructor___init__self_vector_stores_listany","title":"Constructor: <code>__init__(self, vector_stores: List[Any])</code>","text":"<p>This method initializes the <code>MultiVectorRetriever</code> object.</p>"},{"location":"Indox/tools/multivector/#parameters","title":"Parameters:","text":"<ul> <li><code>vector_stores</code> (List[Any]): A list of initialized vector store instances, each capable of performing similarity searches (e.g., Deeplake, ApacheCassandra).</li> </ul>"},{"location":"Indox/tools/multivector/#method_similarity_search_with_scoreself_query_str_k_int__5_-_listtupleany_float","title":"Method: <code>similarity_search_with_score(self, query: str, k: int = 5) -&gt; List[Tuple[Any, float]]</code>","text":"<p>Executes similarity searches across all vector stores, returning the most relevant results based on their similarity scores.</p>"},{"location":"Indox/tools/multivector/#parameters_1","title":"Parameters:","text":"<ul> <li><code>query</code> (str): The query text to be searched across the vector stores.</li> <li><code>k</code> (int): The number of results to return. Defaults to 5.</li> </ul>"},{"location":"Indox/tools/multivector/#returns","title":"Returns:","text":"<ul> <li>List[Tuple[Any, float]]: A list of tuples where each tuple contains a document and a similarity score. The list is sorted in descending order of similarity.</li> </ul>"},{"location":"Indox/tools/multivector/#raises","title":"Raises:","text":"<ul> <li>Logs any exceptions encountered during the search process.</li> </ul>"},{"location":"Indox/tools/multivector/#example_usage","title":"Example Usage","text":"<p>In this example, we initialize two vector stores (<code>Deeplake</code> and <code>ApacheCassandra</code>), add documents to them, and then create a <code>MultiVectorRetriever</code> instance. A query is then run across both vector stores, retrieving the top 5 most similar results.</p> <pre><code>from indox.vector_stores import Chroma, Milvus, ApacheCassandra, FAISS, PGVector, Deeplake\nfrom indox.multi_vector_retriever import MultiVectorRetriever\n\n# Define vector store instances\ndb1 = Deeplake(embedding_function=embed_openai_indox, collection_name=\"sample\")\ndb2 = ApacheCassandra(embedding_function=embed_openai_indox, keyspace=\"sample\")\n\n# Add documents to the vector stores\ndb1.add(docs=docs1)\ndb2.add(docs=docs2)\n\n# Initialize MultiVectorRetriever with multiple vector stores\nmultivector = MultiVectorRetriever(vector_stores=[db1, db2])\n\n# Define the query\nquery = \"How cinderella reach her happy ending?\"\n\n# Perform the retrieval\nretriever = indox.QuestionAnswer(vector_database=multivector, llm=openai_qa_indox, top_k=5)\n\n# Example query results\nresults = retriever.retrieve(query=query)\n</code></pre>"},{"location":"Indox/tools/multivector/#explanation","title":"Explanation","text":"<ol> <li> <p>Vector Store Initialization: We initialize two vector stores, <code>Deeplake</code> and <code>ApacheCassandra</code>. Each of these is configured with an embedding function and other parameters specific to the store.</p> </li> <li> <p>Adding Documents: Documents (<code>docs1</code>, <code>docs2</code>) are added to the vector stores, allowing them to store vectors for future searches.</p> </li> <li> <p>MultiVectorRetriever: A <code>MultiVectorRetriever</code> object is created, taking in a list of vector stores (<code>db1</code>, <code>db2</code>).</p> </li> <li> <p>Query: The query string, \"How cinderella reach her happy ending?\", is used to search across all vector stores. The results are returned from the <code>QuestionAnswer</code> system, which uses the <code>MultiVectorRetriever</code> for similarity search and <code>openai_qa_indox</code> as the language model.</p> </li> <li> <p>Retrieval: The <code>retriever.retrieve()</code> method executes the query and retrieves the top 5 results based on their similarity scores.</p> </li> </ol>"},{"location":"Indox/tools/multivector/#logging","title":"Logging","text":"<p>This class uses the <code>loguru</code> library to log the following: - INFO level logs for general information. - ERROR level logs for exceptions encountered during the search process.</p>"},{"location":"Indox/vectorstores/ApachCassandra/","title":"Apache Cassandra","text":""},{"location":"Indox/vectorstores/ApachCassandra/#apachecassandra_vector_store","title":"<code>ApacheCassandra</code> Vector store","text":""},{"location":"Indox/vectorstores/ApachCassandra/#overview","title":"Overview","text":"<p>The <code>ApacheCassandra</code> class provides an interface to store, manage, and search through text data using embeddings, with the Apache Cassandra database as the backend. This class enables you to add documents with their embeddings to Cassandra and perform similarity searches to retrieve the most relevant documents.</p>"},{"location":"Indox/vectorstores/ApachCassandra/#attributes","title":"Attributes","text":"<ul> <li>cluster_ips (List[str]): List of Cassandra node IP addresses (currently hardcoded to <code>127.0.0.1</code>).</li> <li>port (int): Port number to connect to Cassandra (default is <code>9042</code>).</li> <li>embedding_function (Callable[[str], np.ndarray]): A function that takes text input and returns an embedding vector as a NumPy array.</li> <li>keyspace (str): The keyspace (database) in Cassandra to use for storage.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#methods","title":"Methods","text":""},{"location":"Indox/vectorstores/ApachCassandra/#__init__self_embedding_function_callablestr_npndarray_keyspace_str","title":"<code>__init__(self, embedding_function: Callable[[str], np.ndarray], keyspace: str)</code>","text":"<p>Initializes the <code>ApacheCassandra</code> instance.</p> <ul> <li>Parameters:</li> <li><code>embedding_function</code>: A callable that computes embedding vectors from text (used for both adding documents and queries).</li> <li> <p><code>keyspace</code>: The Cassandra keyspace where the data is stored.</p> </li> <li> <p>Raises:</p> </li> <li><code>RuntimeError</code>: If Cassandra connection initialization fails.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#_setup_keyspaceself","title":"<code>_setup_keyspace(self)</code>","text":"<p>Private method to set up the Cassandra keyspace and create the required table for storing embeddings.</p> <ul> <li>Raises:</li> <li><code>RuntimeError</code>: If the keyspace or table setup fails.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#addself_docs_liststr","title":"<code>add(self, docs: List[str])</code>","text":"<p>Adds a list of documents to the Cassandra vector store by embedding the documents and storing the embeddings.</p> <ul> <li>Parameters:</li> <li> <p><code>docs</code>: A list of text documents to be embedded and stored in the Cassandra database.</p> </li> <li> <p>Raises:</p> </li> <li><code>ValueError</code>: If <code>docs</code> is not a list of strings.</li> <li><code>RuntimeError</code>: If there is any issue while adding the documents to the Cassandra store.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#similarity_search_with_scoreself_query_str_k_int__5_-_listtupledocument_float","title":"<code>similarity_search_with_score(self, query: str, k: int = 5) -&gt; List[Tuple[Document, float]]</code>","text":"<p>Performs a similarity search on the Cassandra vector store using the provided query. The method returns the most similar documents along with their cosine similarity scores.</p> <ul> <li>Parameters:</li> <li><code>query</code>: A string query text for which similar documents are to be found.</li> <li> <p><code>k</code>: An integer specifying the number of top results to return (default: <code>5</code>).</p> </li> <li> <p>Returns:</p> </li> <li> <p>A list of tuples where each tuple contains:</p> <ul> <li><code>Document</code>: The document object corresponding to the retrieved text.</li> <li><code>float</code>: The similarity score (cosine similarity) between the query and the document.</li> </ul> </li> <li> <p>Raises:</p> </li> <li><code>RuntimeError</code>: If there is an issue with the Cassandra operation or similarity search.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#shutdownself","title":"<code>shutdown(self)</code>","text":"<p>Shuts down the Cassandra cluster connection.</p> <ul> <li>Raises:</li> <li><code>RuntimeError</code>: If Cassandra shutdown fails.</li> </ul>"},{"location":"Indox/vectorstores/ApachCassandra/#example_usage","title":"Example Usage","text":"<pre><code>from indox.vector_stores import ApacheCassandra\nfrom indox.embeddings import embed_openai_indox\nfrom indox.llms import openai_qa_indox\nfrom indox.core import Document\n\n# Initialize Apache Cassandra vector store with OpenAI embedding function\ndb = ApacheCassandra(embedding_function=embed_openai_indox, keyspace=\"sample\")\n\ndb.add(docs=docs)\n\n# Perform a similarity search with a query\nquery = \"How did Cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa_indox, top_k=5)\n\nretriever.invoke(query)\n\n# Shutdown the Cassandra connection\ndb.shutdown()\n</code></pre>"},{"location":"Indox/vectorstores/ApachCassandra/#explanation","title":"Explanation:","text":"<ol> <li> <p>Initialization: The <code>ApacheCassandra</code> instance is created by passing an embedding function (<code>embed_openai_indox</code>) and specifying a keyspace (<code>\"sample\"</code>).</p> </li> <li> <p>Adding Documents: A list of documents (<code>docs</code>) is embedded and stored in the Cassandra vector store via the <code>add</code> method.</p> </li> <li> <p>Similarity Search: A query is provided (<code>\"How did Cinderella reach her happy ending?\"</code>), and a <code>QuestionAnswer</code> retriever is used to perform a similarity search with the top 5 most relevant documents.</p> </li> <li> <p>Retrieval: After invoking the retriever with the query, the relevant context is extracted from the results and printed.</p> </li> <li> <p>Shutdown: The Cassandra connection is gracefully closed using the <code>shutdown</code> method.</p> </li> </ol>"},{"location":"Indox/vectorstores/Couchbase/","title":"Couchbase Vector Store","text":""},{"location":"Indox/vectorstores/Couchbase/#overview","title":"Overview","text":"<p>This code defines a <code>Couchbase</code> class that integrates with Couchbase, allowing the storage and retrieval of documents using vector embeddings. It provides methods to add documents with their corresponding embeddings to a Couchbase bucket and perform similarity searches based on a query. The embedding function is used to convert the text into vectors for storage and querying.</p> <p>The class utilizes the <code>indox</code> library for embedding functions and document representation. The <code>similarity_search_with_score</code> method returns a ranked list of documents with their similarity scores, enabling a search functionality based on vector similarity.</p>"},{"location":"Indox/vectorstores/Couchbase/#requirements","title":"Requirements","text":"<ul> <li>Couchbase server instance with a properly configured bucket.</li> <li><code>couchbase</code> Python SDK (Cluster, Bucket, etc.)</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#class_couchbase","title":"Class: <code>Couchbase</code>","text":""},{"location":"Indox/vectorstores/Couchbase/#constructor___init__","title":"Constructor: <code>__init__</code>","text":"<pre><code>def __init__(self, embedding_function: Callable[[str], np.ndarray], bucket_name: str,\n             cluster_url: str = 'couchbase://localhost', username: str = 'Administrator',\n             password: str = 'indoxmain')\n</code></pre>"},{"location":"Indox/vectorstores/Couchbase/#parameters","title":"Parameters:","text":"<ul> <li>embedding_function (<code>Callable[[str], np.ndarray]</code>): A function used to embed the documents and queries into vector representations.</li> <li>bucket_name (<code>str</code>): The name of the Couchbase bucket used to store the documents.</li> <li>cluster_url (<code>str</code>, optional): The URL of the Couchbase cluster (default: <code>couchbase://localhost</code>).</li> <li>username (<code>str</code>, optional): Username for Couchbase authentication (default: <code>Administrator</code>).</li> <li>password (<code>str</code>, optional): Password for Couchbase authentication (default: <code>indoxmain</code>).</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#exceptions","title":"Exceptions:","text":"<ul> <li>CouchbaseException: Raised if there's an error in connecting to Couchbase.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#method_add","title":"Method: <code>add</code>","text":"<pre><code>def add(self, docs: List[str])\n</code></pre>"},{"location":"Indox/vectorstores/Couchbase/#parameters_1","title":"Parameters:","text":"<ul> <li>docs (<code>List[str]</code>): A list of document strings to be added to the Couchbase bucket.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#functionality","title":"Functionality:","text":"<ul> <li>Each document in the list is embedded using the <code>embedding_function</code>, generating a vector representation.</li> <li>A unique document ID is generated using <code>uuid.uuid4()</code>.</li> <li>The document's content and its vector embedding are stored in Couchbase using the <code>upsert</code> method.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#exceptions_1","title":"Exceptions:","text":"<ul> <li>ValueError: Raised if the <code>docs</code> argument is not a list.</li> <li>CouchbaseException: Raised if there's an error while adding documents to Couchbase.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#method_similarity_search_with_score","title":"Method: <code>similarity_search_with_score</code>","text":"<pre><code>def similarity_search_with_score(self, query: str, k: int = 5) -&gt; List[Tuple[Document, float]]\n</code></pre>"},{"location":"Indox/vectorstores/Couchbase/#parameters_2","title":"Parameters:","text":"<ul> <li>query (<code>str</code>): The query string for which similarity search is performed.</li> <li>k (<code>int</code>, optional): The number of top results to return (default: 5).</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#returns","title":"Returns:","text":"<ul> <li>List[Tuple[Document, float]]: A list of tuples containing <code>Document</code> objects and their corresponding similarity scores, sorted in descending order of similarity.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#functionality_1","title":"Functionality:","text":"<ul> <li>Embeds the query using the <code>embedding_function</code>.</li> <li>Searches for similar documents in the Couchbase bucket using full-text search and retrieves documents along with their similarity scores.</li> <li>Constructs <code>Document</code> objects with the retrieved content and metadata, such as document ID.</li> <li>The results are sorted by similarity score and returned as a list of <code>(Document, score)</code> tuples.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#exceptions_2","title":"Exceptions:","text":"<ul> <li>CouchbaseException: Raised if there's an error during the similarity search or document retrieval.</li> </ul>"},{"location":"Indox/vectorstores/Couchbase/#example_usage","title":"Example Usage","text":"<pre><code>import numpy as np\nfrom indox.vector_stores import Couchbase\n\n# Initialize the Couchbase instance with an embedding function\ndb = Couchbase(embedding_function=embed_openai_indox, bucket_name=\"QA\")\n\ndb.add(docs)\n\n# Query the database for a similarity search\nquery = \"Does Travelers Insurance Have Renters Insurance?\"\n\n# Use the Couchbase instance within the indox.QuestionAnswer class\nretriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa_indox, top_k=5)\n\n# Invoke the retriever with the query\nretriever.invoke(query)\n</code></pre>"},{"location":"Indox/vectorstores/Couchbase/#explanation","title":"Explanation:","text":"<ol> <li>Embedding Function: The Couchbase class uses an embedding function (e.g., <code>embed_openai_indox</code>) to convert text into vector embeddings before storing or querying.</li> <li>Querying for Similarity: The <code>similarity_search_with_score</code> method is called within the <code>indox.QuestionAnswer</code> class, enabling a hybrid retrieval system that returns documents based on vector similarity.</li> <li>Retrieval Process: The <code>retriever.invoke(query)</code> is responsible for executing the query and retrieving the relevant results. It utilizes the Couchbase class for vector database interaction and the <code>openai_qa_indox</code> LLM for question answering.</li> </ol>"},{"location":"Indox/vectorstores/Couchbase/#output","title":"Output:","text":"<p>The retrieval process returns the most similar documents to the query, along with their relevance scores, allowing for efficient similarity-based search.</p>"},{"location":"Indox/vectorstores/Couchbase/#notes","title":"Notes:","text":"<ul> <li>Ensure that the Couchbase Full-Text Search (FTS) index <code>FTS_QA</code> is properly set up for querying.</li> <li>Customize the <code>embedding_function</code> to match the vectorization model used in your project.</li> </ul>"},{"location":"Indox/vectorstores/Deeplake/","title":"Deeplake","text":""},{"location":"Indox/vectorstores/Deeplake/#deeplake_vector_store","title":"Deeplake vector store","text":""},{"location":"Indox/vectorstores/Deeplake/#overview","title":"Overview","text":"<p>The <code>Deeplake</code> class interfaces with a DeepLake-based vector store for storing and searching text data using embeddings. This class provides methods to add documents to the vector store and perform similarity searches.</p>"},{"location":"Indox/vectorstores/Deeplake/#attributes","title":"Attributes","text":"<ul> <li>collection_name (str): The name of the collection to store in the vector store.</li> <li>embedding_function (callable): A function that takes text input and returns an embedding vector.</li> </ul>"},{"location":"Indox/vectorstores/Deeplake/#example_usage","title":"Example Usage","text":"<pre><code># Import necessary classes from Indox library\nfrom indox.llms import IndoxApi\nfrom indox.embeddings import IndoxApiEmbedding\nfrom indox.data_loader_splitter import ClusteredSplit\nfrom indox.vector_stores import Deeplake\n\n# Create instances for API access and text embedding\nopenai_qa_indox = IndoxApi(api_key=INDOX_API_KEY)\nembed_openai_indox = IndoxApiEmbedding(api_key=INDOX_API_KEY, model=\"text-embedding-3-small\")\n\n# Specify the path to your text file\nfile_path = \"sample.txt\"\n\n# Create a ClusteredSplit instance for handling file loading and chunking\nloader_splitter = ClusteredSplit(file_path=file_path, embeddings=embed_openai_indox, summary_model=openai_qa_indox)\n\n# Load and split the document into chunks using ClusteredSplit\ndocs = loader_splitter.load_and_chunk()\n\n# Initialize the Deeplake instance\ncollection_name = \"sample\"\ndb = Deeplake(collection_name=collection_name, embedding_function=embed_openai_indox)\n\n# Add documents to the vector store\ndb.add(docs=docs)\n\n# Perform a similarity search\nquery = \"example query text\"\nresults = db.similarity_search_with_score(query=query, k=5)\n</code></pre>"},{"location":"Indox/vectorstores/Faiss/","title":"Faiss","text":"<p>To use Faiss as the vector store, users need to install faiss and set the embedding model.</p> <pre><code>pip install faiss-cpu\n</code></pre>"},{"location":"Indox/vectorstores/Faiss/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>embedding (Embedding): The embedding to be used.</li> </ul>"},{"location":"Indox/vectorstores/Faiss/#installation","title":"Installation","text":"<p>For instructions on installing faiss, refer to the FAISS installation guide.</p> <pre><code>from indox.vector_stores import FAISSVectorStore\ndb = FAISSVectorStore(embedding=embed)\n</code></pre>"},{"location":"Indox/vectorstores/Faiss/#usage","title":"Usage","text":"<p>Store documents in the vector store:</p> <pre><code>db.add(docs=docs)\n</code></pre> <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/Vespa/","title":"Vespa Vector store","text":""},{"location":"Indox/vectorstores/Vespa/#overview","title":"Overview","text":"<p>This code defines a <code>VESPA</code> class that interacts with a Vespa application to handle text embeddings, document storage, and semantic search functionality. The Vespa application uses a tensor-based embedding for semantic search and integrates with an embedding model. The documents are processed by extracting titles, generating embeddings, and storing them in Vespa. Users can query the stored documents using semantic similarity based on embeddings.</p> <p>The class provides methods to deploy the Vespa application, add documents, retrieve relevant documents based on queries, and display the results in a pandas DataFrame. It also includes a <code>similarity_search_with_score</code> method to return top documents and their similarity scores.</p>"},{"location":"Indox/vectorstores/Vespa/#key_components","title":"Key Components","text":"<ul> <li>extract_title(text: str): Extracts the first sentence from the input text to use as a title.</li> <li>generate_id(): Generates a unique identifier using UUID for each document.</li> <li>process_docs(input_list: List[str], embedding_function: Any): Processes a list of text documents, generating titles, embeddings, and document IDs.</li> <li>VESPA class: Main class to interact with Vespa. It manages deployment, adding documents, querying, and similarity search.</li> <li><code>__init__(app_name: str, embedding_function: Any)</code>: Initializes the Vespa instance, configures the schema, and determines the embedding dimensions dynamically.</li> <li><code>deploy()</code>: Deploys the Vespa application using Docker.</li> <li><code>add(docs: List[Dict[str, Any]])</code>: Adds a list of documents to Vespa, embedding them before ingestion.</li> <li><code>_get_relevant_documents(query: str)</code>: Retrieves relevant documents for a given query using semantic search.</li> <li><code>display_hits_as_df(documents: List[Document])</code>: Displays the retrieved documents and their relevance as a pandas DataFrame.</li> <li><code>similarity_search_with_score(query: str, k: int)</code>: Performs a similarity search and returns the top <code>k</code> documents along with their scores.</li> </ul>"},{"location":"Indox/vectorstores/Vespa/#example_usage","title":"Example Usage","text":"<pre><code># Initialize Vespa instance with an application name and embedding function\ndb = VESPA(app_name=\"xxxx\", embedding_function=embed_openai_indox)\n\n# Add the documents to Vespa, embedding each one\ndb.add(docs=docs)\n\n# Perform a semantic search on Vespa for relevant documents based on a query\nquery = \"How did Cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=openai_qa_indox, top_k=5)\n\n# Retrieve the most relevant context based on the query\ncontext = retriever.context\nprint(context)\n</code></pre>"},{"location":"Indox/vectorstores/Vespa/#explanation","title":"Explanation","text":"<ol> <li> <p>Initializing Vespa: <code>VESPA(app_name=\"xxxx\", embedding_function=embed_openai_indox)</code> initializes a Vespa instance with the given application name and an embedding function (<code>embed_openai_indox</code>), which will handle generating embeddings for the documents.</p> </li> <li> <p>Adding Documents: <code>db.add(docs=docs)</code> processes and adds a list of document texts to Vespa. Each document is embedded using the specified embedding function and stored in the Vespa schema.</p> </li> <li> <p>Querying: The query <code>\"How did Cinderella reach her happy ending?\"</code> is processed using the <code>QuestionAnswer</code> retriever from the <code>indox</code> library. The retriever queries the Vespa instance (<code>db</code>) to retrieve relevant documents based on semantic similarity.</p> </li> <li> <p>Result Retrieval: The <code>retriever.context</code> retrieves and displays the most relevant documents and their contexts for the query.</p> </li> </ol>"},{"location":"Indox/vectorstores/chroma/","title":"Chroma","text":"<p>To use chroma as the vector store, users need to install Chroma and set the collection_name of database and embedding model.</p> <pre><code>pip install chromadb\n</code></pre>"},{"location":"Indox/vectorstores/chroma/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>collection_name (str): The name of the collection in the database.</li> <li>embedding (Embedding): The embedding to be used.</li> </ul>"},{"location":"Indox/vectorstores/chroma/#installation","title":"Installation","text":"<p>For instructions on installing chroma, refer to the chroma installation guide.</p> <pre><code>from indox.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"name\",embedding=embed)\n</code></pre>"},{"location":"Indox/vectorstores/chroma/#usage","title":"Usage","text":"<p>Store documents in the vector store:</p> <pre><code>db.add(docs=docs)\n</code></pre> <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/duckdb/","title":"DuckDB","text":"<p><code>DuckDB</code> is a lightweight, in-memory database solution designed for storing text documents and their vector embeddings. It supports adding documents, performing similarity searches using cosine similarity, and managing data within an in-memory DuckDB instance.</p> <p>Note: To use <code>DuckDB</code>, users need to have the <code>duckdb</code> package installed and provide an embedding function for generating vector embeddings.</p> <p>To use <code>DuckDB</code> as the vector store:</p> <pre><code>from indox.vector_stores import DuckDB\nfrom indox.embeddings import HuggingFaceEmbedding\n\ndb = DuckDB(\n    embedding_function=HuggingFaceEmbedding(),\n    table_name=\"your_table\"\n)\n</code></pre>"},{"location":"Indox/vectorstores/duckdb/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>embedding_function [Any]: Function to generate embeddings for the documents (must be provided).</li> <li>vector_key [str]: Column name for storing the embeddings in the table (default: \"embedding\").</li> <li>id_key [str]: Column name for unique document IDs (default: \"id\").</li> <li>text_key [str]: Column name for storing the text of documents (default: \"text\").</li> <li>table_name [str]: Name of the DuckDB table to store embeddings (default: \"embeddings\").</li> </ul>"},{"location":"Indox/vectorstores/duckdb/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/duckdb/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/duckdb/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/duckdb/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/duckdb/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre></p> <p>Import Essential Libraries <pre><code>from indox.llms import HuggingFaceModel\nfrom indox.embeddings import AzureOpenAIEmbeddings\nfrom indox import IndoxRetrievalAugmentation\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nazure_embed = AzureOpenAIEmbeddings(api_key=OPENAI_API_KEY,model=\"text-embedding-3-small\")\n</code></pre> Setting Up Reference Directory and File Path <pre><code>from indox.splitter import RecursiveCharacterTextSplitter\n!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\nfile_path = \"sample.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\nsplitter = RecursiveCharacterTextSplitter(400,20)\ncontent_chunks = splitter.split_text(text)\n</code></pre> Initialize DuckDB <pre><code>from indox.vector_stores import DuckDB\nvector_store = DuckDB(\n    embedding_function=azure_embed,\n    vector_key=\"embedding\",   \n    id_key=\"id\",              \n    text_key=\"text\",          \n    table_name=\"embeddings\"\n)\n</code></pre> Connecting VectorStore to Indox <pre><code>vector_store.add(texts=content_chunks)\n</code></pre> Querying the Database <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=vector_store, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/lantern/","title":"LanternDB","text":"<p>LanternDB is a document storage solution using PostgreSQL for storing text documents and their vector embeddings. It supports adding documents, performing similarity searches with cosine similarity, and deleting documents from a specified collection.</p> <p>Note: To use LanternDB as the vector store, users need to have a running PostgreSQL instance and provide the connection parameters when initializing the LanternDB vector store.</p> <p>To use LanternDB as the vector store:</p> <pre><code>from indox.vector_stores import LanternDB\nfrom indox.embeddings import HuggingFaceEmbedding\n\ndb = LanternDB(\n    collection_name=\"your_collection\",\n    embedding_function=HuggingFaceEmbedding(),\n    connection_params={\n        'dbname': 'your_db',\n        'user': 'your_user',\n        'password': 'your_password',\n        'host': 'host',\n        'port': 'port'\n    }\n)\n</code></pre>"},{"location":"Indox/vectorstores/lantern/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>collection_name [str]: Name of the PostgreSQL table (collection) to use.</li> <li>embedding_function [Optional[Embeddings]]: Function to generate embeddings for documents.</li> <li>connection_params [dict]: Connection parameters for PostgreSQL (e.g., dbname, user, password, host, port).</li> <li>dimension [int]: Dimensionality of vector embeddings (default: 768).</li> <li>text_key [str]: Key used to store document text in the collection (default: \"text\").</li> <li>vector_key [str]: Key used to store document embeddings in the collection (default: \"embedding\").</li> </ul>"},{"location":"Indox/vectorstores/lantern/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/lantern/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/lantern/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/lantern/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/lantern/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n</code></pre> Import Essential Libraries <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter.SimpleLoadAndSplit import SimpleLoadAndSplit\nfrom indox.vector_stores import MongoDB\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre> Setting Up Reference Directory and File Path <pre><code>from indox.splitter import SemanticTextSplitter\n\n!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\nfile_path = \"sample.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\nsplitter = SemanticTextSplitter(400)\ncontent_chunks = splitter.split_text(text)\n</code></pre> Initialize MongoDB <pre><code>db = LanternDB(\n    collection_name=\"your_collection\",\n    embedding_function=HuggingFaceEmbedding(),\n    connection_params={\n        'dbname': 'your_db',\n        'user': 'your_user',\n        'password': 'your_password',\n        'host': 'localhost',\n        'port': 'port'\n    }\n)\n</code></pre> Connecting VectorStore to Indox <pre><code>db.add_texts(content_chunks)\n</code></pre> Querying the Database <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/memgraphvector/","title":"MemgraphVector","text":"<p>The <code>MemgraphVector</code> class allows you to use Memgraph for similarity search on graph-structured data by leveraging embeddings. This can be used for tasks such as information retrieval and recommendation systems where vector or keyword search is needed. The class supports different search methods: vector-based, keyword-based, and hybrid search.</p>"},{"location":"Indox/vectorstores/memgraphvector/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>uri (str): The URI of the Memgraph instance (e.g., <code>bolt://localhost:7687</code>).</li> <li>username (str): The username for authenticating to the Memgraph database.</li> <li>password (str): The password for authenticating to the Memgraph database.</li> <li>embedding_function (Callable): A function to convert query text into embeddings (e.g., OpenAI model, custom embedding function).</li> <li>search_type (str, optional): The default type of search ('vector', 'keyword', or 'hybrid'). Defaults to <code>'vector'</code>.</li> </ul>"},{"location":"Indox/vectorstores/memgraphvector/#installation","title":"Installation","text":"<p>To use <code>MemgraphVector</code>, you need to install the <code>neo4j</code> driver and ensure you have a running Memgraph instance. You can install the required driver with:</p> <pre><code>pip install neo4j\n</code></pre> <p>Refer to the official Memgraph documentation for setting up a Memgraph instance.</p>"},{"location":"Indox/vectorstores/memgraphvector/#example_usage","title":"Example Usage","text":""},{"location":"Indox/vectorstores/memgraphvector/#initialize_memgraphvector","title":"Initialize MemgraphVector","text":"<p>You can initialize the <code>MemgraphVector</code> class by passing the necessary connection details and the embedding function.</p> <pre><code>from indox.vector_stores import MemgraphVector\n\n# Initialize MemgraphVector with connection details and an embedding function\nmemgraph_vector = MemgraphVector(\n    uri=\"bolt://localhost:7687\",\n    username=\"username\",\n    password=\"password\",\n    embedding_function=my_embedding_function,  # Your embedding function\n    search_type='vector'\n)\n</code></pre>"},{"location":"Indox/vectorstores/memgraphvector/#perform_a_similarity_search","title":"Perform a Similarity Search","text":"<p>To perform a similarity search, use the <code>search</code> method, which supports vector, keyword, or hybrid search types.</p> <pre><code># Run a similarity search\nresults = memgraph_vector.search(query=\"Sample query text\", search_type=\"vector\", k=5)\n\n# Print the results\nfor document in results:\n    print(document.page_content)\n</code></pre> <ul> <li>query (str): The search query text.</li> <li>search_type (str, optional): The type of search to perform ('vector', 'keyword', or 'hybrid'). Defaults to the class-level search type.</li> <li>k (int, optional): The number of top results to return. Defaults to 4.</li> </ul>"},{"location":"Indox/vectorstores/memgraphvector/#retrieve_documents_with_scores","title":"Retrieve Documents with Scores","text":"<p>You can also retrieve documents along with their similarity scores using the <code>retrieve</code> method.</p> <pre><code># Retrieve documents with similarity scores\ncontexts, scores = memgraph_vector.retrieve(query=\"Sample query\", top_k=5, search_type=\"hybrid\")\n\n# Output the documents and their scores\nfor context, score in zip(contexts, scores):\n    print(f\"Document: {context}, Score: {score}\")\n</code></pre> <ul> <li>top_k (int, optional): The number of top results to return. Defaults to 5.</li> <li>search_type (str, optional): The type of search to perform ('vector', 'keyword', or 'hybrid'). Defaults to the class-level search type.</li> </ul>"},{"location":"Indox/vectorstores/memgraphvector/#close_the_connection","title":"Close the Connection","text":"<p>After performing your operations, remember to close the Memgraph connection:</p> <pre><code>memgraph_vector.close()\n</code></pre>"},{"location":"Indox/vectorstores/memgraphvector/#search_methods","title":"Search Methods","text":"<p>The <code>MemgraphVector</code> class supports the following search methods:</p> <ul> <li>_run_vector_search: Searches for similar embeddings based on cosine similarity.</li> <li>_run_keyword_search: Searches for documents containing the keyword query.</li> <li>_run_hybrid_search: Combines vector and keyword search with weighted scores.</li> </ul>"},{"location":"Indox/vectorstores/memgraphvector/#example_workflow","title":"Example Workflow","text":"<pre><code>from indox.vector_stores import MemgraphVector\n\n# Initialize MemgraphVector\nmemgraph_vector = MemgraphVector(\n    uri=\"bolt://localhost:7687\",\n    username=\"username\",\n    password=\"password\",\n    embedding_function=my_embedding_function,\n    search_type=\"vector\"\n)\n\n# Perform vector similarity search\nvector_results = memgraph_vector.search(query=\"Machine learning\", search_type=\"vector\", k=5)\n\n# Perform hybrid search\nhybrid_results = memgraph_vector.search(query=\"Artificial intelligence\", search_type=\"hybrid\", k=5)\n\n# Close the Memgraph connection\nmemgraph_vector.close()\n</code></pre>"},{"location":"Indox/vectorstores/milvus/","title":"Milvus","text":""},{"location":"Indox/vectorstores/milvus/#step_1_install_docker","title":"Step 1: Install Docker","text":"<ol> <li>Install Docker if you don\u2019t have it yet.</li> <li>Follow the official Docker installation guide for your OS:<ul> <li>Windows</li> <li>MacOS</li> <li>Linux</li> </ul> </li> </ol>"},{"location":"Indox/vectorstores/milvus/#verify","title":"Verify","text":"<ul> <li>Command to Verify Installation: <pre><code>docker --version\n</code></pre></li> <li>Expected Output: <pre><code>Docker version 20.10.7, build f0df350\n</code></pre></li> <li>Error Handling:</li> <li>Docker Command Not Found:<ul> <li>Ensure Docker is installed. Follow the installation guide for your OS.</li> <li>Reinstall Docker if necessary.</li> </ul> </li> <li>Docker Daemon Not Running:<ul> <li>Start Docker service:   <pre><code>sudo systemctl start docker\n</code></pre></li> <li>Verify Docker is running:   <pre><code>sudo systemctl status docker\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#2_install_docker_compose_if_needed","title":"2. Install Docker Compose (if needed)","text":"<ul> <li>Command to Verify Installation: <pre><code>docker-compose --version\n</code></pre></li> <li>Expected Output: <pre><code>docker-compose version 1.29.2, build 5becea4c\n</code></pre></li> <li>Error Handling:</li> <li>Docker Compose Command Not Found:<ul> <li>Install Docker Compose by following the installation guide.</li> </ul> </li> <li>Permission Issues:<ul> <li>Run commands with <code>sudo</code> if necessary:   <pre><code>sudo docker-compose --version\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#3_clone_the_milvus_repository","title":"3. Clone the Milvus Repository","text":"<ul> <li>Command: <pre><code>git clone https://github.com/milvus-io/milvus.git\n</code></pre></li> <li>Expected Output: <pre><code>Cloning into 'milvus'...\nremote: Enumerating objects: 1234, done.\nremote: Counting objects: 100% (1234/1234), done.\nremote: Compressing objects: 100% (567/567), done.\nremote: Total 1234 (delta 678), reused 1234 (delta 678), pack-reused 0\nReceiving objects: 100% (1234/1234), 123.45 MiB | 12.34 MiB/s, done.\nResolving deltas: 100% (678/678), done.\n</code></pre></li> <li>Error Handling:</li> <li>Git Command Not Found:<ul> <li>Ensure Git is installed:   <pre><code>sudo apt-get install git\n</code></pre></li> </ul> </li> <li>Network Issues:<ul> <li>Check your internet connection.</li> <li>Verify the repository URL.</li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#4_change_directory_to_docker_deployment","title":"4. Change Directory to Docker Deployment","text":"<ul> <li>Command: <pre><code>cd milvus/deployments/docker\n</code></pre></li> <li>Error Handling:</li> <li>Directory Not Found:<ul> <li>Verify that the repository was cloned successfully.</li> <li>Check the path for typos.</li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#5_pull_milvus_docker_images","title":"5. Pull Milvus Docker Images","text":"<ul> <li>Command: <pre><code>docker-compose pull\n</code></pre></li> <li>Expected Output: <pre><code>Pulling milvus     ... done\nPulling etcd       ... done\nPulling minio      ... done\nPulling pulsar     ... done\n</code></pre></li> <li>Error Handling:</li> <li>Image Pull Failure:<ul> <li>Check network connectivity.</li> <li>Verify Docker Hub is reachable.</li> <li>Retry pulling images:   <pre><code>docker-compose pull\n</code></pre></li> </ul> </li> <li>Authentication Issues:<ul> <li>Ensure you are logged into Docker Hub:   <pre><code>docker login\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#6_start_milvus_with_docker_compose","title":"6. Start Milvus with Docker Compose","text":"<ul> <li>Command: <pre><code>docker-compose up -d\n</code></pre></li> <li>Expected Output: <pre><code>Creating network \"docker_default\" with the default driver\nCreating volume \"docker_minio\" with default driver\nCreating volume \"docker_pulsar\" with default driver\nCreating milvus ... done\nCreating etcd   ... done\nCreating minio  ... done\nCreating pulsar ... done\n</code></pre></li> <li>Error Handling:</li> <li>Containers Not Starting:<ul> <li>Check the status of the containers:   <pre><code>docker-compose ps\n</code></pre></li> <li>View logs for detailed error messages:   <pre><code>docker-compose logs\n</code></pre></li> </ul> </li> <li>Port Conflicts:<ul> <li>Check if ports are already in use:   <pre><code>sudo lsof -i :19530\nsudo kill -9 &lt;PID&gt;\n</code></pre></li> <li>Modify ports in <code>docker-compose.yml</code> if necessary.</li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#7_check_running_containers","title":"7. Check Running Containers","text":"<ul> <li>Command to Check Container Status: <pre><code>docker-compose ps\n</code></pre></li> <li>Expected Output: <pre><code>Name                 Command                  State                 Ports\n---------------------------------------------------------------------------------\ndocker_etcd_1         etcd --name etcd --data  Up      2379/tcp, 2380/tcp\ndocker_minio_1        /usr/bin/docker-entrpoi  Up      9000/tcp\ndocker_milvus_1       /bin/sh -c 'milvus-ser  Up      0.0.0.0:19530-&gt;19530/tcp\ndocker_pulsar_1       /pulsar/bin/pulsar ser  Up      6650/tcp, 8080/tcp\n</code></pre></li> <li>Error Handling:</li> <li>Containers Not Running:<ul> <li>Check logs for errors:   <pre><code>docker-compose logs\n</code></pre></li> <li>Ensure no errors during startup.</li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#8_test_milvus_connection_with_python_sdk","title":"8. Test Milvus Connection with Python SDK","text":"<ul> <li>Python Code to Test Connection: <pre><code>from pymilvus import connections\n\ntry:\n    connections.connect(host='127.0.0.1', port='19530')\n    print(\"Connection successful\")\nexcept Exception as e:\n    print(f\"Connection failed: {e}\")\n</code></pre></li> <li>Expected Output: <pre><code>Connection successful\n</code></pre></li> <li>Error Handling:</li> <li>Connection Failed:<ul> <li>Verify Milvus is running and accessible at the specified host and port.</li> <li>Check container logs:   <pre><code>docker-compose logs milvus\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#9_stop_milvus","title":"9. Stop Milvus","text":"<ul> <li>Command: <pre><code>docker-compose down\n</code></pre></li> <li>Expected Output: <pre><code>Stopping pulsar  ... done\nStopping minio   ... done\nStopping etcd    ... done\nStopping milvus  ... done\nRemoving pulsar  ... done\nRemoving minio   ... done\nRemoving etcd    ... done\nRemoving milvus  ... done\nRemoving network docker_default\n</code></pre></li> <li>Error Handling:</li> <li>Containers Not Stopping:<ul> <li>Manually stop containers:   <pre><code>docker stop &lt;container_id&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/milvus/#10_remove_containers_and_volumes","title":"10. Remove Containers and Volumes","text":"<ul> <li>Command: <pre><code>docker-compose down -v\n</code></pre></li> <li>Expected Output: <pre><code>Stopping pulsar  ... done\nStopping minio   ... done\nStopping etcd    ... done\nStopping milvus  ... done\nRemoving pulsar  ... done\nRemoving minio   ... done\nRemoving etcd    ... done\nRemoving milvus  ... done\nRemoving network docker_default\nRemoving volume docker_minio\nRemoving volume docker_pulsar\n</code></pre></li> <li>Error Handling:</li> <li>Volumes Not Removed:<ul> <li>List and remove volumes manually if needed:   <pre><code>docker volume ls\ndocker volume rm &lt;volume_name&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"Indox/vectorstores/mongodb/","title":"MongoDB","text":"<p>MongoDB is a document-oriented NoSQL database that provides high performance, high availability, and easy scalability. This implementation uses MongoDB for document storage and retrieval, supporting vector similarity search using cosine similarity when an embedding function is provided.</p> <p>Note: To use MongoDB as the vector store, users need to install pymongo and have a running MongoDB instance. The MongoDB connection string, database name, and collection name should be provided when initializing the MongoDB vector store.</p> <p>To use MongoDB as the vector store:</p> <pre><code>from indox.vector_stores import MongoDB\nfrom indox.embeddings import HuggingFaceEmbedding\n\ndb = MongoDB(\n    collection_name=\"your_collection\",\n    embedding_function=HuggingFaceEmbedding(),\n    connection_string=\"mongodb://localhost:27017/\",\n    database_name=\"vector_db\"\n)\n</code></pre>"},{"location":"Indox/vectorstores/mongodb/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>collection_name [str]: Name of the MongoDB collection to use.</li> <li>embedding_function [Optional[Embeddings]]: Function to generate embeddings for documents.</li> <li>connection_string [str]: MongoDB connection string (default: \"mongodb://localhost:27017/\").</li> <li>database_name [str]: Name of the MongoDB database to use (default: \"vector_db\").</li> <li>index_name [str]: Name of the index used for similarity search (default: \"default\").</li> <li>text_key [str]: Key used to store document text in the collection (default: \"text\").</li> <li>embedding_key [str]: Key used to store document embeddings in the collection (default: \"embedding\").</li> </ul>"},{"location":"Indox/vectorstores/mongodb/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/mongodb/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/mongodb/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/mongodb/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/mongodb/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n</code></pre> Import Essential Libraries <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter.SimpleLoadAndSplit import SimpleLoadAndSplit\nfrom indox.vector_stores import MongoDB\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre> Setting Up Reference Directory and File Path <pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\nfile_path = \"sample.txt\"\nsimpleLoadAndSplit = SimpleLoadAndSplit(file_path=file_path, remove_sword=False, max_chunk_size=200)\ndocs = simpleLoadAndSplit.load_and_chunk()\n</code></pre> Initialize MongoDB <pre><code>db = MongoDB(\n    collection_name=\"Indox_collection\",\n    embedding_function=embed,\n    connection_string=\"mongodb://localhost:27017/\",\n    database_name=\"vector_db\"\n)\n</code></pre> Connecting VectorStore to Indox <pre><code>db.add(docs=docs)\n</code></pre> Querying the Database <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/neo4j_graph/","title":"Neo4jGraph","text":"<p>The Neo4jGraph class allows you to use Neo4j as a graph database to store and query graph-structured data. This can be used in a Retrieval-Augmented Generation (RAG) system or other knowledge graph-related applications.</p>"},{"location":"Indox/vectorstores/neo4j_graph/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>uri (str): The URI of the Neo4j instance (e.g., \"bolt://localhost:7687\").</li> <li>username (str): The username for authenticating to the Neo4j database.</li> <li>password (str): The password for authenticating to the Neo4j database.</li> </ul>"},{"location":"Indox/vectorstores/neo4j_graph/#installation","title":"Installation","text":"<p>To use Neo4j with this package, you first need to install the <code>Neo4j</code> driver. You can install it using pip:</p> <p><pre><code>pip install neo4j\n</code></pre> You will also need a running Neo4j instance. Refer to the official Neo4j installation guide for instructions on setting up a Neo4j instance locally or in the cloud.</p>"},{"location":"Indox/vectorstores/neo4j_graph/#example_usage","title":"Example Usage","text":""},{"location":"Indox/vectorstores/neo4j_graph/#initialize_the_neo4jgraph","title":"Initialize the Neo4jGraph","text":"<p>You can initialize the Neo4jGraph class by passing the Neo4j connection details.</p> <pre><code>from indox.vector_stores import Neo4jGraph\n\n# Initialize the Neo4jGraph with connection details\nneo4j_graph = Neo4jGraph(uri=\"bolt://localhost:7687\", username=\"neo4j\", password=\"your_password\")\n</code></pre>"},{"location":"Indox/vectorstores/neo4j_graph/#add_graph_documents_to_neo4j","title":"Add Graph Documents to Neo4j","text":"<p>To add graph documents (which contain nodes and relationships) to the Neo4j database, you can use the <code>add_graph_documents</code> method. This method allows you to store a list of graph documents with options to include source metadata and entity labels.</p> <pre><code># Assuming `graph_documents` is a list of GraphDocument objects you want to store\nneo4j_graph.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n</code></pre> <ul> <li>graph_documents: A list of GraphDocument objects to store in the Neo4j database.</li> <li>base_entity_label (bool, optional): If True, adds a base \"Entity\" label to all nodes. Defaults to True.</li> <li>include_source (bool, optional): If True, includes the source document as part of the graph. Defaults to True.</li> </ul> <pre><code># Query Relationships by Entity\nTo query relationships in Neo4j, you can use the `search_relationships_by_entity` method. This method allows you to search for relationships by specifying the entity ID and the relationship type.\n\n# Search for parent relationships for the entity with ID \"Elizabeth_I\"\nrelationships = neo4j_graph.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Print the found relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n</code></pre> <ul> <li>entity_id (str): The ID of the entity for which you want to find relationships.</li> <li>relationship_type (str): The type of relationship to search for (e.g., \"PARENT\").</li> </ul>"},{"location":"Indox/vectorstores/neo4j_graph/#close_the_connection","title":"Close the Connection","text":"<p>After performing your operations, always remember to close the Neo4j connection:</p> <pre><code>neo4j_graph.close()\n</code></pre>"},{"location":"Indox/vectorstores/neo4j_graph/#example_workflow","title":"Example Workflow","text":"<p>Here's an example of how to use <code>Neo4jGraph</code> to store and query relationships in Neo4j:</p> <pre><code>from neo4j_graph import Neo4jGraph\n\n# Initialize Neo4jGraph\nneo4j_graph = Neo4jGraph(uri=\"bolt://localhost:7687\", username=\"neo4j\", password=\"your_password\")\n\n# Assuming you have a list of GraphDocument objects ready to store\ngraph_documents = [...]  # List of GraphDocument objects\n\n# Add the graph documents to Neo4j\nneo4j_graph.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n\n# Query relationships of an entity\nrelationships = neo4j_graph.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Output the relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n\n# Close the Neo4j connection\nneo4j_graph.close()\n</code></pre>"},{"location":"Indox/vectorstores/neo4j_graph/#faq","title":"FAQ","text":"<p>Q : What do I need to use <code>Neo4jGraph</code>?</p> <ul> <li>You need a running Neo4j database and the Neo4j Python driver installed.</li> </ul> <p>Q: Can I use this with an existing Neo4j database?</p> <ul> <li>Yes, you can connect to any Neo4j instance by providing the correct URI, username, and password.</li> </ul>"},{"location":"Indox/vectorstores/pathway/","title":"Pathway Vector Store","text":"<p>To use Pathway Vector Store, you can create a client instance by specifying the <code>host</code>, <code>port</code>, or <code>url</code>.</p>"},{"location":"Indox/vectorstores/pathway/#hyperparameters","title":"Hyperparameters","text":"<ul> <li><code>host</code> (Optional[str]): The host on which Pathway Vector Store listens.</li> <li><code>port</code> (Optional[int]): The port on which Pathway Vector Store listens.</li> <li><code>url</code> (Optional[str]): The URL at which Pathway Vector Store listens.</li> </ul>"},{"location":"Indox/vectorstores/pathway/#installation","title":"Installation","text":"<p>You can connect to the Pathway Vector Store by creating an instance of the <code>PathwayVectorClient</code>:</p> <pre><code>from indox.vector_stores import PathwayVectorClient\nclient = PathwayVectorClient(host=\"localhost\", port=8080)\n</code></pre> <p>Alternatively, you can connect using a full URL:</p> <pre><code>client = PathwayVectorClient(url=\"http://localhost:8080\")\n</code></pre>"},{"location":"Indox/vectorstores/pathway/#usage","title":"Usage","text":"<p>Once the client is initialized, you can use it to query the Pathway Vector Store.</p> <p>Store documents in the vector store:</p> <pre><code>client.add(docs=docs)\n</code></pre> <p>Query the vector store:</p> <pre><code>query = \"How can Cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=client, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/","title":"PineconeVectorStore","text":"<p>Pinecone provides long-term memory for high-performance AI applications. It\u2019s a managed, cloud-native vector database.</p> <p>Note: If you have not created the index before, you can create it through Pinecone and then give the name of the created index as a parameter to the input of PineconeVectoStore. Note that the created index must have dimensions that match the dimensions of the desired embedding model.</p> <p>To use Pinecone as the vector store, users need to install Pinecone and set the API key, Index_name and Embedding Model.</p> <pre><code>from indox.vector_stores import PineconeVectorStore\ndb = PineconeVectorStore(embedding=embed,\n                         pinecone_api_key=PINECONE_API_KEY,\n                         index_name=Index_name)\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>embedding [Embeddings]: The embedding to be used.</li> <li>pinecone_api_key [str]: The API key of Pinecone.</li> <li>index_name [str]: The name of the index in the database.</li> </ul>"},{"location":"Indox/vectorstores/pinecone/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/pinecone/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/pinecone/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>indox_judge\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/pinecone/#macoslinux","title":"macOS/Linux","text":"<ol> <li> <p>Create the virtual environment: <pre><code>   python3 -m venv indox\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>   source indox/bin/activate\n</code></pre></p> </li> </ol>"},{"location":"Indox/vectorstores/pinecone/#get_started","title":"Get Started","text":""},{"location":"Indox/vectorstores/pinecone/#import_essential_libraries","title":"Import Essential Libraries","text":"<pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter.SimpleLoadAndSplit import SimpleLoadAndSplit\nfrom indox.vector_stores import PineconeVectorStore\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/#setting_up_reference_directory_and_file_path","title":"Setting Up Reference Directory and File Path","text":"<pre><code>!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\nfile_path = \"sample.txt\"\n\nsimpleLoadAndSplit = SimpleLoadAndSplit(file_path=\"sample.txt\",remove_sword=False,max_chunk_size=200)\ndocs = simpleLoadAndSplit.load_and_chunk()\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/#initialize_pinecone","title":"Initialize Pinecone","text":"<pre><code>from pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nPINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n\npc = Pinecone(api_key=PINECONE_API_KEY)\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/#create_index_and_pineconevectorstore","title":"Create Index and PineconeVectorStore","text":"<pre><code>index_name = \"your-index-name\"\n\nif index_name not in pc.list_indexes().names():\n    pc.create_index(\n        name=index_name,\n        dimension=768,      # change the dimension to the desired value\n        metric=\"cosine\",\n        spec=ServerlessSpec(\n            cloud='aws', \n            region='us-east-1'\n        ) \n    ) \nindex = pc.Index(index_name)\ndb = PineconeVectorStore(embedding=embed,pinecone_api_key=PINECONE_API_KEY,index_name=index_name)\n</code></pre>"},{"location":"Indox/vectorstores/pinecone/#usage_1","title":"Usage","text":"<p>Store documents in the vector store:</p> <pre><code>db.add(docs=docs)\n</code></pre> <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/postgres/","title":"Postgres Using PgVector","text":"<p>First you should install pgvector and set the database address. <pre><code>pip install pgvector\npip install psycopg2\n</code></pre> To use pgvector as the vector store, users need to install pgvector and set the database address.</p>"},{"location":"Indox/vectorstores/postgres/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>host (str): The host of the PostgreSQL database.</li> <li>port (int): The port of the PostgreSQL database.</li> <li>dbname (str): The name of the database.</li> <li>user (str): The user for the PostgreSQL database.</li> <li>password (str): The password for the PostgreSQL database.</li> <li>collection_name (str): The name of the collection in the database.</li> <li>embedding (Embedding): The embedding to be used.</li> </ul>"},{"location":"Indox/vectorstores/postgres/#installation","title":"Installation","text":"<p>For instructions on installing pgvector, refer to the pgvector installation guide.</p> <pre><code>from indox.vector_stores import PGVectorStore\ndb = PGVectorStore(host=\"host\",port=port,dbname=\"dbname\",user=\"username\",password=\"password\",collection_name=\"sample\",embedding=embed)\n</code></pre>"},{"location":"Indox/vectorstores/postgres/#usage","title":"Usage","text":"<p>Store documents in the vector store:</p> <pre><code>db.add(docs=docs)\n</code></pre> <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db,llm=mistral_qa,top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/qdrant/","title":"Qdrant","text":"<p>Qdrant is a high-performance vector database designed for large-scale similarity search. This implementation uses Qdrant as a vector store, supporting cosine similarity for vector searches when an embedding function is provided.</p> <p>Note: To use Qdrant as the vector store, users need to install <code>qdrant-client</code> and have access to a running Qdrant service. The Qdrant service URL, API key, and collection name must be provided when initializing the Qdrant vector store.</p> <p>To use Qdrant as the vector store:</p> <pre><code>from indox.vector_stores import Qdrant\n\nqdrant_db = Qdrant(\n    collection_name=\"your_collection\",\n    embedding_function=HuggingFaceEmbedding(),\n    url=\"https://qdrant-api-url\",\n    api_key=\"your_api_key\"\n)\n</code></pre>"},{"location":"Indox/vectorstores/qdrant/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>collection_name [str]: Name of the Qdrant collection to use.</li> <li>embedding_function [Optional[Embeddings]]: Function to generate embeddings for documents.</li> <li>url [str]: URL of the Qdrant service (e.g., \"https://qdrant-api-url\").</li> <li>api_key [str]: API key for Qdrant cloud service.</li> <li>texts [List[str]]: Texts to be added to the collection.</li> <li>metadatas [Optional[List[dict]]]: Metadata for each text (default: None).</li> <li>ids [Optional[List[str]]]: IDs for each text (default: autogenerated).</li> <li>k [int]: Number of results to return for similarity search (default: 4).</li> <li>filter [Optional[Dict[str, Any]]]: Filters to apply for search (default: None).</li> </ul>"},{"location":"Indox/vectorstores/qdrant/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/qdrant/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/qdrant/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/qdrant/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/qdrant/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key And Qdrant API KEY <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\nQDRANT_API_KEY = os.environ['Qdrant_API_KEY']\n</code></pre> Import Essential Libraries <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre> Setting Up Reference Directory and File Path <pre><code>from indox.splitter import RecursiveCharacterTextSplitter\n\nfile_path = \"sample.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\n\nsplitter = RecursiveCharacterTextSplitter(400,20)\ncontent_chunks = splitter.split_text(text)\n</code></pre> Initialize Qdrant <pre><code>from indox.vector_stores import Qdrant\n\nurl = \"url\" \nqdrant = Qdrant(\n    collection_name=\"IndoxTest\", \n    embedding_function=embed, \n    url=url, \n    api_key=QDRANT_API_KEY\n\n)\n</code></pre> Add Texts to VectorStore <pre><code>qdrant.add(texts=content_chunks)\n</code></pre> Querying the Database <pre><code>retriever = indox.QuestionAnswer(vector_database=qdrant,llm=mistral_qa,top_k=5)\nquery = \"How cinderella reach her happy ending?\"\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/redis/","title":"RedisDB","text":"<p>RedisDB is a concrete implementation of a vector store using Redis for storage. This implementation supports storing text documents and their embeddings, performing similarity searches, and deleting documents. It uses an embedding function to generate document embeddings and stores them in Redis, supporting cosine similarity searches.</p> <p>Note: To use RedisDB as a vector store. The Redis connection details (host, port, and password) must be provided when initializing RedisDB.</p> <p>To use RedisDB as the vector store:</p> <pre><code>from indox.vector_stores import RedisDB\nfrom indox.embeddings import HuggingFaceEmbedding\n\nredis_db = RedisDB(\n    host=\"host\",\n    port=6379,\n    password=\"your_password\",\n    embedding=HuggingFaceEmbedding()\n)\n</code></pre>"},{"location":"Indox/vectorstores/redis/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>host [str]: The Redis server's host (e.g., \"localhost\").</li> <li>port [int]: The port of the Redis server (default: 6379).</li> <li>password [str]: The password for the Redis server.</li> <li>embedding [Optional[Embeddings]]: Function to generate embeddings for documents.</li> <li>prefix [str]: Prefix for Redis keys (default: \"doc\").</li> <li>texts [List[str]]: Texts to be added to the Redis collection.</li> <li>metadatas [Optional[List[dict]]]: Metadata for each text (default: None).</li> <li>k [int]: Number of results to return for similarity search (default: 5).</li> <li>ids [Optional[List[str]]]: IDs for each text (default: autogenerated).</li> </ul>"},{"location":"Indox/vectorstores/redis/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/redis/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/redis/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/redis/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/redis/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.environ['HUGGINGFACE_API_KEY']\n</code></pre> Import Essential Libraries <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre> Setting Up Reference Directory and File Path <pre><code>from indox.splitter import RecursiveCharacterTextSplitter\n\nfile_path = \"sample.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\n\nsplitter = RecursiveCharacterTextSplitter(400,20)\ncontent_chunks = splitter.split_text(text)\n</code></pre> Initialize Redis <pre><code>from indox.vector_stores import RedisDB\nredis_store = RedisDB(\n    host='host',\n    port=port,\n    password='password',\n    embedding=embed\n)\n)\n</code></pre> Add Texts to VectorStore <pre><code>redis_store.add(texts=content_chunks)\n</code></pre> Querying the Database <pre><code>retriever = indox.QuestionAnswer(vector_database=redis_store,llm=mistral_qa,top_k=5)\nquery = \"How cinderella reach her happy ending?\"\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/singlestore/","title":"SingleStoreVectorDB","text":"<p><code>SingleStoreVectorDB</code> is a document storage solution using SingleStore for storing text documents and their vector embeddings. It supports adding documents, performing similarity searches with cosine similarity, and managing vector indexes to optimize search performance.</p> <p>Note: To use <code>SingleStoreVectorDB</code>, users need to have a running SingleStore instance and provide the necessary connection parameters when initializing the vector store.</p> <p>To use <code>SingleStoreVectorDB</code> as the vector store:</p> <pre><code>from indox.vector_stores import SingleStoreVectorDB\nfrom indox.embeddings import HuggingFaceEmbedding\n\ndb = SingleStoreVectorDB(\n    connection_params={\n        'user': 'your_user',\n        'password': 'your_password',\n        'host': 'your_host',\n        'database': 'your_database'\n    },\n    embedding_function=HuggingFaceEmbedding(),\n    table_name=\"your_table\"\n)\n</code></pre>"},{"location":"Indox/vectorstores/singlestore/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>connection_params [dict]: Connection parameters for SingleStore (e.g., user, password, host, database).</li> <li>table_name [str]: Name of the SingleStore table to store embeddings (default: \"embeddings\").</li> <li>embedding_function [Optional[Embeddings]]: Function to generate embeddings for documents.</li> <li>vector_dimension [int]: Dimensionality of vector embeddings (default: 768).</li> <li>use_vector_index [bool]: Whether to use vector indexing for fast similarity searches (default: True).</li> <li>use_full_text_search [bool]: Whether to enable full-text search along with vector search (default: False).</li> <li>vector_index_options [Optional[dict]]: Additional options for vector indexing (default: None).</li> </ul>"},{"location":"Indox/vectorstores/singlestore/#usage","title":"Usage","text":""},{"location":"Indox/vectorstores/singlestore/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"Indox/vectorstores/singlestore/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre> 2Activate the virtual environment: <pre><code>indox\\Scripts\\activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/singlestore/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment: <pre><code>python -m venv indox\n</code></pre></li> <li>Activate the virtual environment: <pre><code>source indox/bin/activate\n</code></pre></li> </ol>"},{"location":"Indox/vectorstores/singlestore/#get_started","title":"Get Started","text":"<p>Import HuggingFace API Key <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nHUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n</code></pre> Import Essential Libraries <pre><code>from indox import IndoxRetrievalAugmentation\nfrom indox.llms import HuggingFaceModel\nfrom indox.embeddings import HuggingFaceEmbedding\nfrom indox.data_loader_splitter.SimpleLoadAndSplit import SimpleLoadAndSplit\nfrom indox.vector_stores import MongoDB\n\nindox = IndoxRetrievalAugmentation()\nmistral_qa = HuggingFaceModel(api_key=HUGGINGFACE_API_KEY,model=\"mistralai/Mistral-7B-Instruct-v0.2\")\nembed = HuggingFaceEmbedding(model=\"multi-qa-mpnet-base-cos-v1\",api_key=HUGGINGFACE_API_KEY)\n</code></pre> Setting Up Reference Directory and File Path <pre><code>from indox.splitter import SemanticTextSplitter\n\n!wget https://raw.githubusercontent.com/osllmai/inDox/master/Demo/sample.txt\nfile_path = \"sample.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\nsplitter = SemanticTextSplitter(400)\ncontent_chunks = splitter.split_text(text)\n</code></pre> Initialize SingleStoreDB <pre><code>from indox.vector_stores import SingleStoreVectorDB\n\nconnection_params = {\n    \"host\": \"host\",\n    \"port\": port,\n    \"user\": \"user\",\n    \"password\": \"password\",\n    \"database\": \"databasename\"\n}\n\ndb = SingleStoreVectorDB(connection_params=connection_params,embedding_function=embed)\n</code></pre> Connecting VectorStore to Indox <pre><code>db.add_texts(content_chunks)\n</code></pre> Querying the Database <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre></p>"},{"location":"Indox/vectorstores/vearch/","title":"Vearch vector store","text":"<p>The <code>Vearch</code> class is designed to work with an external embedding function for creating a vector-based search database using the Vearch engine. It allows users to store documents, generate embeddings, and perform similarity searches.</p>"},{"location":"Indox/vectorstores/vearch/#initialization","title":"Initialization","text":"<pre><code>Vearch(embedding_function: Any, db_name: str, space_name: str = \"text_embedding_space\")\n</code></pre> <p>Initializes a Vearch instance with the following parameters:</p> <ul> <li>embedding_function: A function or model that converts text into embeddings. This function should have an <code>embed_query</code> method that returns a vector for a given query.</li> <li>db_name: The name of the database where the documents will be stored.</li> <li>space_name: The name of the vector space (default: <code>\"text_embedding_space\"</code>). This defines the structure of the data space in the database.</li> </ul> <p>Upon initialization, the class automatically defines the dimensions of the embedding space by testing the embedding function with a sample query. It also creates a schema for the space.</p>"},{"location":"Indox/vectorstores/vearch/#attributes","title":"Attributes","text":"<ul> <li>embedding_function: The external model or function used to create text embeddings.</li> <li>db_name: Name of the database.</li> <li>space_name: Name of the space.</li> <li>data: List of documents with their embeddings, initialized as an empty list.</li> <li>dim: The dimensionality of the embeddings, which is determined by the embedding function.</li> <li>space_schema: The schema for the space, including the fields for storing the text and embeddings.</li> <li>config: Configuration object to connect to the Vearch server, including host and token.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#methods","title":"Methods","text":""},{"location":"Indox/vectorstores/vearch/#create_space_schema_-_spaceschema","title":"<code>create_space_schema() -&gt; SpaceSchema</code>","text":"<p>Creates a schema for the space with two fields: a document text field and an embedding field.</p> <ul> <li>Returns: A <code>SpaceSchema</code> object that defines the structure of the space.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#create_spacespace_name_str","title":"<code>create_space(space_name: str)</code>","text":"<p>Creates a new space in the database using the defined schema.</p> <ul> <li>space_name: The name of the space to be created.</li> <li>Returns: The result of the space creation operation.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#create_databasedb_name_str","title":"<code>create_database(db_name: str)</code>","text":"<p>Creates a new database.</p> <ul> <li>db_name: The name of the database to be created.</li> <li>Returns: The result of the database creation operation.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#adddocs_listdocument","title":"<code>add(docs: List[Document])</code>","text":"<p>Adds a list of documents into the Vearch database.</p> <ul> <li>docs: A list of <code>Document</code> objects. Each <code>Document</code> contains text and optionally metadata. The method computes the embeddings for each document using the <code>embedding_function</code> and appends them to the data storage.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#similarity_search_with_scorequery_str_k_int__5_-_listtupledocument_float","title":"<code>similarity_search_with_score(query: str, k: int = 5) -&gt; List[Tuple[Document, float]]</code>","text":"<p>Performs a similarity search using a query string. It compares the embeddings of the query with the stored documents and returns the top <code>k</code> most similar documents along with their similarity scores.</p> <ul> <li>query: The text query to search for.</li> <li>k: The number of top results to return (default: 5).</li> <li>Returns: A list of tuples, where each tuple contains a <code>Document</code> object and a similarity score. The results are sorted in descending order of similarity.</li> </ul>"},{"location":"Indox/vectorstores/vearch/#example_usage","title":"Example Usage","text":"<pre><code>openai_qa_indox = IndoxApi(api_key=INDOX_API_KEY)\nembed_openai_indox = IndoxApiEmbedding(api_key=INDOX_API_KEY, model=\"text-embedding-3-small\")\n\n# Create a Vearch instance with the embedding function and a database name\ndb = Vearch(embedding_function=embed_openai_indox, db_name=\"sadra\")\n\n# Add documents to the database\ndb.add(docs=docs)\n\n# Perform a similarity search\nresults = db.similarity_search_with_score(query=\"example search query\", k=5)\n\n# Output the top results\nfor doc, score in results:\n    print(f\"Document: {doc.page_content}, Similarity: {score}\")\n</code></pre>"},{"location":"Indox/vectorstores/vearch/#dependencies","title":"Dependencies","text":"<ul> <li><code>vearch.config.Config</code>: Used to set up configuration for the Vearch client.</li> <li><code>vearch.schema.field.Field</code>: Defines fields for the schema.</li> <li><code>vearch.schema.space.SpaceSchema</code>: Defines the schema for the space in the Vearch database.</li> <li><code>vearch.utils.DataType</code>, <code>vearch.utils.MetricType</code>: Utilities for defining the data types and metrics used for embedding similarity.</li> <li><code>vearch.schema.index.FlatIndex</code>, <code>vearch.schema.index.ScalarIndex</code>: Indexing methods for fields.</li> <li><code>sklearn.metrics.pairwise.cosine_similarity</code>: Used for computing the similarity between query embeddings and stored document embeddings.</li> </ul>"},{"location":"Indox/vectorstores/weaviate/","title":"Weaviate","text":"<p>To use Weaviate as the vector store, users need to install the <code>weaviate-client</code> and set the <code>index_name</code>, <code>text_key</code>, and embedding model.</p> <pre><code>pip install weaviate-client\n</code></pre>"},{"location":"Indox/vectorstores/weaviate/#hyperparameters","title":"Hyperparameters","text":"<ul> <li><code>client</code> (Any): The Weaviate client instance.</li> <li><code>index_name</code> (str): The name of the index in the Weaviate database.</li> <li><code>text_key</code> (str): The key of the text field in the objects.</li> <li><code>embedding</code> (Optional[Embeddings]): The embedding model to be used.</li> <li><code>attributes</code> (Optional[List[str]]): Specific attributes to be retrieved.</li> <li><code>relevance_score_fn</code> (Optional[Callable[[float], float]]): Function to normalize the relevance scores.</li> <li><code>by_text</code> (bool): If True, embeddings are calculated by the text content.</li> </ul>"},{"location":"Indox/vectorstores/weaviate/#installation","title":"Installation","text":"<p>For instructions on installing Weaviate, refer to the Weaviate installation guide.</p> <pre><code>from indox.vector_stores import WeaviateVectorStore\ndb = WeaviateVectorStore(client=weaviate_client, index_name=\"index_name\", text_key=\"text_key\", embedding=embed)\n</code></pre>"},{"location":"Indox/vectorstores/weaviate/#usage","title":"Usage","text":"<p>Store documents in the vector store:</p> <pre><code>db.add(docs=docs)\n</code></pre> <p>Query the vector store:</p> <pre><code>query = \"How cinderella reach her happy ending?\"\nretriever = indox.QuestionAnswer(vector_database=db, llm=mistral_qa, top_k=5, document_relevancy_filter=True)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"Indox/vectorstores/weaviate/#running_weaviate_in_docker","title":"Running Weaviate in Docker","text":"<p>To run Weaviate in Docker, use the following commands:</p> <p>Pull the latest Weaviate Docker image:</p> <pre><code>docker pull semitechnologies/weaviate:latest\n</code></pre> <p>Run Weaviate in a container:</p> <pre><code>docker run -d -p 8080:8080 semitechnologies/weaviate:latest\n</code></pre> <p>If you encounter permission errors, you can run Weaviate with the following command:</p> <pre><code>docker run -d -p 8080:8080 -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true -e PERSISTENCE_DATA_PATH=/var/lib/weaviate -v /path/on/host:/var/lib/weaviate semitechnologies/weaviate:latest\n</code></pre>"},{"location":"IndoxGen/","title":"Home","text":""},{"location":"IndoxGen/#overview","title":"Overview","text":"<p>IndoxGen is a powerful generative data synthesis framework designed for creating high-quality synthetic data. It leverages advanced machine learning techniques, including Generative Adversarial Networks (GANs) and Large Language Models (LLMs), to generate structured and unstructured data. IndoxGen enables researchers, developers, and data scientists to create diverse datasets for training, testing, and analysis without compromising privacy or data quality.</p> <p>Whether you're working with numerical, textual, or mixed data, IndoxGen provides the tools and flexibility to generate data tailored to your needs.</p>"},{"location":"IndoxGen/#key_features","title":"Key Features","text":""},{"location":"IndoxGen/#1_advanced_generative_techniques","title":"1. Advanced Generative Techniques","text":"<p>IndoxGen combines state-of-the-art methods for data synthesis:</p> <ul> <li> <p>GAN-based Numerical Data Synthesis: Generate structured numerical data while preserving statistical properties.</p> </li> <li> <p>LLM-based Text Data Synthesis: Create realistic text data using prompt-based generation and few-shot learning.</p> </li> <li> <p>Hybrid GAN-LLM Pipelines: Combine numerical and textual data synthesis for cohesive datasets.</p> </li> </ul>"},{"location":"IndoxGen/#2_diverse_synthesis_modes","title":"2. Diverse Synthesis Modes","text":"<p>IndoxGen supports multiple modes of data synthesis:</p> <ul> <li> <p>Prompt-Based Synthesis: Use natural language instructions to guide data generation.</p> </li> <li> <p>Few-Shot Learning: Generate new data from minimal examples.</p> </li> <li> <p>Interactive Feedback: Refine generated data through human feedback loops.</p> </li> </ul>"},{"location":"IndoxGen/#3_privacy-preserving_data_generation","title":"3. Privacy-Preserving Data Generation","text":"<p>Generate synthetic data that maintains the statistical properties of real-world data without exposing sensitive information. This is ideal for applications in healthcare, finance, and other privacy-sensitive domains.</p>"},{"location":"IndoxGen/#4_modular_and_customizable","title":"4. Modular and Customizable","text":"<p>IndoxGen is highly modular, allowing users to:</p> <ul> <li> <p>Define custom data synthesis workflows.</p> </li> <li> <p>Use pre-built modules for common tasks.</p> </li> <li> <p>Extend functionality with their own generators or evaluators.</p> </li> </ul>"},{"location":"IndoxGen/#use_cases","title":"Use Cases","text":""},{"location":"IndoxGen/#1_training_machine_learning_models","title":"1. Training Machine Learning Models","text":"<p>Generate large, diverse datasets to train robust machine learning models, especially in scenarios where real data is limited or unavailable.</p>"},{"location":"IndoxGen/#2_data_augmentation","title":"2. Data Augmentation","text":"<p>Expand existing datasets with high-quality synthetic data to improve model performance.</p>"},{"location":"IndoxGen/#3_privacy-conscious_data_sharing","title":"3. Privacy-Conscious Data Sharing","text":"<p>Share synthetic datasets with collaborators without exposing sensitive information.</p>"},{"location":"IndoxGen/#4_testing_and_development","title":"4. Testing and Development","text":"<p>Simulate edge cases and rare scenarios by generating tailored synthetic data.</p>"},{"location":"IndoxGen/#getting_started","title":"Getting Started","text":""},{"location":"IndoxGen/#1_install_indoxgen","title":"1. Install IndoxGen","text":"<pre><code>pip install indoxgen\n</code></pre>"},{"location":"IndoxGen/#2_explore_synthesis_options","title":"2. Explore Synthesis Options","text":"<p>Learn about the available data synthesis techniques and choose one that fits your needs:</p> <ul> <li> <p>Hybrid GAN and LLM Pipelines \u2192</p> </li> <li> <p>Prompt-Based Synthesis \u2192</p> </li> <li> <p>Interactive Feedback Synthesis \u2192</p> </li> </ul>"},{"location":"IndoxGen/#3_build_your_first_synthetic_dataset","title":"3. Build Your First Synthetic Dataset","text":"<p>Follow the Quick Start Guide \u2192 to set up and generate your first synthetic dataset.</p>"},{"location":"IndoxGen/#feedback_and_contributions","title":"Feedback and Contributions","text":"<p>We value your feedback and ideas! If you encounter any issues, have suggestions, or wish to contribute to the development of IndoxGen, please reach out or submit a pull request on our GitHub repository.</p>"},{"location":"IndoxGen/AttributePromptSynth/","title":"AttributePromptSynth","text":""},{"location":"IndoxGen/AttributePromptSynth/#overview","title":"Overview","text":"<p><code>AttributePromptSynth</code> is a Python class designed to generate synthetic data based on a set of attributes and user instructions. It utilizes language models (LLMs) to generate prompts and retrieve responses that can be saved as a DataFrame or exported to an Excel file.</p>"},{"location":"IndoxGen/AttributePromptSynth/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"IndoxGen/AttributePromptSynth/#installation","title":"Installation","text":"<p>To use the <code>AttributePromptSynth</code> class, you need to have Python 3.9+ installed. You can install the <code>indoxGen</code> package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"IndoxGen/AttributePromptSynth/#language_model_setup","title":"Language Model Setup","text":"<p><code>AttributePromptSynth</code> requires an LLM (Language Model) for generating responses from provided prompts. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up the language model for this class:</p> <pre><code>from indoxGen.llms import IndoxApi\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nINDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre> <p>The <code>indoxGen</code> library supports various models, including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides routing for OpenAI, enabling easy switching between different models.</p>"},{"location":"IndoxGen/AttributePromptSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use the <code>AttributePromptSynth</code> class:</p> <pre><code>from indoxGen.synthCore import AttributePromptSynth\n\n# Define the arguments for generating prompts\nargs = {\n    \"instruction\": \"Generate a {adjective} sentence that is {length}.\",\n    \"attributes\": {\n        \"adjective\": [\"serious\", \"funny\"],\n        \"length\": [\"short\", \"long\"]\n    },\n    \"llm\": LLM\n}\n\n# Create an instance of AttributePromptSynth\ndataset = AttributePromptSynth(prompt_name=\"ExamplePrompt\",\n                                   args=args,\n                                   outputs={})\n\n# Run the prompt generation\ndf = dataset.run()\n\n# Display the generated DataFrame\nprint(df)\n</code></pre>"},{"location":"IndoxGen/AttributePromptSynth/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/AttributePromptSynth/#attributepromptsynth_1","title":"<code>AttributePromptSynth</code>","text":"<p><pre><code>def __init__(self, prompt_name: str, args: dict, outputs: dict):\n</code></pre> Initializes the <code>AttributePromptSynth</code> class.</p> <p>Generates synthetic data based on the attribute setup and returns it as a pandas DataFrame.</p> <p>Returns: </p> <ul> <li>A <code>pandas.DataFrame</code> containing the generated data.</li> </ul> <p><pre><code>def save_to_excel(self, file_path: str, df: pd.DataFrame) -&gt; None:\n</code></pre> Saves the generated DataFrame to an Excel file.</p> <ul> <li><code>file_path</code> (str): The path where the Excel file will be saved.</li> <li><code>df</code> (pd.DataFrame): The DataFrame to be saved.</li> <li>Raises: <code>ValueError</code> if the DataFrame is empty or cannot be saved.</li> </ul>"},{"location":"IndoxGen/AttributePromptSynth/#examples","title":"Examples","text":""},{"location":"IndoxGen/AttributePromptSynth/#generating_data_based_on_attributes","title":"Generating Data Based on Attributes","text":"<pre><code>from indoxGen.synthCore import DataFromPrompt\nfrom indoxGen.utils import Excel\n\ndataset_file_path = \"output_dataFromPrompt.xlsx\"\n\nexcel_loader = Excel(dataset_file_path) \ndf = excel_loader.load()  \nuser_prompt = \" based on given dataset generate one unique row about soccer\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\nadded_row = DataFromPrompt(llm=LLM, user_instruction=user_prompt, example_data=df, verbose=1).generate_data()\nprint(added_row)\n</code></pre>"},{"location":"IndoxGen/AttributePromptSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>AttributePromptSynth</code> are welcome. To contribute, please follow these steps:</p> <ol> <li> <p>Fork the repository.</p> </li> <li> <p>Create a new branch for your feature.</p> </li> <li> <p>Add your changes and write tests if applicable.</p> </li> <li> <p>Submit a pull request with a clear description of your changes.</p> </li> </ol>"},{"location":"IndoxGen/FewShotPromptSynth/","title":"FewShotPromptSynth","text":""},{"location":"IndoxGen/FewShotPromptSynth/#overview","title":"Overview","text":"<p>FewShotPromptSynth is a Python class designed for generating synthetic data based on few-shot learning examples and user-provided instructions. It utilizes language models to generate diverse datasets, leveraging pre-existing examples for guidance. The class supports outputting the generated data as a pandas DataFrame and allows saving the results to an Excel file.</p>"},{"location":"IndoxGen/FewShotPromptSynth/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Contributing</li> </ul>"},{"location":"IndoxGen/FewShotPromptSynth/#installation","title":"Installation","text":"<p>To use FewShotPromptSynth, you need to have Python 3.9+ installed. You can install the required package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"IndoxGen/FewShotPromptSynth/#language_model_setup","title":"Language Model Setup","text":"<p>FewShotPromptSynth requires a language model for generating synthetic data. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up a language model for use in the class:</p> <pre><code>from indoxGen.llms import IndoxApi\n\n# Setup for IndoxApi model\nllm = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre> <p>The indoxGen library supports various models, including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, indoxGen provides a router for OpenAI, allowing for easy switching between different models.</p>"},{"location":"IndoxGen/FewShotPromptSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use FewShotPromptSynth:</p> <pre><code>from indoxGen.synthCore import FewShotPromptSynth\n# Define your Language Model (LLM) instance (replace with the actual LLM you're using)\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\n# Define a user prompt for the generation task\nuser_prompt = \"Describe the formation of stars in simple terms. Return the result in JSON format, with the key 'description'.\"\n\n# Define few-shot examples (input-output pairs) to help guide the LLM\nexamples = [\n    {\n        \"input\": \"Describe the process of photosynthesis.\",\n        \"output\": \"Photosynthesis is the process by which green plants use sunlight to synthesize food from carbon dioxide and water.\"\n    },\n    {\n        \"input\": \"Explain the water cycle.\",\n        \"output\": \"The water cycle is the process by which water circulates between the earth's oceans, atmosphere, and land, involving precipitation, evaporation, and condensation.\"\n    }\n]\n\n# Create an instance of FewShotPromptSynth using the defined LLM, user prompt, and few-shot examples\ndata_generator = FewShotPromptSynth(\n    llm=LLM,                            # Language model instance (LLM)\n    user_instruction=user_prompt,        # Main user instruction or query\n    examples=examples,                   # Few-shot input-output examples\n    verbose=1,                           # Verbosity level (optional)\n    max_tokens=8000                      # Max tokens for generation (optional)\n)\n\n# Generate the data based on the few-shot setup\ndf = data_generator.generate_data()\n</code></pre>"},{"location":"IndoxGen/FewShotPromptSynth/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/FewShotPromptSynth/#fewshotpromptsynth_1","title":"FewShotPromptSynth","text":"<p><pre><code>def __init__(self, prompt_name: str, args: dict, outputs: dict, examples: List[Dict[str, str]]):\n</code></pre> Initializes the FewShotPromptSynth class.</p> <p><pre><code>def save_to_excel(self, file_path: str, df: pd.DataFrame) -&gt; None:\n</code></pre> Saves the generated DataFrame to an Excel file.</p> <ul> <li><code>file_path</code> (str): The path where the Excel file will be saved.</li> <li><code>df</code> (pd.DataFrame): The DataFrame to be saved.</li> <li>Raises: <code>ValueError</code> if the DataFrame is empty or cannot be saved.</li> </ul>"},{"location":"IndoxGen/FewShotPromptSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve FewShotPromptSynth are welcome. Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol>"},{"location":"IndoxGen/GAN-Torch-Tensor/","title":"IndoxGen-Torch &amp; IndoxGen-Tensor: GAN-Based Synthetic Data Generation Framework","text":""},{"location":"IndoxGen/GAN-Torch-Tensor/#overview","title":"Overview","text":"<p>IndoxGen-Torch and IndoxGen-Tensor are two advanced frameworks designed for generating synthetic tabular data using Generative Adversarial Networks (GANs). IndoxGen-Torch is based on PyTorch, while IndoxGen-Tensor is based on TensorFlow, providing flexibility depending on user preference. Both frameworks support various data types, including categorical, continuous, and integer data.</p> <p>These tools extend the capabilities of the IndoxGen project by offering easy-to-use configurations, efficient training pipelines, scalable synthetic data generation, and evaluation methods to assess the quality of the generated data.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Usage</li> <li>IndoxGen-Torch Example</li> <li>IndoxGen-Tensor Example</li> <li>Configuration</li> <li>Evaluation Methods</li> <li>API Reference</li> <li>Contributing</li> </ul>"},{"location":"IndoxGen/GAN-Torch-Tensor/#installation","title":"Installation","text":"<p>To install IndoxGen-Torch or IndoxGen-Tensor, you need Python 3.9+.</p> <ul> <li> <p>For IndoxGen-Torch:</p> <p>pip install indoxgen-torch</p> </li> <li> <p>For IndoxGen-Tensor:</p> <p>pip install indoxgen-tensor</p> </li> </ul> <p>Both libraries require dependencies like PyTorch or TensorFlow, depending on the version you're using. Ensure your environment supports GPU for faster model training.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#usage","title":"Usage","text":""},{"location":"IndoxGen/GAN-Torch-Tensor/#indoxgen_torch_example","title":"IndoxGen Torch Example","text":"<pre><code>from indoxGen_torch import TabularGANConfig, TabularGANTrainer\nimport pandas as pd\n\n# Load your dataset\ndata = pd.read_csv(\"data/Adult.csv\")\n\n# Define column types\ncategorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\", \"income\"]\nmixed_columns = {\"capital-gain\": \"positive\", \"capital-loss\": \"positive\"}\ninteger_columns = [\"age\", \"fnlwgt\", \"hours-per-week\", \"capital-gain\", \"capital-loss\"]\n\n# Set up the configuration\nconfig = TabularGANConfig(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=128,\n    epochs=50,\n    n_critic=5\n)\n\n# Initialize the trainer\ntrainer = TabularGANTrainer(\n    config=config,\n    categorical_columns=categorical_columns,\n    mixed_columns=mixed_columns,\n    integer_columns=integer_columns\n)\n\n# Train the model\nhistory = trainer.train(data, patience=15)\n\n# Generate synthetic data\nsynthetic_data = trainer.generate_samples(50000)\nprint(synthetic_data.head())\n</code></pre>"},{"location":"IndoxGen/GAN-Torch-Tensor/#indoxgen_tensor_example","title":"IndoxGen Tensor Example","text":"<pre><code>from indoxGen_tensor import TabularGANConfig, TabularGANTrainer\nimport pandas as pd\n\n# Load your dataset\ndata = pd.read_csv(\"data/Adult.csv\")\n\n# Define column types\ncategorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\", \"income\"]\nmixed_columns = {\"capital-gain\": \"positive\", \"capital-loss\": \"positive\"}\ninteger_columns = [\"age\", \"fnlwgt\", \"hours-per-week\", \"capital-gain\", \"capital-loss\"]\n\n# Set up the configuration\nconfig = TabularGANConfig(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=128,\n    epochs=50,\n    n_critic=5\n)\n\n# Initialize the trainer\ntrainer = TabularGANTrainer(\n    config=config,\n    categorical_columns=categorical_columns,\n    mixed_columns=mixed_columns,\n    integer_columns=integer_columns\n)\n\n# Train the model\nhistory = trainer.train(data, patience=15)\n\n# Generate synthetic data\nsynthetic_data = trainer.generate_samples(50000)\nprint(synthetic_data.head())\n</code></pre>"},{"location":"IndoxGen/GAN-Torch-Tensor/#configuration","title":"Configuration","text":"<p>The TabularGANConfig class provides extensive customization options to adapt the model architecture and training process to your dataset.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#key_parameters","title":"Key Parameters:","text":"<ul> <li>input_dim: Dimension of the input noise vector for the generator.</li> <li>generator_layers: List of neuron counts in each layer of the generator.</li> <li>discriminator_layers: List of neuron counts in each layer of the discriminator.</li> <li>learning_rate: The learning rate used in the Adam optimizer.</li> <li>batch_size: Number of samples in each batch during training.</li> <li>epochs: Number of full passes over the dataset during training.</li> <li>n_critic: Number of discriminator updates per generator update, used in WGAN-GP.</li> </ul> <p>These parameters can be modified when initializing the TabularGANConfig.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#evaluation_methods","title":"Evaluation Methods","text":"<p>To ensure the quality of the synthetic data generated by IndoxGen-Torch and IndoxGen-Tensor, we provide several evaluation methods:</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#1_utility_evaluation","title":"1. Utility Evaluation","text":"<p>Utility evaluation compares the performance of machine learning classifiers trained on real data versus synthetic data. We assess the accuracy, AUC (Area Under Curve), and F1 score of various classifiers, including:</p> <ul> <li> <p>Logistic Regression</p> </li> <li> <p>Decision Tree</p> </li> <li> <p>Random Forest</p> </li> <li> <p>Multi-Layer Perceptron (MLP)</p> </li> </ul> <p>This helps determine how well synthetic data can replicate the utility of real data in predictive models.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#2_statistical_similarity","title":"2. Statistical Similarity","text":"<p>Statistical similarity evaluates how closely the synthetic data mirrors the real data. We use metrics like:</p> <ul> <li> <p>Wasserstein Distance (for continuous columns)</p> </li> <li> <p>Jensen-Shannon Divergence (for categorical columns)</p> </li> <li> <p>Correlation distance (between real and synthetic data correlation matrices)</p> </li> </ul> <p>These metrics give insight into how well the synthetic data captures the underlying statistical properties of the real dataset.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#3_privacy_evaluation","title":"3. Privacy Evaluation","text":"<p>We evaluate privacy by measuring:</p> <ul> <li> <p>Distance to Closest Record (DCR): The minimum distance between each real and synthetic data point.</p> </li> <li> <p>Nearest Neighbor Distance Ratio (NNDR): The ratio of distances between real and synthetic points to ensure diversity and privacy.</p> </li> </ul> <p>These metrics help ensure that synthetic data doesn't reveal sensitive information about real data records.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#4_data_drift_evaluation","title":"4. Data Drift Evaluation","text":"<p>Data drift is assessed by comparing the distributions of real and synthetic data using:</p> <ul> <li> <p>Population Stability Index (PSI) for categorical features.</p> </li> <li> <p>Kolmogorov-Smirnov (K-S) test for numerical features.</p> </li> </ul> <p>This ensures that the synthetic data maintains the same distributional properties as the real data, indicating no significant drift.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#5_visualization_of_distributions","title":"5. Visualization of Distributions","text":"<p>We provide tools to visualize the distributions of real and synthetic data side-by-side, allowing for easy comparison. This helps validate that the synthetic data replicates key patterns from the real data, especially for critical variables.</p>"},{"location":"IndoxGen/GAN-Torch-Tensor/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/GAN-Torch-Tensor/#tabularganconfig","title":"TabularGANConfig","text":""},{"location":"IndoxGen/GAN-Torch-Tensor/#__init___parameters","title":"<code>__init__</code> parameters:","text":"<ul> <li>input_dim: The size of the random noise vector.</li> <li>generator_layers: A list of integers specifying the number of neurons in each generator layer.</li> <li>discriminator_layers: A list of integers specifying the number of neurons in each discriminator layer.</li> <li>learning_rate: The learning rate for both the generator and discriminator (default: 0.0002).</li> <li>batch_size: The batch size for training (default: 128).</li> <li>epochs: The number of epochs for training (default: 50).</li> <li>n_critic: The number of discriminator updates per generator update (default: 5).</li> </ul>"},{"location":"IndoxGen/GAN-Torch-Tensor/#contributing","title":"Contributing","text":"<p>Contributions to IndoxGen-Torch and IndoxGen-Tensor are welcome. Follow these steps to contribute:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature or bug fix.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol> <p>We encourage contributions that improve the GAN models, add more data handling features, or extend the functionality of the framework.</p>"},{"location":"IndoxGen/GenerativeDataSynth/","title":"GenerativeDataSynth","text":""},{"location":"IndoxGen/GenerativeDataSynth/#overview","title":"Overview","text":"<p><code>GenerativeDataSynth</code> is a Python class designed for generating synthetic data based on example data and user instructions. It utilizes language models to generate and judge synthetic data points, ensuring diversity and adherence to specified criteria.</p>"},{"location":"IndoxGen/GenerativeDataSynth/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"IndoxGen/GenerativeDataSynth/#installation","title":"Installation","text":"<p>To use the <code>GenerativeDataSynth</code>, you need to have Python 3.9+ installed. You can install the package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"IndoxGen/GenerativeDataSynth/#language_model_setup","title":"Language Model Setup","text":"<p>The <code>GenerativeDataSynth</code> requires two language models: one for generating data and another for judging data quality. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up the language models:</p> <pre><code>from indoxGen.llms import OpenAi\n\n# Setup for OpenAI model\nopenai = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4-mini\")\n\n# Setup for NVIDIA model\nnemotron = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                  base_url=\"https://integrate.api.nvidia.com/v1\")\n</code></pre> <p>The <code>indoxGen</code> library supports various models including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides a router for OpenAI, allowing for easy switching between different models.</p> <p>When initializing the <code>GenerativeDataSynth</code>, you'll pass these language model instances as the <code>generator_llm</code> and <code>judge_llm</code> parameters.</p>"},{"location":"IndoxGen/GenerativeDataSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use the <code>GenerativeDataSynth</code>:</p> <pre><code>from indoxGen.synthCore import GenerativeDataSynth\nfrom indoxGen.llms import OpenAi\n\n# Setup language models\ngenerator_llm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4-mini\")\njudge_llm = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                   base_url=\"https://integrate.api.nvidia.com/v1\")\n\ncolumns = [\"name\", \"age\", \"occupation\"]\nexample_data = [\n    {\"name\": \"John Doe\", \"age\": 30, \"occupation\": \"Engineer\"},\n    {\"name\": \"Jane Smith\", \"age\": 28, \"occupation\": \"Teacher\"}\n]\nuser_instruction = \"Generate diverse fictional employee data\"\n\ngenerator = GenerativeDataSynth(\n    generator_llm=generator_llm,\n    judge_llm=judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction\n)\n\nsynthetic_data = generator.generate_data(num_samples=100)\nprint(synthetic_data)\n</code></pre>"},{"location":"IndoxGen/GenerativeDataSynth/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/GenerativeDataSynth/#generativedatasynth_1","title":"GenerativeDataSynth","text":""},{"location":"IndoxGen/GenerativeDataSynth/#__init__self_generator_llm_judge_llm_columns_example_data_user_instruction_real_datanone_diversity_threshold07_max_diversity_failures20_verbose0","title":"<code>__init__(self, generator_llm, judge_llm, columns, example_data, user_instruction, real_data=None, diversity_threshold=0.7, max_diversity_failures=20, verbose=0)</code>","text":"<p>Initialize the GenerativeDataSynth.</p> <ul> <li><code>generator_llm</code>: Language model for generating data.</li> <li><code>judge_llm</code>: Language model for judging data quality.</li> <li><code>columns</code>: List of column names for the synthetic data.</li> <li><code>example_data</code>: List of example data points.</li> <li><code>user_instruction</code>: Instruction for data generation.</li> <li><code>real_data</code>: Optional list of real data points.</li> <li><code>diversity_threshold</code>: Threshold for determining data diversity (default: 0.7).</li> <li><code>max_diversity_failures</code>: Maximum number of diversity failures before forcing acceptance (default: 20).</li> <li><code>verbose</code>: Verbosity level (0 for minimal output, 1 for detailed feedback) (default: 0).</li> </ul>"},{"location":"IndoxGen/GenerativeDataSynth/#generate_dataself_num_samples_int_-_pddataframe","title":"<code>generate_data(self, num_samples: int) -&gt; pd.DataFrame</code>","text":"<p>Generate synthetic data points.</p> <ul> <li><code>num_samples</code>: Number of data points to generate.</li> <li>Returns: DataFrame containing the generated data.</li> </ul>"},{"location":"IndoxGen/GenerativeDataSynth/#examples","title":"Examples","text":""},{"location":"IndoxGen/GenerativeDataSynth/#generating_employee_data","title":"Generating Employee Data","text":"<pre><code>columns = [\"name\", \"age\", \"department\", \"salary\"]\nexample_data = [\n    {\"name\": \"Alice Johnson\", \"age\": 35, \"department\": \"Marketing\", \"salary\": 75000},\n    {\"name\": \"Bob Williams\", \"age\": 42, \"department\": \"Engineering\", \"salary\": 90000}\n]\nuser_instruction = \"Generate diverse employee data for a tech company\"\n\ngenerator = GenerativeDataSynth(\n    generator_llm=your_generator_llm,\n    judge_llm=your_judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    verbose=1\n)\n\nemployee_data = generator.generate_data(num_samples=50)\nprint(employee_data.head())\n</code></pre>"},{"location":"IndoxGen/GenerativeDataSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>GenerativeDataSynth</code> are welcome. Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol>"},{"location":"IndoxGen/HybridGAN%2BLLM/","title":"Hybrid LLM-GAN: Synthetic Text and Numerical Data Generation Framework","text":""},{"location":"IndoxGen/HybridGAN%2BLLM/#overview","title":"Overview","text":"<p>Hybrid LLM-GAN is a powerful framework that combines Generative Adversarial Networks (GANs) for generating synthetic numerical data and Language Models (LLMs) for generating synthetic text data. This hybrid approach allows for generating diverse datasets that contain both structured numerical columns (e.g., age, income) and unstructured text columns (e.g., job title, remarks).</p> <p>The framework leverages GANs for structured data generation and LLMs for unstructured data generation, ensuring that the two are coherent and contextually aligned.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Usage</li> <li>Example: GAN and LLM Hybrid Pipeline</li> <li>Configuration</li> <li>API Reference</li> <li>Contributing</li> </ul>"},{"location":"IndoxGen/HybridGAN%2BLLM/#installation","title":"Installation","text":"<p>To install the hybrid framework, ensure you have Python 3.9+. Install both IndoxGen-Torch (for GAN) and the necessary LLM libraries (for text generation):</p> <pre><code>pip install indoxgen-torch\npip install openai  # Or any other LLM provider library\npip install python-dotenv  # For managing API keys securely\n</code></pre> <p>Additionally, make sure you have PyTorch or TensorFlow (depending on your system configuration) installed for the GAN-based part.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#usage","title":"Usage","text":""},{"location":"IndoxGen/HybridGAN%2BLLM/#example_gan_and_llm_hybrid_pipeline","title":"Example: GAN and LLM Hybrid Pipeline","text":"<pre><code>import pandas as pd\nfrom indoxGen.hybrid_synth import TextTabularSynth, initialize_gan_synth, initialize_llm_synth\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables (API keys for LLM)\nload_dotenv('api.env')\n\nINDOX_API_KEY = os.environ['INDOX_API_KEY']\nNVIDIA_API_KEY = os.environ['NVIDIA_API_KEY']\n\nfrom indoxGen.llms import OpenAi, IndoxApi\n\n# Initialize LLMs for text generation\nindox = IndoxApi(api_key=INDOX_API_KEY)\nnemotron = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\", base_url=\"https://integrate.api.nvidia.com/v1\")\n\n# Sample dataset containing both numerical and text data\nsample_data = [\n    {'age': 25, 'income': 45.5, 'years_of_experience': 3, 'job_title': 'Junior Developer', 'remarks': 'Looking to grow my career.'},\n    {'age': 32, 'income': 60.0, 'years_of_experience': 7, 'job_title': 'Developer', 'remarks': 'Experienced professional.'},\n    # ... more data entries\n]\n\ndata = pd.DataFrame(sample_data)\n\n# Define numerical and text columns\nnumerical_columns = ['age', 'income', 'years_of_experience']\ntext_columns = ['job_title', 'remarks']\n\n# Initialize LLM setup for text generation\nllm_setup = initialize_llm_synth(\n    generator_llm=nemotron,\n    judge_llm=indox,\n    columns=text_columns,\n    example_data=sample_data,\n    user_instruction=\"Generate realistic and diverse text data based on the provided numerical context.\",\n    diversity_threshold=0.4,\n    max_diversity_failures=30,\n    verbose=1\n)\n\n# Initialize GAN setup for numerical data generation\nnumerical_data = data[numerical_columns]\ngan_setup = initialize_gan_synth(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=64,\n    epochs=50,\n    n_critic=5,\n    categorical_columns=[],\n    mixed_columns={},\n    integer_columns=['age', 'years_of_experience'],\n    data=numerical_data\n)\n\n# Create the hybrid pipeline for both text and numerical generation\nsynth_pipeline = TextTabularSynth(tabular=gan_setup, text=llm_setup)\n\n# Generate synthetic data\nnum_samples = 10\nsynthetic_data = synth_pipeline.generate(num_samples)\n\n# Preview the generated synthetic data\nprint(synthetic_data.head())\n</code></pre>"},{"location":"IndoxGen/HybridGAN%2BLLM/#configuration","title":"Configuration","text":""},{"location":"IndoxGen/HybridGAN%2BLLM/#gan_configuration","title":"GAN Configuration:","text":"<p>The TabularGANConfig class allows customization of the GAN model for numerical data generation. You can modify parameters like the number of layers, learning rates, and batch sizes for fine-tuning.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#llm_configuration","title":"LLM Configuration:","text":"<p>In addition to configuring the GAN, you also set up the LLM with the following parameters: - generator_llm: The language model used to generate text (e.g., OpenAI, Nemotron). - judge_llm: A model for judging the quality of generated text. - columns: The columns in the dataset that represent text data. - user_instruction: Custom instruction guiding the LLM to generate relevant and diverse text.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/HybridGAN%2BLLM/#initialize_gan_synth","title":"<code>initialize_gan_synth</code>","text":"<p>Used to initialize and configure the GAN for generating synthetic numerical data.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#initialize_llm_synth","title":"<code>initialize_llm_synth</code>","text":"<p>Initializes and configures the LLM for generating synthetic text data.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#texttabularsynth","title":"<code>TextTabularSynth</code>","text":"<p>Combines the GAN and LLM pipelines into one hybrid pipeline for generating synthetic data with both numerical and text fields.</p>"},{"location":"IndoxGen/HybridGAN%2BLLM/#contributing","title":"Contributing","text":"<p>Contributions to the Hybrid LLM-GAN project are welcome. Follow these steps to contribute: 1. Fork the repository. 2. Create a new branch for your feature or bug fix. 3. Add your changes and write tests if applicable. 4. Submit a pull request with a clear description of your changes.</p>"},{"location":"IndoxGen/InteractiveFeedbackSynth/","title":"InteractiveFeedbackSynth","text":""},{"location":"IndoxGen/InteractiveFeedbackSynth/#overview","title":"Overview","text":"<p><code>InteractiveFeedbackSynth</code> is a Python class designed to generate synthetic data based on example data and user instructions. It uses language models for data generation and quality assessment, ensuring the output is diverse, realistic, and adheres to specified criteria. The class also provides a human feedback mechanism for reviewing and regenerating data points.</p>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#table_of_contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ol>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#installation","title":"Installation","text":"<p>To use <code>InteractiveFeedbackSynth</code>, you need Python 3.9+ installed. Install the required dependencies using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#language_model_setup","title":"Language Model Setup","text":"<p><code>InteractiveFeedbackSynth</code> requires two language models: - Generator LLM: For generating synthetic data. - Judge LLM: For assessing the quality of generated data.</p> <p>Here\u2019s an example setup:</p> <pre><code>from indoxGen.llms import IndoxApi, OpenAi\n\n# Initialize the generator LLM\ngenerator_llm = IndoxApi(api_key=INDOX_API_KEY)\n\n# Initialize the judge LLM\njudge_llm = OpenAi(\n    api_key=NVIDIA_API_KEY,\n    model=\"nvidia/nemotron-4-340b-instruct\",\n    base_url=\"https://integrate.api.nvidia.com/v1\"\n)\n</code></pre> <p>These models are passed as <code>generator_llm</code> and <code>judge_llm</code> to <code>InteractiveFeedbackSynth</code>.</p>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#usage","title":"Usage","text":""},{"location":"IndoxGen/InteractiveFeedbackSynth/#example_basic_workflow","title":"Example: Basic Workflow","text":"<p>Below is an example workflow for generating synthetic data with <code>InteractiveFeedbackSynth</code>:</p> <pre><code>from indoxGen.synthCore import InteractiveFeedbackSynth\nfrom indoxGen.llms import IndoxApi, OpenAi\n\n# Setup language models\ngenerator_llm = IndoxApi(api_key=INDOX_API_KEY)\njudge_llm = OpenAi(\n    api_key=NVIDIA_API_KEY,\n    model=\"nvidia/nemotron-4-340b-instruct\",\n    base_url=\"https://integrate.api.nvidia.com/v1\"\n)\n\n# Define data structure\ncolumns = [\"name\", \"age\", \"occupation\"]\nexample_data = [\n    {\"name\": \"John Doe\", \"age\": 30, \"occupation\": \"Engineer\"},\n    {\"name\": \"Jane Smith\", \"age\": 28, \"occupation\": \"Teacher\"}\n]\nuser_instruction = \"Generate diverse fictional employee data\"\n\n# Initialize InteractiveFeedbackSynth\nsynth = InteractiveFeedbackSynth(\n    generator_llm=generator_llm,\n    judge_llm=judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    feedback_min_score=0.8\n)\n\n# Generate synthetic data\nsynthetic_data = synth.generate_data(num_samples=10)\nprint(synthetic_data)\n\n# Review and regenerate data\naccepted_rows = [0, 1, 2]\nregenerate_rows = [3, 4]\nfeedback = \"Ensure more diversity in occupations\"\nupdated_data = synth.user_review_and_regenerate(\n    accepted_rows,\n    regenerate_rows,\n    feedback,\n    min_score=0.7\n)\nprint(updated_data)\n</code></pre>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/InteractiveFeedbackSynth/#interactivefeedbacksynth_1","title":"<code>InteractiveFeedbackSynth</code>","text":""},{"location":"IndoxGen/InteractiveFeedbackSynth/#__init__","title":"<code>__init__</code>","text":"<p>Initialize the <code>InteractiveFeedbackSynth</code> class.</p> <p>Parameters:</p> <ul> <li> <p><code>generator_llm</code>: Language model for data generation.</p> </li> <li> <p><code>judge_llm</code>: Language model for assessing data quality.</p> </li> <li> <p><code>columns</code>: List of column names for the synthetic data.</p> </li> <li> <p><code>example_data</code>: Example dataset as a list of dictionaries.</p> </li> <li> <p><code>user_instruction</code>: Instruction for generating data.</p> </li> <li> <p><code>real_data</code> (optional): List of real data points.</p> </li> <li> <p><code>diversity_threshold</code> (default: 0.7): Threshold for determining data diversity.</p> </li> <li> <p><code>max_diversity_failures</code> (default: 20): Maximum allowed diversity failures.</p> </li> <li> <p><code>verbose</code> (default: 0): Verbosity level (0 for minimal output).</p> </li> <li> <p><code>feedback_min_score</code> (default: 0.8): Minimum score for accepting generated data.</p> </li> </ul>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#generate_data","title":"<code>generate_data</code>","text":"<p>Generates synthetic data points.</p> <p>Parameters:</p> <ul> <li><code>num_samples</code>: Number of synthetic data points to generate.</li> </ul> <p>Returns:</p> <ul> <li>A Pandas DataFrame containing the generated data.</li> </ul>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#user_review_and_regenerate","title":"<code>user_review_and_regenerate</code>","text":"<p>Allows users to review and regenerate synthetic data based on feedback.</p> <p>Parameters:</p> <ul> <li> <p><code>accepted_rows</code>: List of indices for rows to accept (or <code>['all']</code>).</p> </li> <li> <p><code>regenerate_rows</code>: List of indices for rows to regenerate (or <code>['all']</code>).</p> </li> <li> <p><code>regeneration_feedback</code>: Feedback for the regeneration process.</p> </li> <li> <p><code>min_score</code>: Minimum score for accepting regenerated data.</p> </li> </ul> <p>Returns:</p> <ul> <li>A Pandas DataFrame containing the updated data.</li> </ul>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#examples","title":"Examples","text":""},{"location":"IndoxGen/InteractiveFeedbackSynth/#generating_employee_data_with_feedback","title":"Generating Employee Data with Feedback","text":"<pre><code>columns = [\"name\", \"age\", \"department\", \"salary\"]\nexample_data = [\n    {\"name\": \"Alice Johnson\", \"age\": 35, \"department\": \"Marketing\", \"salary\": 75000},\n    {\"name\": \"Bob Williams\", \"age\": 42, \"department\": \"Engineering\", \"salary\": 90000}\n]\nuser_instruction = \"Generate diverse employee data for a tech company\"\n\n# Initialize InteractiveFeedbackSynth\ngenerator = InteractiveFeedbackSynth(\n    generator_llm=your_generator_llm,\n    judge_llm=your_judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    verbose=1,\n    feedback_min_score=0.8\n)\n\n# Generate synthetic data\nemployee_data = generator.generate_data(num_samples=50)\nprint(employee_data.head())\n\n# Review and regenerate data\naccepted_rows = [0, 1, 2]\nregenerate_rows = [3, 4]\nfeedback = \"Ensure diversity in departments and wider salary ranges\"\nupdated_data = generator.user_review_and_regenerate(\n    accepted_rows,\n    regenerate_rows,\n    feedback,\n    min_score=0.7\n)\nprint(updated_data.head())\n</code></pre>"},{"location":"IndoxGen/InteractiveFeedbackSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>InteractiveFeedbackSynth</code> are welcome. Follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature or bug fix.</li> <li>Add your changes and write tests, if applicable.</li> <li>Submit a pull request with a detailed description of your changes.</li> </ol> <p>When contributing, maintain the existing code style and add proper documentation for new features.</p>"},{"location":"IndoxGen/PromptBasedSynth/","title":"PromptBasedSynth","text":""},{"location":"IndoxGen/PromptBasedSynth/#overview","title":"Overview","text":"<p><code>PromptBasedSynth</code> is a Python class designed to generate synthetic data using a Language Learning Model (LLM) based on a prompt and predefined data structure. It allows users to generate data from scratch or augment existing data by feeding a DataFrame to the model. The class can handle both text generation and JSON responses, ensuring that the generated data fits the specified prompt and format.</p>"},{"location":"IndoxGen/PromptBasedSynth/#table_of_contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>LLM Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ol>"},{"location":"IndoxGen/PromptBasedSynth/#installation","title":"Installation","text":"<p>To use <code>PromptBasedSynth</code>, install the required libraries using pip: <pre><code>pip install pandas loguru json\n</code></pre></p> <p>Additionally, you need the <code>indoxGen</code> library to connect to various language models for data generation.</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"IndoxGen/PromptBasedSynth/#llm_setup","title":"LLM Setup","text":"<p><code>PromptBasedSynth</code> uses language models (LLMs) to generate synthetic data. The library supports various models via the <code>indoxGen</code> library.</p> <p>For example, you can initialize an <code>IndoxApi</code> model like this: <pre><code>from indoxGen.llms import IndoxApi\nimport os\nfrom dotenv import load_dotenv\n\n# Load API key from environment variables\nload_dotenv()\nINDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre></p> <p>The <code>indoxGen</code> library supports various models including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides a router for OpenAI, allowing for easy switching between different models.</p>"},{"location":"IndoxGen/PromptBasedSynth/#usage","title":"Usage","text":"<p>The <code>PromptBasedSynth</code> class can be used to either generate new data from a user-provided prompt or augment existing datasets by generating additional rows based on the provided data.</p>"},{"location":"IndoxGen/PromptBasedSynth/#example_1_generate_data_from_scratch","title":"Example 1: Generate data from scratch","text":"<pre><code>from indoxGen.synthCore import PromptBasedSynth\n\nuser_prompt = \"Generate a dataset with 3 column and 3 row about soccer.\"\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n# instruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndata_generator = PromptBasedSynth(llm=LLM,user_instruction=user_prompt,verbose=1)\n\ngenerated_df = data_generator.generate_data()\n\n# print(generated_df)\ndata_generator.save_to_excel(\"output_dataFromPrompt.xlsx\")\n</code></pre>"},{"location":"IndoxGen/PromptBasedSynth/#example_2_generate_data_using_an_existing_dataset","title":"Example 2: Generate data using an existing dataset","text":"<pre><code>from indoxGen.synthCore import PromptBasedSynth\nfrom indoxGen.utils import Excel\n\ndataset_file_path = \"output_dataFromPrompt.xlsx\"\n\nexcel_loader = Excel(dataset_file_path) \ndf = excel_loader.load()  \nuser_prompt = \" based on given dataset generate one unique row about soccer\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\nadded_row = PromptBasedSynth(llm=LLM, user_instruction=user_prompt, example_data=df, verbose=1).generate_data()\nprint(added_row)\n</code></pre>"},{"location":"IndoxGen/PromptBasedSynth/#api_reference","title":"API Reference","text":""},{"location":"IndoxGen/PromptBasedSynth/#promptbasedsynth_1","title":"<code>PromptBasedSynth</code>","text":""},{"location":"IndoxGen/PromptBasedSynth/#__init__self_prompt_name_str_args_dict_outputs_dict_dataframe_pddataframe__none","title":"<code>__init__(self, prompt_name: str, args: dict, outputs: dict, dataframe: pd.DataFrame = None)</code>","text":"<p>Initializes the <code>PromptBasedSynth</code> class.</p> <p>Arguments:</p> <ul> <li> <p><code>prompt_name</code> (str): The name of the prompt used for data generation.</p> </li> <li> <p><code>args</code> (dict): Arguments containing the LLM instance and user instructions.</p> </li> <li> <p><code>outputs</code> (dict): Expected output format.</p> </li> <li> <p><code>dataframe</code> (pd.DataFrame, optional): Existing DataFrame to augment data from.</p> </li> </ul>"},{"location":"IndoxGen/PromptBasedSynth/#runself_-_pddataframe","title":"<code>run(self) -&gt; pd.DataFrame</code>","text":"<p>Generates the data and returns a DataFrame.</p> <p>Returns:</p> <ul> <li><code>pd.DataFrame</code>: A DataFrame containing generated or augmented data.</li> </ul>"},{"location":"IndoxGen/PromptBasedSynth/#save_to_excelself_file_path_str_-_none","title":"<code>save_to_excel(self, file_path: str) -&gt; None</code>","text":"<p>Saves the generated DataFrame to an Excel file.</p> <p>Arguments:</p> <ul> <li><code>file_path</code> (str): Path to save the Excel file.</li> </ul>"},{"location":"IndoxGen/PromptBasedSynth/#examples","title":"Examples","text":""},{"location":"IndoxGen/PromptBasedSynth/#generate_new_data_from_llm","title":"Generate New Data from LLM","text":"<pre><code>user_prompt = \"Generate a list of 3 planets and their distances from Earth.\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\ninstruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndata_generator = PromptBasedSynth(\n    prompt_name=\"Generate Planet Data\",\n    args={\n        \"llm\": LLM,\n        \"n\": 1,\n        \"instruction\": instruction,\n    },\n    outputs={\"generations\": \"generate\"},\n)\n\ngenerated_df = data_generator.run()\nprint(generated_df)\ndata_generator.save_to_excel(\"planet_data.xlsx\")\n</code></pre>"},{"location":"IndoxGen/PromptBasedSynth/#augment_existing_data","title":"Augment Existing Data","text":"<pre><code>dataset_file_path = \"planet_data.xlsx\"\n\nexcel_loader = Excel(dataset_file_path)\ndf = excel_loader.load()\n\nuser_prompt = \"Add a new planet to the dataset.\"\ninstruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndataset = PromptBasedSynth(\n    prompt_name=\"Augment Planet Data\",\n    args={\n        \"llm\": LLM,\n        \"n\": 1,\n        \"instruction\": instruction,\n    },\n    outputs={\"generations\": \"generate\"},\n    dataframe=df\n)\n\nupdated_df = dataset.run()\nprint(updated_df)\ndataset.save_to_excel(\"updated_planet_data.xlsx\")\n</code></pre>"},{"location":"IndoxGen/PromptBasedSynth/#contributing","title":"Contributing","text":"<p>Contributions to the <code>PromptBasedSynth</code> class are welcome. To contribute:</p> <ol> <li> <p>Fork the repository.</p> </li> <li> <p>Create a new branch for your feature.</p> </li> <li> <p>Add your changes and write tests if applicable.</p> </li> <li> <p>Submit a pull request with a clear description of your changes.</p> </li> </ol>"},{"location":"IndoxJudge/","title":"Home","text":""},{"location":"IndoxJudge/#overview","title":"Overview","text":"<p>IndoxJudge is a comprehensive evaluation framework designed for assessing the performance, reliability, and safety of machine learning models and data-driven systems. It provides a robust set of tools and metrics to evaluate models on critical aspects such as relevance, faithfulness, bias, and fairness. With its modular pipeline structure, IndoxJudge enables customizable evaluation workflows tailored to your specific requirements.</p> <p>Whether you're testing a new language model or validating the output of a generative AI system, IndoxJudge ensures transparency, accountability, and high-quality results.</p>"},{"location":"IndoxJudge/#key_features","title":"Key Features","text":""},{"location":"IndoxJudge/#1_advanced_metrics","title":"1. Advanced Metrics","text":"<p>Evaluate models on various dimensions, including:</p> <ul> <li> <p>Relevance: How well does the model's output align with the input query?</p> </li> <li> <p>Faithfulness: Does the output stay true to the input data or reasoning process?</p> </li> <li> <p>Bias and Fairness: Assess for harmful biases, stereotypes, or discriminatory patterns.</p> </li> <li> <p>Toxicity and Safety: Identify toxic or harmful outputs to ensure safety and compliance.</p> </li> </ul> <p>Learn more about Metrics \u2192</p>"},{"location":"IndoxJudge/#2_modular_pipelines","title":"2. Modular Pipelines","text":"<p>IndoxJudge provides a flexible pipeline system for building and customizing evaluation workflows:</p> <ul> <li> <p>Pre-built pipelines for common evaluation tasks.</p> </li> <li> <p>Easy integration with custom metrics and evaluators.</p> </li> <li> <p>Support for automated and manual review processes.</p> </li> </ul> <p>Learn more about Pipelines \u2192</p>"},{"location":"IndoxJudge/#3_integration_with_llms","title":"3. Integration with LLMs","text":"<p>Leverage the power of large language models (LLMs) for:</p> <ul> <li> <p>Advanced text understanding in evaluations.</p> </li> <li> <p>Robust scoring of generated content.</p> </li> <li> <p>Context-aware assessments for more nuanced evaluations.</p> </li> </ul>"},{"location":"IndoxJudge/#4_comprehensive_reporting","title":"4. Comprehensive Reporting","text":"<p>Generate detailed evaluation reports, including:</p> <ul> <li> <p>Visual summaries of model performance.</p> </li> <li> <p>Metrics comparison across datasets or models.</p> </li> <li> <p>Recommendations for improving model reliability.</p> </li> </ul>"},{"location":"IndoxJudge/#use_cases","title":"Use Cases","text":""},{"location":"IndoxJudge/#1_large_language_model_evaluation","title":"1. Large Language Model Evaluation","text":"<p>Assess LLMs like GPT or BERT on tasks such as summarization, question answering, and dialogue generation.</p>"},{"location":"IndoxJudge/#2_bias_and_fairness_analysis","title":"2. Bias and Fairness Analysis","text":"<p>Ensure AI systems are free from harmful biases and meet ethical guidelines.</p>"},{"location":"IndoxJudge/#3_safety_and_toxicity_testing","title":"3. Safety and Toxicity Testing","text":"<p>Evaluate AI systems for toxic or unsafe outputs to ensure compliance with industry standards.</p>"},{"location":"IndoxJudge/#4_custom_model_assessment","title":"4. Custom Model Assessment","text":"<p>Define and measure custom metrics for domain-specific applications, such as medical AI or financial predictions.</p>"},{"location":"IndoxJudge/#getting_started","title":"Getting Started","text":""},{"location":"IndoxJudge/#1_install_indoxjudge","title":"1. Install IndoxJudge","text":"<pre><code>pip install indoxjudge\n</code></pre>"},{"location":"IndoxJudge/#2_explore_metrics","title":"2. Explore Metrics","text":"<p>Dive into the available metrics and learn how to configure them for your use case: Metrics Documentation \u2192</p>"},{"location":"IndoxJudge/#3_build_an_evaluation_pipeline","title":"3. Build an Evaluation Pipeline","text":"<p>Use pre-built pipelines or create your own: Pipelines Documentation \u2192</p>"},{"location":"IndoxJudge/#feedback_and_contributions","title":"Feedback and Contributions","text":"<p>We value your feedback! If you have any questions, suggestions, or ideas for improvement, please reach out to us or submit an issue in our GitHub repository. Contributions are always welcome!</p>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/","title":"AdversarialRobustness","text":"<p>Class for evaluating the adversarial robustness of language model outputs by analyzing the robustness score, reasons, and verdicts using a specified language model.</p>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#initialization","title":"Initialization","text":"<p>The <code>AdversarialRobustness</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for adversarial robustness.</li> </ul> <pre><code>class AdversarialRobustness:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the AdversarialRobustness class to evaluate the robustness of language model outputs\n        by analyzing various aspects including perturbation resistance and consistency.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for adversarial robustness.\n        \"\"\"\n        self.model = None\n        self.template = RobustnessTemplate()\n        self.input_sentence = input_sentence\n        self.robustness_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for adversarial robustness.</li> </ul>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>AdversarialRobustness</code> class:</p> <pre><code>from indoxJudge.metrics import AdversarialRobustness\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The model predicts that the likelihood of success is low.\"\n\n# Initialize the AdversarialRobustness object\nrobustness = AdversarialRobustness(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[robustness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[AdversarialRobustness Initialization] --&gt; B[Judge Model]\n    B --&gt; C[get_robustness]\n    B --&gt; D[get_reason]\n    B --&gt; E[get_verdict]\n    C --&gt; F[Generate Prompt]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Call Judge Model]\n    G --&gt; H{Parse JSON Response}\n    H -- Valid --&gt; I[Return Verdict or Reason]\n    H -- Invalid --&gt; J[Return Default Verdict or Reason]\n    I --&gt; K[Update Robustness Score]\n    K --&gt; L[Return Final Score]\n</code></pre>"},{"location":"IndoxJudge/metrics/AdversarialRobustness/#notes","title":"Notes","text":"<ul> <li>The robustness evaluation considers multiple factors including semantic preservation, grammatical correctness, and logical consistency.</li> <li>The evaluation process uses the specified language model to generate adversarial examples and assess their impact.</li> <li>The class uses a default RobustnessTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/","title":"AnswerRelevancy","text":"<p>Class for evaluating the relevancy of language model outputs by analyzing statements, generating verdicts, and calculating relevancy scores.</p>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/#initialization","title":"Initialization","text":"<p>The <code>AnswerRelevancy</code> class is initialized with the following parameters:</p> <ul> <li><code>query</code>: The query being evaluated.</li> <li><code>llm_response</code>: The response generated by the language model.</li> <li><code>model</code>: The language model to use for evaluation. Defaults to <code>None</code>.</li> <li><code>threshold</code>: The threshold for determining relevancy. Defaults to <code>0.5</code>.</li> <li><code>include_reason</code>: Whether to include reasoning for the relevancy verdicts. Defaults to <code>True</code>.</li> <li><code>strict_mode</code>: Whether to use strict mode, which forces a score of 0 if relevancy is below the threshold. Defaults to <code>False</code>.</li> </ul> <pre><code>class AnswerRelevancy:\n    \"\"\"\n    Class for evaluating the relevancy of language model outputs by analyzing statements,\n    generating verdicts, and calculating relevancy scores.\n    \"\"\"\n    def __init__(self, query: str, llm_response: str, model=None, threshold: float = 0.5, include_reason: bool = True,\n                 strict_mode: bool = False):\n        \"\"\"\n        Initializes the AnswerRelevancy class with the query, LLM response, and evaluation settings.\n\n        :param query: The query being evaluated.\n        :param llm_response: The response generated by the language model.\n        :param model: The language model to use for evaluation. Defaults to None.\n        :param threshold: The threshold for determining relevancy. Defaults to 0.5.\n        :param include_reason: Whether to include reasoning for the relevancy verdicts. Defaults to True.\n        :param strict_mode: Whether to use strict mode, which forces a score of 0 if relevancy is below the threshold. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>query: A string containing the query for which relevancy is being evaluated.</p> </li> <li> <p>llm_response: A string containing the response from the language model that needs to be evaluated.</p> </li> <li> <p>model: This is the language model object used for the evaluation process. If not provided, the default model is used.</p> </li> <li> <p>threshold: A float value representing the minimum relevancy score required for a response to be considered relevant. The default value is 0.5.</p> </li> <li> <p>include_reason: A boolean indicating whether the evaluation should include detailed reasons for the relevancy verdict. Default is True.</p> </li> <li> <p>strict_mode: A boolean that, when set to True, ensures that any score below the threshold results in a relevancy score of 0. This is useful for enforcing strict relevancy criteria. Default is False.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>AnswerRelevancy</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import AnswerRelevancy\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the query and the response to be evaluated\nquery = \"What is the capital of France?\"\nllm_response = \"The capital of France is Paris.\"\n\n# Initialize the AnswerRelevancy metric\nanswer_relevancy_metric = AnswerRelevancy(\n    query=query,\n    llm_response=llm_response,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\nevaluator = Evaluator(model=llm, metrics=[answer_relevancy_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/#hyperparameters_explanation_1","title":"Hyperparameters Explanation","text":"<ul> <li> <p>query: A string representing the query for which the contextual relevancy is being evaluated.</p> </li> <li> <p>retrieval_context: A list of strings, each representing a context retrieved in response to the query.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/AnswerRelevancy/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[AnswerRelevancy Initialization] --&gt; B[Judge Model]\n    B --&gt; C[Measure Relevancy]\n    C --&gt; D[Generate Statements]\n    D --&gt; E[Judge Model Evaluates Statements]\n    E --&gt; F[Generate Verdicts]\n    F --&gt; G[Calculate Relevancy Score]\n    G --&gt; H[Check for Irrelevant Statements]\n    H --&gt; I[Generate Reason for Irrelevant Statements]\n    I --&gt; J[Return Final Score and Verdict]\n</code></pre>"},{"location":"IndoxJudge/metrics/BLEU/","title":"BLEU","text":"<p>Class for evaluating the similarity between a generated response and one or more expected responses using the BLEU metric, which is based on n-gram overlaps.</p>"},{"location":"IndoxJudge/metrics/BLEU/#initialization","title":"Initialization","text":"<p>The <code>BLEU</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by a language model.</li> <li>retrieval_context: The expected response(s) to compare against the actual response.</li> <li>n: The maximum size of the n-grams to use for evaluation.</li> <li>remove_repeating_ngrams: Option to remove repeating n-grams from consideration.</li> </ul> <pre><code>class BLEU:\n    def __init__(\n        self,\n        llm_response: str,\n        retrieval_context: Union[str, List[str]],\n        n: int = 2,\n        remove_repeating_ngrams: bool = False,\n    ):\n        \"\"\"\n        Initialize the BLEU evaluator with the desired n-gram size and option to remove repeating n-grams.\n\n        Parameters:\n        llm_response (str): The response generated by a language model.\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\n        n (int): The maximum size of the n-grams to use for evaluation (default is 2).\n        remove_repeating_ngrams (bool): Whether to remove repeating n-grams (default is False).\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/BLEU/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li> <p>llm_response: The actual response generated by the language model that needs to be evaluated.</p> </li> <li> <p>retrieval_context: The expected responses for comparison. Can be a single string or a list of strings.</p> </li> <li> <p>n: The maximum n-gram size to use for evaluation. Default is <code>2</code>.</p> </li> <li> <p>remove_repeating_ngrams: A boolean flag to indicate whether to remove repeating n-grams from consideration. Default is <code>False</code>.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/BLEU/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>BLEU</code> class:</p> <pre><code>from indoxJudge.metrics import BLEU\nfrom indoxJudge.pipelines import CustomEvaluator\n\n# Define a sample response and context\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\nretrieval_context = [\n    \"The fast brown fox leaps over the lazy dog.\",\n    \"A speedy brown fox jumps over a sleepy dog.\"\n]\n\n# Initialize the BLEU object\nbleu = BLEU(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context,\n    n=2,\n    remove_repeating_ngrams=False\n)\n\n# Measure the BLEU score\nevaluator = CustomEvaluator(model=None, metrics=[bleu])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/BLEU/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[BLEU Initialization] --&gt; B[Set n-gram Size and Options]\n    B --&gt; C[Measure BLEU Score]\n    C --&gt; D[Preprocess and Tokenize Texts]\n    D --&gt; E[Generate n-grams for Context and LLM Response]\n    E --&gt; F[Calculate Clipped Precision]\n    F --&gt; G[Calculate Brevity Penalty]\n    G --&gt; H[Combine Precision and Penalty]\n    H --&gt; I[Compute BLEU Score]\n    I --&gt; J[Chunk Text if Necessary]\n    J --&gt; K[Calculate Average BLEU Score]\n    K --&gt; L[Return Final BLEU Score]\n</code></pre>"},{"location":"IndoxJudge/metrics/BertScoreSummary/","title":"BertScore","text":"<p>Class for evaluating semantic similarity between generated and reference texts using BERT embeddings.</p>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#initialization","title":"Initialization","text":"<pre><code>class BertScore:\n    def __init__(\n        self,\n        generated: str,\n        reference: str,\n        embedding_model_name: str = \"bert-base-uncased\",\n        include_reason: bool = True,\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        model=None,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>generated: Generated text to be evaluated</li> <li>reference: Original reference text for comparison</li> <li>embedding_model_name: BERT model for embeddings (default: \"bert-base-uncased\")</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> <li>device: Computation device (default: CUDA if available, else CPU)</li> <li>model: Optional language model for generating verdicts</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#usage_example","title":"Usage Example","text":"<pre><code>from bertscore import BertScore\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare generated and reference texts\ngenerated_text = \"The quick brown fox jumps over the lazy dog.\"\nreference_text = \"A fast brown fox leaps above a sleepy canine.\"\n\n# Create BertScore instance\nbert_metric = BertScore(\n    generated=generated_text,\n    reference=reference_text,\n    include_reason=True,\n    embedding_model_name=\"bert-base-uncased\"\n)\n\n# Set the language model\nbert_metric.set_model(llm)\n\n# Perform semantic similarity evaluation\nresult = bert_metric.measure()\n\n# Access the results\nprint(result['overall_score'])  # Overall semantic similarity score\nprint(result['detailed_scores'])  # Detailed BERTScore metrics\nprint(result['verdict'])  # Textual explanation of similarity\n</code></pre>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>overall_score</code>: Semantic similarity F1 score (0-1 range)</li> <li><code>detailed_scores</code>: Comprehensive BERTScore metrics</li> <li><code>precision</code>: Semantic precision</li> <li><code>recall</code>: Semantic recall</li> <li><code>f1</code>: F1 score of semantic similarity</li> <li><code>details</code>: Additional similarity metrics<ul> <li><code>avg_precision_similarity</code></li> <li><code>avg_recall_similarity</code></li> <li><code>token_matches</code></li> <li><code>max_similarity</code></li> <li><code>min_similarity</code></li> </ul> </li> <li><code>verdict</code>: Detailed textual explanation of semantic similarity (if <code>include_reason</code> is True)</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#key_features","title":"Key Features","text":"<ul> <li>Advanced semantic similarity assessment</li> <li>BERT-based embedding comparison</li> <li>Precision, recall, and F1 score calculation</li> <li>Flexible embedding model selection</li> <li>Configurable device support (CPU/CUDA)</li> <li>Detailed similarity metrics</li> <li>Optional natural language explanation</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#dependencies","title":"Dependencies","text":"<ul> <li><code>torch</code>: Deep learning framework</li> <li><code>transformers</code>: Hugging Face transformer models</li> <li><code>numpy</code>: Numerical computing</li> <li><code>scikit-learn</code>: Cosine similarity calculation</li> <li><code>pydantic</code>: Data validation</li> <li><code>loguru</code>: Logging</li> <li><code>json</code>: Response parsing</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#computational_details","title":"Computational Details","text":"<ul> <li>Uses cosine similarity for embedding comparison</li> <li>Supports GPU acceleration via CUDA</li> <li>Configurable BERT embedding model</li> <li>Tracks token-level semantic matches</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#logging","title":"Logging","text":"<p>The class logs token usage information, including:</p> <ul> <li>Total input tokens</li> <li>Total output tokens</li> <li>Total token count used during evaluation</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>ValueError</code> if:</li> <li>No language model is set for verdict generation</li> <li>Language model returns an empty response</li> <li>Provides robust embedding and similarity computation</li> <li>Graceful handling of verdict generation failures</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#extensibility","title":"Extensibility","text":"<p>The class is designed to be easily extended:</p> <ul> <li>Customizable embedding model</li> <li>Pluggable language model for verdicts</li> <li>Flexible device support</li> <li>Detailed logging and</li> </ul>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[BertScore Initialization] --&gt; B[Load Pre-trained BERT Model and Tokenizer]\n    B --&gt; C[Measure BERTScore]\n    C --&gt; D[Generate Embeddings for Generated Text]\n    C --&gt; E[Generate Embeddings for Reference Text]\n    D --&gt; F[Calculate Similarity Matrix]\n    E --&gt; F\n    F --&gt; G[Calculate Precision, Recall, and F1 Score]\n    G --&gt; H[Generate Detailed Scores]\n    H --&gt; I[Optional: Include Reason for Verdict]\n    I --&gt; J[Return Final Verdict and Scores]\n\n</code></pre>"},{"location":"IndoxJudge/metrics/BertScoreSummary/#note","title":"Note","text":"<p>BertScore provides a sophisticated approach to semantic similarity evaluation, leveraging advanced transformer-based embeddings to capture nuanced textual relationships beyond simple lexical matching.</p>"},{"location":"IndoxJudge/metrics/Bertscore/","title":"BertScore","text":"<p>Class for evaluating the similarity between a generated response and one or more expected responses using embeddings from a pre-trained transformer model.</p>"},{"location":"IndoxJudge/metrics/Bertscore/#initialization","title":"Initialization","text":"<p>The <code>BertScore</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by a language model.</li> <li>retrieval_context: The expected response(s) to compare against the actual response.</li> <li>model_name: The identifier for the pre-trained transformer model to use for generating embeddings.</li> <li>max_length: The maximum length of input sequences to be processed by the model.</li> </ul> <pre><code>class BertScore:\n    def __init__(\n        self,\n        llm_response\n        retrieval_context,\n        model_name = \"bert-base-uncased\",\n        max_length = 1024,\n    ):\n        \"\"\"\n        Initialize the BertScore class to evaluate the similarity between a generated response\n        and one or more expected responses using a specified pre-trained transformer model.\n\n        Parameters:\n        llm_response (str): The response generated by a language model.\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\n        model_name (str): The identifier for the pre-trained model to be used for generating embeddings.\n                          Defaults to \"roberta-base\".\n        max_length (int): The maximum length of input sequences to be processed by the model. Defaults to 1024.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Bertscore/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li> <p>llm_response: The actual response generated by the language model that needs to be evaluated.</p> </li> <li> <p>retrieval_context: The expected responses for comparison. Can be a single string or a list of strings.</p> </li> <li> <p>model_name: The name of the pre-trained transformer model used for generating text embeddings. Default is <code>\"bert-base-uncased\"</code>.</p> </li> <li> <p>max_length: The maximum length for input sequences that the model will handle. Default is <code>1024</code>.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/Bertscore/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>BertScore</code> class:</p> <pre><code>from indoxJudge.metrics import BertScore\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample response and context\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\nretrieval_context = [\n    \"The fast brown fox leaps over the lazy dog.\",\n    \"A speedy brown fox jumps over a sleepy dog.\"\n]\n\n# Initialize the BertScore object\nbert_score = BertScore(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context,\n    max_length=512\n)\n\n# Measure the similarity\nevaluator = Evaluator(model=None, metrics=[bert_score])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Bertscore/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[BertScore Initialization] --&gt; B[Load Pre-trained Model and Tokenizer]\n    B --&gt; C[Measure Similarity Score]\n    C --&gt; D[Generate Embeddings for LLM Response]\n    C --&gt; E[Generate Embeddings for Context]\n    D --&gt; F[Calculate Cosine Similarity]\n    E --&gt; F\n    F --&gt; G[Calculate Precision, Recall, F1-Score]\n    G --&gt; H[Return Scores]\n</code></pre>"},{"location":"IndoxJudge/metrics/Bias/","title":"Bias","text":"<p>Class for evaluating potential bias in language model outputs by analyzing opinions, generating verdicts, and calculating bias scores.</p>"},{"location":"IndoxJudge/metrics/Bias/#initialization","title":"Initialization","text":"<p>The <code>Bias</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by the language model.</li> <li>threshold: The threshold for determining bias. Defaults to <code>0.5</code>.</li> <li>include_reason: Whether to include reasoning for the bias verdicts. Defaults to <code>True</code>.</li> <li>strict_mode: Whether to use strict mode, which forces a score of 1 if bias exceeds the threshold. Defaults to <code>False</code>.</li> </ul> <pre><code>class Bias:\n    \"\"\"\n    Class for evaluating potential bias in language model outputs by analyzing opinions,\n    generating verdicts, and calculating bias scores.\n    \"\"\"\n    def __init__(self, llm_response, threshold: float = 0.5, include_reason: bool = True, strict_mode: bool = False):\n        \"\"\"\n        Initializes the Bias class with the LLM response and evaluation settings.\n\n        :param llm_response: The response generated by the language model.\n        :param threshold: The threshold for determining bias. Defaults to 0.5.\n        :param include_reason: Whether to include reasoning for the bias verdicts. Defaults to True.\n        :param strict_mode: Whether to use strict mode, which forces a score of 1 if bias exceeds the threshold. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Bias/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>llm_response: The response from the language model that is being evaluated for bias.</p> </li> <li> <p>threshold: A float value representing the bias threshold. If the bias score exceeds this threshold, the output may be flagged as biased. The default value is 0.5.</p> </li> <li> <p>include_reason: A boolean that determines whether the evaluation should include reasoning for why a certain bias score was assigned. Default is True.</p> </li> <li> <p>strict_mode: A boolean that, when set to True, forces a score of 1 if the bias exceeds the threshold, regardless of the exact score. This is useful for stringent bias detection. Default is False.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/Bias/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Bias</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import Bias\nfrom indoxJudge.pipelines import Evaluator\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the query and the response to be evaluated\nquery = \"What is the capital of France?\"\nllm_response = \"The capital of France is Paris.\"\n\n# Initialize the evaluation metrics\nbias_metric = Bias(llm_response=llm_response, threshold=0.5, include_reason=True, strict_mode=False)\n\n# Initialize the Evaluator with the model and metrics\nevaluator = Evaluator(model=llm, metrics=[bias_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Bias/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Bias Initialization] --&gt; B[Set Threshold and Include Reason]\n    B --&gt; C[Measure Bias]\n    C --&gt; D[Generate Opinions]\n    D --&gt; E[Call Language Model to Extract Opinions]\n    E --&gt; F[Generate Verdicts for Opinions]\n    F --&gt; G[Calculate Bias Score]\n    G --&gt; H[Check Against Threshold]\n    H --&gt; I[Optional: Generate Reason for Bias]\n    I --&gt; J[Return Final Bias Score and Verdict]\n</code></pre>"},{"location":"IndoxJudge/metrics/BleuSummary/","title":"BLEU Score","text":"<p>Class for evaluating the quality of generated text by comparing it to reference text using the BLEU (Bilingual Evaluation Understudy) metric.</p>"},{"location":"IndoxJudge/metrics/BleuSummary/#initialization","title":"Initialization","text":"<pre><code>class Bleu:\n    def __init__(\n        self,\n        summary: str,\n        source: str,\n        weights: List[float] = None,\n        include_reason: bool = True,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/BleuSummary/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Generated text to be evaluated</li> <li>source: Original reference text for comparison</li> <li>weights: Weights for n-gram precisions (default: <code>[0.9, 0.1, 0.000, 0.000]</code>)</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#usage_example","title":"Usage Example","text":"<pre><code>from bleu import Bleu\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare generated and reference texts\ngenerated_text = \"The quick brown fox jumps over the lazy dog.\"\nreference_text = \"A fast brown fox leaps above a sleepy canine.\"\n\n# Create Bleu instance\nbleu_metric = Bleu(\n    summary=generated_text,\n    source=reference_text,\n    include_reason=True\n)\n\n# Set the language model\nbleu_metric.set_model(llm)\n\n# Perform BLEU score evaluation\nresult = bleu_metric.measure()\n\n# Access the results\nprint(result['overall_score'])  # Overall BLEU score\nprint(result['detailed_scores'])  # Detailed BLEU metrics\nprint(result['verdict'])  # Textual explanation of translation quality\n</code></pre>"},{"location":"IndoxJudge/metrics/BleuSummary/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>overall_score</code>: BLEU score (0-1 range)</li> <li><code>detailed_scores</code>: Comprehensive BLEU metric details</li> <li><code>score</code>: Overall BLEU score</li> <li><code>precisions</code>: Precision scores for different n-grams</li> <li><code>brevity_penalty</code>: Brevity penalty factor</li> <li><code>details</code>: Additional evaluation metrics<ul> <li><code>matching_1grams</code>: Number of matching unigrams</li> <li><code>matching_2grams</code>: Number of matching bigrams</li> <li><code>total_generated_1grams</code>: Total generated unigrams</li> <li><code>generated_length</code>: Length of generated text</li> <li><code>reference_length</code>: Length of reference text</li> </ul> </li> <li><code>verdict</code>: Detailed textual explanation of translation quality (if <code>include_reason</code> is True)</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#key_features","title":"Key Features","text":"<ul> <li>Precise text quality evaluation</li> <li>N-gram precision calculation</li> <li>Brevity penalty consideration</li> <li>Lemmatization-based preprocessing</li> <li>Flexible weighting of n-gram precisions</li> <li>Optional natural language explanation</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#preprocessing_techniques","title":"Preprocessing Techniques","text":"<ul> <li>Lowercase conversion</li> <li>Special character removal</li> <li>Tokenization</li> <li>Lemmatization using WordNet</li> <li>N-gram extraction</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#computational_details","title":"Computational Details","text":"<ul> <li>Uses modified precision calculation</li> <li>Applies smoothing for higher-order n-grams</li> <li>Calculates brevity penalty</li> <li>Bounds final score between 0 and 1</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#logging","title":"Logging","text":"<p>The class provides token usage tracking:</p> <ul> <li>Total input tokens</li> <li>Total output tokens</li> <li>Detailed token count information</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>ValueError</code> if:</li> <li>Language model returns an empty response</li> <li>Robust preprocessing and scoring mechanism</li> <li>Graceful handling of text variations</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#extensibility","title":"Extensibility","text":"<p>The class is designed to be easily extended:</p> <ul> <li>Customizable n-gram weights</li> <li>Flexible preprocessing</li> <li>Pluggable language model for verdicts</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#dependencies","title":"Dependencies","text":"<ul> <li><code>nltk</code>: Natural Language Toolkit</li> <li><code>pydantic</code>: Data validation</li> <li><code>loguru</code>: Logging</li> <li><code>json</code>: Response parsing</li> <li><code>math</code>: Mathematical operations</li> <li><code>re</code>: Regular expression processing</li> </ul>"},{"location":"IndoxJudge/metrics/BleuSummary/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Bleu Initialization] --&gt; B[Set Texts]\n    B --&gt; C[Measure BLEU Score]\n    C --&gt; D[Preprocess Texts]\n    D --&gt; E[Calculate Modified Precision]\n    E --&gt; F[Calculate Brevity Penalty]\n    F --&gt; G[Combine Precisions and Penalty]\n    G --&gt; H[Compute Final BLEU Score]\n    H --&gt; I[Optional: Generate Verdict]\n    I --&gt; J[Return Score and Details]\n\n</code></pre>"},{"location":"IndoxJudge/metrics/BleuSummary/#note","title":"Note","text":"<p>BLEU score provides a standardized method for evaluating text generation quality, particularly useful in machine translation, summarization, and text generation tasks. It captures the precision of n-grams between generated and reference texts while penalizing very short outputs.</p>"},{"location":"IndoxJudge/metrics/Conciseness/","title":"Conciseness","text":"<p>Class for evaluating the conciseness of language model outputs by analyzing redundancy, wordiness, and length efficiency.</p>"},{"location":"IndoxJudge/metrics/Conciseness/#initialization","title":"Initialization","text":"<p>The <code>Conciseness</code> class is initialized with the following parameters:</p> <ul> <li><code>summary</code>: The text to be evaluated for conciseness.</li> <li><code>source_text</code>: Optional original text for comparison. Defaults to <code>None</code>.</li> <li><code>target_length</code>: Optional target length for the summary. Defaults to <code>None</code>.</li> <li><code>weights</code>: Optional dictionary to customize scoring weights. Defaults to <code>None</code>.</li> <li><code>conciseness_threshold</code>: The threshold for determining conciseness. Defaults to <code>0.7</code>.</li> </ul> <pre><code>class Conciseness:\n    \"\"\"\n    Class for evaluating the conciseness of language model outputs by analyzing\n    redundancy, wordiness, and length efficiency.\n    \"\"\"\n    def __init__(\n        self,\n        summary: str,\n        source_text: str = None,\n        target_length: int = None,\n        weights: Dict[str, float] = None,\n        conciseness_threshold: float = 0.7,\n    ):\n        \"\"\"\n        Initializes the Conciseness class with the summary, source text, and evaluation settings.\n        :param summary: The text to be evaluated for conciseness.\n        :param source_text: Optional original text for comparison. Defaults to None.\n        :param target_length: Optional target length for the summary. Defaults to None.\n        :param weights: Optional dictionary to customize scoring weights. Defaults to None.\n        :param conciseness_threshold: The threshold for determining conciseness. Defaults to 0.7.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Conciseness/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li>summary: A string containing the text to be evaluated for conciseness.</li> <li>source_text: An optional string representing the original text for comparative analysis. If provided, helps in assessing the summary's efficiency.</li> <li>target_length: An optional integer specifying the desired length of the summary, used to evaluate length efficiency.</li> <li>weights: A dictionary allowing customization of scoring weights. Default weights are:</li> <li><code>\"redundancy\"</code>: 0.4</li> <li><code>\"wordiness\"</code>: 0.4</li> <li><code>\"length_ratio\"</code>: 0.2</li> <li>conciseness_threshold: A float value representing the minimum conciseness score required. The default value is 0.7.</li> </ul>"},{"location":"IndoxJudge/metrics/Conciseness/#key_metrics","title":"Key Metrics","text":"<p>The <code>Conciseness</code> class provides detailed metrics:</p> <ol> <li> <p>Redundancy Analysis</p> </li> <li> <p>Identifies repeated phrases</p> </li> <li>Detects redundant information</li> <li> <p>Generates a redundancy score</p> </li> <li> <p>Wordiness Metrics</p> </li> <li> <p>Counts total words</p> </li> <li>Calculates average sentence length</li> <li>Identifies filler words</li> <li>Detects complex phrases</li> <li> <p>Generates a wordiness score</p> </li> <li> <p>Length Efficiency</p> </li> <li>Compares summary length to source text</li> <li>Calculates length ratio score</li> </ol>"},{"location":"IndoxJudge/metrics/Conciseness/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Conciseness\nfrom indoxJudge.models import YourLanguageModel\n\n# Initialize the language model\nllm = YourLanguageModel()\n\n# Define the summary and source text\nsummary = \"Paris is the capital of France, located in France.\"\nsource_text = \"Paris is the beautiful capital of the European country France, situated in Western Europe.\"\n\n# Initialize the Conciseness metric\nconciseness_metric = Conciseness(\n    summary=summary,\n    source_text=source_text,\n    target_length=50,  # Optional target length\n    weights={\"redundancy\": 0.5, \"wordiness\": 0.3, \"length_ratio\": 0.2},  # Optional custom weights\n    conciseness_threshold=0.7\n)\n\n# Set the language model\nconciseness_metric.set_model(llm)\n\n# Measure conciseness\nresult = conciseness_metric.measure()\nprint(result)\n</code></pre>"},{"location":"IndoxJudge/metrics/Conciseness/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a detailed dictionary with:</p> <ul> <li><code>overall_score</code>: Weighted conciseness score</li> <li><code>metrics</code>: Breakdown of redundancy, wordiness, and length ratio scores</li> <li><code>issues</code>: Specific conciseness issues found</li> <li><code>suggestions</code>: Recommendations for improving conciseness</li> </ul>"},{"location":"IndoxJudge/metrics/Conciseness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Conciseness] --&gt; B[Set Summary, Source Text, and Weights]\n    B --&gt; C[Measure Conciseness]\n    C --&gt; D[Analyze Redundancy]\n    D --&gt; E[Measure Wordiness]\n    E --&gt; F[Calculate Length Ratio]\n    F --&gt; G[Calculate Weighted Score]\n    G --&gt; H[Identify Issues]\n    H --&gt; I[Generate Suggestions]\n    I --&gt; J[Return Detailed Report and Score]\n</code></pre>"},{"location":"IndoxJudge/metrics/Conciseness/#key_features","title":"Key Features","text":"<ul> <li>Identifies and quantifies redundancy</li> <li>Highlights wordiness and complex language</li> <li>Provides actionable improvement suggestions</li> <li>Flexible scoring with customizable weights</li> <li>Supports comparative analysis with source text</li> </ul>"},{"location":"IndoxJudge/metrics/ContextualRelevancy/","title":"ContextualRelevancy","text":"<p>Class for evaluating the contextual relevancy of retrieval contexts based on a given query using a specified language model.</p>"},{"location":"IndoxJudge/metrics/ContextualRelevancy/#initialization","title":"Initialization","text":"<p>The <code>ContextualRelevancy</code> class is initialized with the following parameters:</p> <ul> <li>query: The query being evaluated.</li> <li>retrieval_context: A list of contexts retrieved for the query.</li> </ul> <pre><code>class ContextualRelevancy:\n    \"\"\"\n    Class for evaluating the contextual relevancy of retrieval contexts based on a given query\n    using a specified language model.\n    \"\"\"\n    def __init__(self, query: str, retrieval_context: List[str]):\n        \"\"\"\n        Initializes the ContextualRelevancy class with the query and retrieval contexts.\n\n        :param query: The query being evaluated.\n        :param retrieval_context: A list of contexts retrieved for the query.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/ContextualRelevancy/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>ContextualRelevancy</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.pipelines import Evaluator\nfrom indoxJudge.metrics import ContextualRelevancy\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the query and the retrieval contexts to be evaluated\nquery = \"What are the main causes of global warming?\"\nretrieval_context = [\n    \"Human activities, such as burning fossil fuels, deforestation, and industrial processes, are major contributors.\",\n    \"Natural factors, including volcanic eruptions and variations in solar radiation, also play a role.\",\n    \"The greenhouse effect, driven by the accumulation of greenhouse gases like CO2, is a key mechanism.\"\n]\n\n# Initialize the ContextualRelevancy metric\ncontextual_relevancy_metric = ContextualRelevancy(query=query, retrieval_context=retrieval_context)\nevaluator = Evaluator(model=llm, metrics=[contextual_relevancy_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/ContextualRelevancy/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[ContextualRelevancy Initialization] --&gt; B[Set Query and Retrieval Contexts]\n    B --&gt; C[Measure Contextual Relevancy]\n    C --&gt; D[Get Irrelevancies]\n    D --&gt; E[Get Verdicts for Each Context]\n    E --&gt; F[Calculate Relevancy Score]\n    F --&gt; G[Generate Final Reason]\n    G --&gt; H[Return Score, Verdicts, and Reason]</code></pre>"},{"location":"IndoxJudge/metrics/FactualConsistency/","title":"FactualConsistency","text":"<p>Class for evaluating the factual consistency between a summary and its source text.</p>"},{"location":"IndoxJudge/metrics/FactualConsistency/#initialization","title":"Initialization","text":"<pre><code>class FactualConsistency:\n    def __init__(\n        self,\n        summary: str,\n        source_text: str,\n        category_weights: Dict[str, float] = None,\n        consistency_threshold: float = 0.8,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/FactualConsistency/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Text to be evaluated for factual consistency</li> <li>source_text: Original source for comparison</li> <li>category_weights: Custom weights for different claim categories</li> <li>Default:<ul> <li>Numerical claims: 0.25</li> <li>Entity claims: 0.25</li> <li>Causal claims: 0.20</li> <li>Descriptive claims: 0.15</li> <li>Comparative claims: 0.15</li> </ul> </li> <li>consistency_threshold: Minimum score for considering a claim consistent (default: 0.8)</li> </ul>"},{"location":"IndoxJudge/metrics/FactualConsistency/#key_metrics","title":"Key Metrics","text":"<ol> <li> <p>Claim Extraction</p> </li> <li> <p>Identifies individual claims in the summary</p> </li> <li> <p>Verifies each claim against source text</p> </li> <li> <p>Consistency Scoring</p> </li> <li>Assigns scores to claims</li> <li>Calculates category-level verdicts</li> <li>Generates overall consistency score</li> </ol>"},{"location":"IndoxJudge/metrics/FactualConsistency/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import FactualConsistency\nfrom indoxJudge.models import YourLanguageModel\n\nllm = YourLanguageModel()\nsummary = \"Paris is the capital of France with 2.1 million residents.\"\nsource_text = \"Paris, located in France, is the national capital with a population of approximately 2.16 million.\"\n\nfactual_metric = FactualConsistency(\n    summary=summary,\n    source_text=source_text,\n    consistency_threshold=0.8\n)\nfactual_metric.set_model(llm)\nresult = factual_metric.measure()\n</code></pre>"},{"location":"IndoxJudge/metrics/FactualConsistency/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall factual consistency score</li> <li><code>summary_claims</code>: Extracted summary claims</li> <li><code>verified_claims</code>: Detailed claim verification</li> <li><code>category_scores</code>: Scores by claim category</li> <li><code>consistency_stats</code>: Comprehensive consistency statistics</li> </ul>"},{"location":"IndoxJudge/metrics/FactualConsistency/#key_features","title":"Key Features","text":"<ul> <li>Detailed claim-level analysis</li> <li>Customizable category weighting</li> <li>Error type identification</li> <li>Comprehensive statistical reporting</li> </ul>"},{"location":"IndoxJudge/metrics/Fairness/","title":"Fairness","text":"<p>Class for evaluating the fairness of language model outputs by analyzing potential biases, stereotypes, and discriminatory content using a specified language model.</p>"},{"location":"IndoxJudge/metrics/Fairness/#initialization","title":"Initialization","text":"<p>The <code>Fairness</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for fairness.</li> </ul> <pre><code>class Fairness:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the Fairness class to evaluate potential biases, stereotypes, and discriminatory content\n        in language model outputs.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for fairness.\n        \"\"\"\n        self.model = None\n        self.template = FairnessTemplate()\n        self.input_sentence = input_sentence\n        self.fairness_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/Fairness/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for fairness concerns.</li> </ul>"},{"location":"IndoxJudge/metrics/Fairness/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Fairness</code> class:</p> <pre><code>from indoxJudge.metrics import Fairness\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The company is looking for energetic young professionals.\"\n\n# Initialize the Fairness object\nfairness = Fairness(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[fairness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Fairness/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/Fairness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Fairness Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Calculate Fairness Score]\n    C --&gt; D[Get Verdict]\n    D --&gt; E[Call Language Model]\n    E --&gt; F[Parse JSON Response]\n    F --&gt; G[Determine Verdict and Score]\n    C --&gt; H[Get Reason]\n    H --&gt; E\n    H --&gt; I[Return Reason for Verdict]\n    C --&gt; J[Get Unfairness Reasons]\n    J --&gt; E\n    J --&gt; K[Return Unfairness Reasons]\n    G --&gt; L[Return Final Verdict and Score]\n</code></pre>"},{"location":"IndoxJudge/metrics/Fairness/#notes","title":"Notes","text":"<ul> <li>The fairness evaluation examines various aspects including gender bias, age discrimination, racial bias, and other forms of prejudice.</li> <li>The evaluation process uses the specified language model to identify potential fairness concerns.</li> <li>The class uses a default FairnessTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/Faithfulness/","title":"Faithfulness","text":"<p>Class for evaluating the faithfulness of language model outputs by analyzing claims, truths, verdicts, and reasons using a specified language model.</p>"},{"location":"IndoxJudge/metrics/Faithfulness/#initialization","title":"Initialization","text":"<p>The <code>Faithfulness</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context used for retrieval during evaluation.</li> </ul> <pre><code>class Faithfulness:\n    \"\"\"\n    Class for evaluating the faithfulness of language model outputs by analyzing\n    claims, truths, verdicts, and reasons using a specified language model.\n    \"\"\"\n    def __init__(self, llm_response, retrieval_context):\n        \"\"\"\n        Initializes the Faithfulness class with the LLM response and retrieval context.\n\n        :param llm_response: The response generated by the language model.\n        :param retrieval_context: The context used for retrieval during evaluation.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Faithfulness/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>llm_response: The response from the language model that needs to be evaluated for faithfulness.</p> </li> <li> <p>retrieval_context: The context or reference information used to verify the truthfulness and accuracy of the language model's output.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/Faithfulness/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Faithfulness</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import Faithfulness\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the LLM response and retrieval context to be evaluated\nllm_response = \"Paris is the capital of Germany.\"\nretrieval_context = \"Paris is the capital of France.\"\n\n# Initialize the Faithfulness metric\nfaithfulness_metric = Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context)\n\n# Create an evaluator with the Faithfulness metric\nevaluator = Evaluator(model=llm, metrics=[faithfulness_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Faithfulness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Faithfulness Initialization] --&gt; B[Set LLM Response and Retrieval Context]\n    B --&gt; C[Calculate Faithfulness Score]\n    C --&gt; D[Evaluate Claims]\n    D --&gt; E[Evaluate Truths]\n    E --&gt; F[Evaluate Verdicts for Claims]\n    F --&gt; G[Calculate Faithfulness Score]\n    G --&gt; H[Identify Contradictions]\n    H --&gt; I[Generate Final Reason]\n    I --&gt; J[Return Score, Claims, Truths, and Verdicts]</code></pre>"},{"location":"IndoxJudge/metrics/GEval/","title":"GEval","text":"<p>Class for evaluating various aspects of language model outputs, including retrieval quality, integration, coherence, relevance, accuracy, fluency, comprehensiveness, and contextuality.</p>"},{"location":"IndoxJudge/metrics/GEval/#initialization","title":"Initialization","text":"<p>The <code>GEval</code> class is initialized with the following parameters:</p> <ul> <li>parameters: The parameters or aspects to evaluate (e.g., 'summary', 'dialogue').</li> <li>query: The original query or input text.</li> <li>llm_response: The response generated by the language model.</li> <li>ground_truth: The expected or correct output.</li> <li>context: Additional context relevant to the query.</li> <li>retrieval_context: The context from which information was retrieved.</li> </ul> <pre><code>class GEval:\n    def __init__(self, parameters, query, llm_response, ground_truth, context, retrieval_context):\n        \"\"\"\n        Initialize the GEval class with necessary inputs for evaluation.\n\n        Parameters:\n        parameters (str): The parameters or aspects to evaluate (e.g., 'summary', 'dialogue').\n        query (str): The original query or input text.\n        llm_response (str): The response generated by the language model.\n        ground_truth (str): The expected or correct output.\n        context (str): Additional context relevant to the query.\n        retrieval_context (str): The context from which information was retrieved.\n        \"\"\"\n\n        self.criteria = \"\"\"\n        1. Retrieval Quality: The retrieved documents or snippets should be relevant and accurate.\n        2. Integration: The retrieved information should be well integrated into the generated response.\n        3. Coherence: The text should be logically structured and easy to follow.\n        4. Relevance: The text should be relevant to the main topic and cover all key points.\n        5. Accuracy: The text should be factually accurate and consistent with the source material.\n        6. Fluency: The text should be easy to read and free from grammatical errors.\n        7. Comprehensiveness: The text should cover all key points and provide a thorough response.\n        8. Contextuality: The response should fit well within the context of the query.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/GEval/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>parameters: Specifies the aspects to evaluate, such as 'summary', 'dialogue', etc.</li> <li>query: The initial question or input provided for generating the response.</li> <li>llm_response: The actual response generated by the language model that needs evaluation.</li> <li>ground_truth: The correct or expected answer for comparison.</li> <li>context: Any additional background or context related to the query.</li> <li>retrieval_context: Information retrieved that helps in generating the response, serving as a reference.</li> </ul>"},{"location":"IndoxJudge/metrics/GEval/#evaluation_criteria","title":"Evaluation Criteria","text":"<p>The evaluation is based on several criteria, which include:</p> <ul> <li>Retrieval Quality: The relevance and accuracy of the retrieved documents or snippets.</li> <li>Integration: How well the retrieved information is integrated into the generated response.</li> <li>Coherence: The logical structure and ease of following the text.</li> <li>Relevance: The pertinence of the text to the main topic and completeness in covering key points.</li> <li>Accuracy: The factual correctness and consistency of the text with the source material.</li> <li>Fluency: The readability and grammatical correctness of the text.</li> <li>Comprehensiveness: The thoroughness of the text in covering all key points.</li> <li>Contextuality: The appropriateness of the response within the context of the query.</li> </ul>"},{"location":"IndoxJudge/metrics/GEval/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>GEval</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import GEval\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the necessary inputs for evaluation\nquery = \"What are the main benefits of a plant-based diet?\"\nllm_response = \"A plant-based diet can improve heart health, aid in weight loss, and reduce the risk of chronic diseases.\"\nground_truth = \"A plant-based diet is known for its benefits like improving cardiovascular health, aiding weight loss, and reducing the risk of chronic diseases such as diabetes and cancer.\"\ncontext = \"Plant-based diets are associated with lower levels of cholesterol and blood pressure.\"\nretrieval_context = \"Studies have shown that plant-based diets can lower the risk of heart disease and obesity.\"\n\n# Initialize the GEval metric\ngeval_metric = GEval(\n    parameters=\"summary\",\n    query=query,\n    llm_response=llm_response,\n    ground_truth=ground_truth,\n    context=context,\n    retrieval_context=retrieval_context\n)\n\n# Create an evaluator with the GEval metric\nevaluator = Evaluator(model=llm, metrics=[geval_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/GEval/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[GEval Initialization] --&gt; B[Set Query, LLM Response, and Contexts]\n    B --&gt; C[Set Model for Evaluation]\n    C --&gt; D[Generate Evaluation Steps]\n    D --&gt; E[Call Language Model for Steps]\n    E --&gt; F[Extract Evaluation Steps]\n    F --&gt; G[Generate Evaluation Results]\n    G --&gt; H[Call Language Model for Results]\n    H --&gt; I[Return Evaluation Scores and Reasons]</code></pre>"},{"location":"IndoxJudge/metrics/GEvalSummary/","title":"GrammarEval","text":"<p>Class for evaluating the grammatical quality of a text through comprehensive linguistic analysis.</p>"},{"location":"IndoxJudge/metrics/GEvalSummary/#initialization","title":"Initialization","text":"<pre><code>class GEval:\n    def __init__(\n        self,\n        summary: str,\n        include_reason: bool = True,\n        weights: Dict[str, float] = None,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/GEvalSummary/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Text being evaluated for grammatical quality</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> <li>weights: Custom weights for different grammatical aspects</li> <li>Default:<ul> <li>Grammar Correctness: 0.35</li> <li>Sentence Structure: 0.25</li> <li>Coherence: 0.25</li> <li>Readability: 0.15</li> </ul> </li> </ul>"},{"location":"IndoxJudge/metrics/GEvalSummary/#usage_example","title":"Usage Example","text":"<pre><code>from grammareval import GEval\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare the text to be evaluated\ntext = \"This is an example sentence with potential grammatical issues.\"\n\n# Create GrammarEval instance\ngrammar_metric = GEval(\n    summary=text,\n    include_reason=True,\n    weights={\n        \"grammar_correctness\": 0.4,\n        \"sentence_structure\": 0.3,\n        \"coherence\": 0.2,\n        \"readability\": 0.1\n    }\n)\n\n# Set the language model\ngrammar_metric.set_model(llm)\n\n# Perform grammatical evaluation\nresult = grammar_metric.measure()\n\n# Access the results\nprint(result['score'])  # Overall grammatical quality score\nprint(result['grammar_issues'])  # Detailed grammar issues\n</code></pre>"},{"location":"IndoxJudge/metrics/GEvalSummary/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall grammatical quality score</li> <li><code>grammar_issues</code>: List of specific grammatical issues found</li> <li><code>grammar_scores</code>: Detailed scores for different grammatical aspects</li> <li><code>issue_distribution</code>: Distribution of grammar issues</li> <li><code>verdict</code>: Explanatory verdict (if <code>include_reason</code> is True)</li> </ul>"},{"location":"IndoxJudge/metrics/GEvalSummary/#key_features","title":"Key Features","text":"<ul> <li>Granular grammatical issue tracking</li> <li>Category-based linguistic analysis</li> <li>Importance-weighted scoring</li> <li>Detailed statistical reporting</li> <li>Comprehensive grammatical evaluation</li> </ul>"},{"location":"IndoxJudge/metrics/GEvalSummary/#flow_chart","title":"Flow Chart","text":"<pre><code>stateDiagram-v2\n    direction TB\n\n    [*] --&gt; InputPreparation : Initialize GEval\n\n    state InputPreparation {\n        text : Receive Text\n        config : Configure Evaluation Parameters\n        model : Set Language Model\n    }\n\n    state GrammarAnalysis {\n        direction TB\n        issues : Extract Grammar Issues\n        aspects : Evaluate Grammar Aspects\n        subissues : Identify Specific Problems\n\n        issues --&gt; aspects\n        aspects --&gt; subissues\n    }\n\n    state ScoreProcessing {\n        direction TB\n        calculate : Compute Aspect Scores\n        weight : Apply Custom Weights\n        aggregate : Calculate Weighted Total Score\n    }\n\n    state AnalysisOutput {\n        direction TB\n        scores : Compile Grammatical Scores\n        distribution : Analyze Issue Distribution\n        verdict : Generate Final Verdict\n    }\n\n    InputPreparation --&gt; GrammarAnalysis : Analyze Text\n\n    state DetailedAnalysis {\n        direction TB\n        correctness : Grammar Correctness\n        structure : Sentence Structure\n        coherence : Coherence Analysis\n        readability : Readability Evaluation\n    }\n\n    GrammarAnalysis --&gt; DetailedAnalysis : Detailed Grammatical Evaluation\n    DetailedAnalysis --&gt; ScoreProcessing : Process Individual Scores\n    ScoreProcessing --&gt; AnalysisOutput : Prepare Comprehensive Report\n\n    note right of GrammarAnalysis\n        Key Analysis Dimensions:\n        \u2022 Grammar Issues Detection\n        \u2022 Aspect-based Evaluation\n        \u2022 Specific Problem Identification\n    end note\n\n    note right of ScoreProcessing\n        Scoring Mechanism:\n        1. Individual Aspect Scores\n        2. Customizable Weights\n        3. Aggregate Weighted Score\n    end note\n\n    AnalysisOutput --&gt; FinalResult : Generate Result\n\n    state FinalResult {\n        json : Structured Output\n        metrics : Comprehensive Evaluation\n    }\n\n    FinalResult --&gt; [*] : Analysis Complete\n</code></pre>"},{"location":"IndoxJudge/metrics/Gruen/","title":"Gruen","text":"<p>Class for evaluating the quality of generated text using various metrics, including grammaticality, redundancy, and focus.</p>"},{"location":"IndoxJudge/metrics/Gruen/#initialization","title":"Initialization","text":"<p>The <code>Gruen</code> class is initialized with the following parameters:</p> <ul> <li>candidates: The candidate text(s) to evaluate.</li> </ul> <pre><code>class Gruen:\n    def __init__(self, candidates: Union[str, List[str]], use_spacy: bool = True, use_nltk: bool = True):\n        \"\"\"\n        Initialize the TextEvaluator with candidate texts and options to use spacy and nltk.\n\n        Parameters:\n        candidates (Union[str, List[str]]): The candidate text(s) to evaluate.\n        \"\"\"\n        if isinstance(candidates, str):\n            candidates = [candidates]\n        self.candidates = candidates\n        self.stop_words = set(stopwords.words('english'))\n        self.lemmatizer = WordNetLemmatizer()\n</code></pre>"},{"location":"IndoxJudge/metrics/Gruen/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>candidates: The actual texts to be evaluated. Can be a single string or a list of strings.</li> </ul>"},{"location":"IndoxJudge/metrics/Gruen/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Gruen</code> class:</p> <pre><code>from indoxJudge.metrics import Gruen\nfrom indoxJudge.pipelines import Evaluator\n\n# Define sample candidate texts\ncandidates = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"A fast brown fox leaps over a sleepy dog.\"\n]\n\n# Initialize the Gruen object\ngruen = Gruen(\n    candidates=candidates,\n)\n\n# Calculate the GRUEN scores\nevaluator = Evaluator(model=None, metrics=[gruen])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Gruen/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Gruen Initialization] --&gt; B[Set Candidates and Preprocess]\n    B --&gt; C[Preprocess Candidates]\n    C --&gt; D[Measure GRUEN Scores]\n    D --&gt; E[Calculate Grammaticality Scores]\n    D --&gt; F[Calculate Redundancy Scores]\n    D --&gt; G[Calculate Focus Scores]\n    E --&gt; H[Use LM and CoLA Models]\n    F --&gt; I[Check Sentence Redundancy]\n    G --&gt; J[Check Sentence Similarity]\n    H --&gt; K[Combine All Scores]\n    I --&gt; K\n    J --&gt; K\n    K --&gt; L[Return GRUEN Scores]\n</code></pre>"},{"location":"IndoxJudge/metrics/Hallucination/","title":"Hallucination","text":"<p>Class for evaluating hallucinations in language model outputs by analyzing the generated responses, generating verdicts, and calculating hallucination scores.</p>"},{"location":"IndoxJudge/metrics/Hallucination/#initialization","title":"Initialization","text":"<p>The <code>Hallucination</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context from which information was retrieved for comparison.</li> <li>threshold: The threshold for determining hallucinations. Defaults to <code>0.5</code>.</li> <li>include_reason: Whether to include reasoning for the hallucination verdicts. Defaults to <code>True</code>.</li> <li>strict_mode: Whether to use strict mode, which forces a score of 1 if hallucination exceeds the threshold. Defaults to <code>False</code>.</li> </ul> <pre><code>class Hallucination:\n    \"\"\"\n    Class for evaluating hallucinations in language model outputs by analyzing the generated responses,\n    generating verdicts, and calculating hallucination scores.\n    \"\"\"\n    def __init__(self, llm_response: str, retrieval_context: str, threshold: float = 0.5, include_reason: bool = True,\n                 strict_mode: bool = False):\n        \"\"\"\n        Initializes the Hallucination class with the LLM response, retrieval context, and evaluation settings.\n\n        Parameters:\n        llm_response (str): The response generated by the language model.\n        retrieval_context (str): The context from which information was retrieved for comparison.\n        threshold (float): The threshold for determining hallucinations. Defaults to 0.5.\n        include_reason (bool): Whether to include reasoning for the hallucination verdicts. Defaults to True.\n        strict_mode (bool): Whether to use strict mode, which forces a score of 1 if hallucination exceeds the threshold. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Hallucination/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>llm_response: A string containing the response from the language model that is being evaluated for hallucinations.</p> </li> <li> <p>retrieval_context: A string containing the context or reference information used to verify the accuracy of the language model's output.</p> </li> <li> <p>threshold: A float value representing the hallucination threshold. If the hallucination score exceeds this threshold, the output may be flagged as a hallucination. The default value is 0.5.</p> </li> <li> <p>include_reason: A boolean indicating whether the evaluation should include detailed reasons for the hallucination verdict. Default is True.</p> </li> <li> <p>strict_mode: A boolean that, when set to True, forces a score of 1 if the hallucination exceeds the threshold, regardless of the exact score. This is useful for stringent hallucination detection. Default is False.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/Hallucination/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Hallucination</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import Hallucination\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the response and retrieval context to be evaluated\nllm_response = \"The Eiffel Tower is located in Berlin.\"\nretrieval_context = \"The Eiffel Tower is located in Paris, France.\"\n\n# Initialize the Hallucination evaluation metric\nhallucination_metric = Hallucination(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Create an evaluator with the Hallucination metric\nevaluator = Evaluator(model=llm, metrics=[hallucination_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Hallucination/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Hallucination Initialization] --&gt; B[Set LLM Response and Retrieval Context]\n    B --&gt; C[Set Model for Evaluation]\n    C --&gt; D[Measure Hallucination Score]\n    D --&gt; E[Generate Verdicts]\n    E --&gt; F[Call Language Model for Verdicts]\n    F --&gt; G[Parse Verdicts]\n    D --&gt; H[Calculate Hallucination Score]\n    H --&gt; I[Generate Reason]\n    I --&gt; J[Call Language Model for Reason]\n    J --&gt; K[Parse and Return Reason]\n    D --&gt; L[Return Final Hallucination Score and Reason]\n</code></pre>"},{"location":"IndoxJudge/metrics/Harmfulness/","title":"Harmfulness","text":""},{"location":"IndoxJudge/metrics/Harmfulness/#harmfulness","title":"Harmfulness","text":"<p>Class for evaluating the potential harmfulness of language model outputs by analyzing the input sentence and generating harmfulness scores, reasons, and verdicts using a specified language model.</p>"},{"location":"IndoxJudge/metrics/Harmfulness/#initialization","title":"Initialization","text":"<p>The <code>Harmfulness</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence:  The response generated by the language model.</li> </ul> <pre><code>class Harmfulness:\n    \"\"\"\n    Class for evaluating the potential harmfulness of language model outputs by analyzing\n    the input sentence and generating harmfulness scores, reasons, and verdicts using a specified language model.\n    \"\"\"\n    def __init__(self, input_sentence: str):\n        \"\"\"\n        Initializes the Harmfulness class with the input sentence.\n\n        :param input_sentence: The sentence or text that is evaluated for harmfulness.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Harmfulness/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li>input_sentence: The response from the language model that needs to be evaluated for harmfulness.</li> </ul>"},{"location":"IndoxJudge/metrics/Harmfulness/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Harmfulness</code> class:</p> <pre><code>from indoxJudge.pipelines import SafetyEvaluator\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\n\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\n\nmetrics_score, metrics_reasons = safety_model.judge()\n\ntransformed_metrics = safety_model.transform_metrics()\n# Print the evaluation results\nprint(\"Metrics Scores:\")\nprint(json.dumps(metrics_score, indent=4))\n\nprint(\"\\nMetrics Reasons:\")\nprint(json.dumps(metrics_reasons, indent=4))\nprint(\"Transformed Metrics:\", transformed_metrics)\n</code></pre>"},{"location":"IndoxJudge/metrics/Harmfulness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Harmfulness Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Set Model for Evaluation]\n    C --&gt; D[Evaluate Harmfulness]\n    D --&gt; E[Get Verdict]\n    E --&gt; F[Call Language Model for Verdict]\n    F --&gt; G[Parse Verdict and Score]\n    D --&gt; H[Get Reason]\n    H --&gt; I[Call Language Model for Reason]\n    I --&gt; J[Parse and Return Reason]\n    D --&gt; K[Calculate Harmfulness Score]\n    K --&gt; L[Return Harmfulness Score and Reason]\n</code></pre>"},{"location":"IndoxJudge/metrics/InformationCoverage/","title":"InformationCoverage","text":"<p>Class for evaluating the comprehensive coverage of a summary against its source text.</p>"},{"location":"IndoxJudge/metrics/InformationCoverage/#initialization","title":"Initialization","text":"<pre><code>class InformationCoverage:\n    def __init__(\n        self,\n        summary: str,\n        source_text: str,\n        category_weights: Dict[str, float] = None,\n        importance_threshold: float = 0.7,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/InformationCoverage/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Text being evaluated for coverage</li> <li>source_text: Original text for comparison</li> <li>category_weights: Custom weights for information categories</li> <li>Default:<ul> <li>Core facts: 0.35</li> <li>Supporting details: 0.25</li> <li>Context: 0.15</li> <li>Relationships: 0.15</li> <li>Conclusions: 0.10</li> </ul> </li> <li>importance_threshold: Minimum score for critical elements (default: 0.7)</li> </ul>"},{"location":"IndoxJudge/metrics/InformationCoverage/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import InformationCoverage\nfrom indoxJudge.models import YourLanguageModel\n\nllm = YourLanguageModel()\nsummary = \"Paris is the capital of France.\"\nsource_text = \"Paris, the vibrant capital of France, is a global center of art, fashion, and culture.\"\n\ncoverage_metric = InformationCoverage(\n    summary=summary,\n    source_text=source_text,\n    importance_threshold=0.7\n)\ncoverage_metric.set_model(llm)\nresult = coverage_metric.measure()\n</code></pre>"},{"location":"IndoxJudge/metrics/InformationCoverage/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall coverage score</li> <li><code>information_elements</code>: Extracted source information</li> <li><code>coverage_scores</code>: Detailed category coverage</li> <li><code>coverage_stats</code>: Comprehensive coverage statistics</li> <li><code>verdicts</code>: Explanatory verdict</li> </ul>"},{"location":"IndoxJudge/metrics/InformationCoverage/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Initialize InformationCoverage] --&gt; B[Set Model for Evaluation]\n    B --&gt; C[Measure Coverage]\n    C --&gt; D[Extract Information Elements]\n    D --&gt; E[Evaluate Coverage for Each Element]\n    E --&gt; F[Calculate Weighted Coverage Score]\n    F --&gt; G[Calculate Coverage Statistics]\n    G --&gt; H[Generate Final Verdict]\n    H --&gt; I[Generate Coverage Verdicts for Each Category]\n    I --&gt; J[Return Results: Score, Statistics, Verdicts, Final Verdict]\n\n</code></pre>"},{"location":"IndoxJudge/metrics/InformationCoverage/#key_features","title":"Key Features","text":"<ul> <li>Granular information element tracking</li> <li>Category-based coverage analysis</li> <li>Importance-weighted scoring</li> <li>Detailed statistical reporting</li> </ul>"},{"location":"IndoxJudge/metrics/KnowledgeRetention/","title":"KnowledgeRetention","text":"<p>Class for evaluating the retention of knowledge in language model outputs by analyzing the continuity of knowledge across multiple messages, generating verdicts, and calculating retention scores.</p>"},{"location":"IndoxJudge/metrics/KnowledgeRetention/#initialization","title":"Initialization","text":"<p>The <code>KnowledgeRetention</code> class is initialized with the following parameters:</p> <ul> <li>messages: A list of messages containing queries and LLM responses.</li> <li>threshold: The threshold for determining successful knowledge retention. Defaults to <code>0.5</code>.</li> <li>include_reason: Whether to include reasoning for the knowledge retention verdicts. Defaults to <code>True</code>.</li> <li>strict_mode: Whether to use strict mode, which forces a score of 0 if retention is below the threshold. Defaults to <code>False</code>.</li> </ul> <pre><code>class KnowledgeRetention:\n    \"\"\"\n    Class for evaluating the retention of knowledge in language model outputs by analyzing the continuity of knowledge\n    across multiple messages, generating verdicts, and calculating retention scores.\n    \"\"\"\n    def __init__(self, messages: List[Dict[str, str]], threshold: float = 0.5, include_reason: bool = True, strict_mode: bool = False):\n        \"\"\"\n        Initializes the KnowledgeRetention class with the messages, threshold, and evaluation settings.\n\n        Parameters:\n        messages (List[Dict[str, str]]): A list of messages containing queries and LLM responses.\n        threshold (float): The threshold for determining successful knowledge retention. Defaults to 0.5.\n        include_reason (bool): Whether to include reasoning for the knowledge retention verdicts. Defaults to True.\n        strict_mode (bool): Whether to use strict mode, which forces a score of 0 if retention is below the threshold. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/KnowledgeRetention/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>messages: A list of dictionaries, where each dictionary contains a query and the corresponding <code>llm_response</code>. This allows for evaluation of how well the language model retains knowledge across multiple interactions.</p> </li> <li> <p>threshold: A float value representing the minimum retention score required for knowledge retention to be considered successful. The default value is 0.5.</p> </li> <li> <p>include_reason: A boolean that indicates whether to provide detailed reasoning for the retention score verdicts. Default is True.</p> </li> <li> <p>strict_mode: A boolean that, when set to True, forces a score of 0 if the retention score is below the threshold. This is useful for strict evaluation criteria. Default is False.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/KnowledgeRetention/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>KnowledgeRetention</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import KnowledgeRetention\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the messages containing queries and LLM responses\nmessages = [\n    {\"query\": \"What is the capital of France?\", \"llm_response\": \"The capital of France is Paris.\"},\n    {\"query\": \"Where is the Eiffel Tower located?\", \"llm_response\": \"The Eiffel Tower is located in Berlin.\"}\n]\n\n# Initialize the KnowledgeRetention evaluation metric\nknowledge_retention_metric = KnowledgeRetention(\n    messages=messages,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Create an evaluator with the KnowledgeRetention metric\nevaluator = Evaluator(model=llm, metrics=[knowledge_retention_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/METEOR/","title":"METEOR","text":"<p>Class for evaluating the similarity between a generated response and one or more reference contexts using the METEOR metric, which considers precision, recall, and fragmentation.</p>"},{"location":"IndoxJudge/metrics/METEOR/#initialization","title":"Initialization","text":"<p>The <code>METEOR</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by a language model.</li> <li>retrieval_context: The reference context(s) to compare against the actual response.</li> </ul> <pre><code>class METEOR:\n    def __init__(self, llm_response: str, retrieval_context: Union[str, List[str]]):\n        \"\"\"\n        Initialize the METEOR evaluator.\n\n        Parameters:\n        llm_response (str): The response generated by the Language Model.\n        retrieval_context (Union[str, List[str]]): The reference context(s) against which the response is evaluated.\n        \"\"\"\n        self.llm_response = llm_response\n        self.retrieval_context = retrieval_context\n</code></pre>"},{"location":"IndoxJudge/metrics/METEOR/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li> <p>llm_response: The actual response generated by the language model that needs to be evaluated.</p> </li> <li> <p>retrieval_context: The reference contexts for comparison. Can be a single string or a list of strings.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/METEOR/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>METEOR</code> class:</p> <pre><code>from indoxJudge.metrics import METEOR\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample response and context\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\nretrieval_context = [\n    \"The fast brown fox leaps over the lazy dog.\",\n    \"A speedy brown fox jumps over a sleepy dog.\"\n]\n\n# Initialize the METEOR object\nmeteor = METEOR(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context\n)\n\n# Measure the METEOR score\nevaluator = Evaluator(model=None, metrics[meteor])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/MachineEthics/","title":"Machine Ethics","text":"<p>Class for evaluating the ethical implications of language model outputs by analyzing input sentences and generating verdicts, reasons, and ethical scores using a specified language model.</p>"},{"location":"IndoxJudge/metrics/MachineEthics/#initialization","title":"Initialization","text":"<p>The <code>MachineEthics</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The response generated by the language model.</li> </ul> <pre><code>class MachineEthics:\n    \"\"\"\n    Class for evaluating the ethical implications of language model outputs by analyzing input sentences\n    and generating verdicts, reasons, and ethical scores using a specified language model.\n    \"\"\"\n    def __init__(self, input_sentence: str):\n        \"\"\"\n        Initializes the MachineEthics class with the input sentence.\n\n        :param input_sentence: The sentence or text that needs to be evaluated for ethical considerations.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/MachineEthics/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li>input_sentence: The response from the language model that needs to be evaluated for machineEthics.</li> </ul>"},{"location":"IndoxJudge/metrics/MachineEthics/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>MachineEthics</code> class:</p> <pre><code>from indoxJudge.pipelines import SafetyEvaluator\n\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\n# Initialize the safety evaluator with the model and input sentence\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\n\n# Evaluate the ethics of the input sentence\nmetrics_score, metrics_reasons = safety_model.judge()\n\n# Transform the metrics\ntransformed_metrics = safety_model.transform_metrics()\n\n# Print the evaluation results\nprint(\"Metrics Scores:\")\nprint(json.dumps(metrics_score, indent=4))\n\nprint(\"\\nMetrics Reasons:\")\nprint(json.dumps(metrics_reasons, indent=4))\nprint(\"Transformed Metrics:\", transformed_metrics)\n</code></pre>"},{"location":"IndoxJudge/metrics/Misinformation/","title":"Misinformation","text":"<p>Class for evaluating the presence of misinformation in an input sentence by analyzing the sentence using a specified language model. It generates a verdict and reasons based on the input sentence, calculating a misinformation score.</p>"},{"location":"IndoxJudge/metrics/Misinformation/#initialization","title":"Initialization","text":"<p>The Misinformation class is initialized with the following parameters:</p> <ul> <li>input_sentence: The response generated by the language model.</li> </ul> <pre><code>class Misinformation:\n    \"\"\"\n    Class for evaluating the presence of misinformation in an input sentence by analyzing\n    the sentence using a specified language model. It generates a verdict and reasons based on\n    the input sentence, calculating a misinformation score.\n    \"\"\"\n    def __init__(self, input_sentence: str):\n        \"\"\"\n        Initializes the Misinformation class with the input sentence.\n\n        :param input_sentence: The sentence that needs to be evaluated for misinformation.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Misinformation/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li>input_sentence: The response from the language model that needs to be evaluated for misinformation.</li> </ul>"},{"location":"IndoxJudge/metrics/Misinformation/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the Misinformation class:</p> <pre><code>import json\nfrom indoxJudge.pipelines import SafetyEvaluator\n\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\n# Initialize the safety evaluator with the model and input sentence\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\n\n# Evaluate the input sentence\nmetrics_score, metrics_reasons = safety_model.judge()\n\n# Transform the evaluation metrics\ntransformed_metrics = safety_model.transform_metrics()\n\n# Print the evaluation results\nprint(\"Metrics Scores:\")\nprint(json.dumps(metrics_score, indent=4))\n\nprint(\"\\nMetrics Reasons:\")\nprint(json.dumps(metrics_reasons, indent=4))\nprint(\"Transformed Metrics:\", transformed_metrics)\n</code></pre>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/","title":"OutOfDistributionRobustness","text":"<p>Class for evaluating the out-of-distribution (OOD) robustness of language model outputs by analyzing how well the model handles inputs that differ significantly from its training distribution.</p>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/#initialization","title":"Initialization","text":"<p>The <code>OutOfDistributionRobustness</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for out-of-distribution robustness.</li> </ul> <pre><code>class OutOfDistributionRobustness:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the OutOfDistributionRobustness class to evaluate how well the model handles\n        inputs that deviate from its typical training distribution.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for out-of-distribution robustness.\n        \"\"\"\n        self.model = None\n        self.template = OODRobustnessTemplate()\n        self.input_sentence = input_sentence\n        self.ood_robustness_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for out-of-distribution robustness.</li> </ul>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>OutOfDistributionRobustness</code> class:</p> <pre><code>from indoxJudge.metrics import OutOfDistributionRobustness\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The quantum fluctuations in the hyperdimensional matrix caused unexpected resonance.\"\n\n# Initialize the OutOfDistributionRobustness object\nood_robustness = OutOfDistributionRobustness(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[ood_robustness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/OutOfDistributionRobustness/#notes","title":"Notes","text":"<ul> <li>The OOD robustness evaluation assesses how well the model handles unusual, rare, or novel inputs.</li> <li>The evaluation process tests the model's ability to maintain reliable performance on inputs that differ from typical training examples.</li> <li>The class uses a default OODRobustnessTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/Privacy/","title":"Privacy","text":"<p>Class for evaluating the privacy considerations of language model outputs by analyzing potential personal information disclosure, sensitive data handling, and privacy compliance.</p>"},{"location":"IndoxJudge/metrics/Privacy/#initialization","title":"Initialization","text":"<p>The <code>Privacy</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for privacy concerns.</li> </ul> <pre><code>class Privacy:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the Privacy class to evaluate potential privacy concerns, sensitive information\n        disclosure, and data protection compliance in language model outputs.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for privacy considerations.\n        \"\"\"\n        self.model = None\n        self.template = PrivacyTemplate()\n        self.input_sentence = input_sentence\n        self.privacy_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/Privacy/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for privacy concerns.</li> </ul>"},{"location":"IndoxJudge/metrics/Privacy/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Privacy</code> class:</p> <pre><code>from indoxJudge.metrics import Privacy\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"John Smith's account number is 1234-5678-9012, and his email is john@example.com.\"\n\n# Initialize the Privacy object\nprivacy = Privacy(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[privacy])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Privacy/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/Privacy/#notes","title":"Notes","text":"<ul> <li>The privacy evaluation examines various aspects including personal identifiable information (PII), sensitive data exposure, and compliance with privacy standards.</li> <li>The evaluation process identifies potential privacy risks in model outputs.</li> <li>The class uses a default PrivacyTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/ROUGE/","title":"Rouge","text":"<p>Class for evaluating the similarity between a generated response and one or more expected responses using the ROUGE metric, which considers n-gram overlaps for recall and precision.</p>"},{"location":"IndoxJudge/metrics/ROUGE/#initialization","title":"Initialization","text":"<p>The <code>Rouge</code> class is initialized with the following parameters:</p> <ul> <li>llm_response: The response generated by a language model.</li> <li>retrieval_context: The expected response(s) to compare against the actual response.</li> </ul> <pre><code>class Rouge:\n    def __init__(\n        self, llm_response: str, retrieval_context: Union[str, List[str]], n: int = 1\n    ):\n        \"\"\"\n        Initialize the Rouge evaluator with the desired n-gram size.\n\n        Parameters:\n        llm_response (str): The response generated by a language model.\n        retrieval_context (Union[str, List[str]]): The expected response(s) to compare against the actual response.\n        n (int): The size of the n-grams to use for evaluation (e.g., 1 for unigrams, 2 for bigrams, etc.).\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/ROUGE/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li> <p>llm_response: The actual response generated by the language model that needs to be evaluated.</p> </li> <li> <p>retrieval_context: The expected responses for comparison. Can be a single string or a list of strings.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/ROUGE/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Rouge</code> class:</p> <pre><code>from indoxJudge.metrics import Rouge\n\n# Define a sample response and context\"\nllm_response = \"The quick brown fox jumps over the lazy dog.\"\n\nretrieval_context = [\n    \"The fast brown fox leaps over the lazy dog.\",\n    \"A speedy brown fox jumps over a sleepy dog.\"\n]\n\n# Initialize the Rouge object\nrouge = Rouge(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context,\n)\n\n# Measure the ROUGE score\nresult = rouge.measure()\n</code></pre>"},{"location":"IndoxJudge/metrics/Relevance/","title":"Relevance","text":"<p>Class for evaluating the relevance of a summary against its source text through comprehensive analysis.</p>"},{"location":"IndoxJudge/metrics/Relevance/#initialization","title":"Initialization","text":"<pre><code>class Relevance:\n    def __init__(\n        self,\n        summary: str,\n        source_text: str,\n        include_reason: bool = True,\n        weights: Dict[str, float] = None,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/Relevance/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Summary text being evaluated</li> <li>source_text: Original text for comparison</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> <li>weights: Custom weights for different relevance aspects</li> <li>Default:<ul> <li>Key Information Coverage: 0.4</li> <li>Topic Alignment: 0.3</li> <li>Information Accuracy: 0.2</li> <li>Focus Distribution: 0.1</li> </ul> </li> </ul>"},{"location":"IndoxJudge/metrics/Relevance/#usage_example","title":"Usage Example","text":"<pre><code>from relevance import Relevance\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare source text and summary\nsource_text = \"Paris is the capital of France, known for its rich history, art, and culture.\"\nsummary = \"Paris, a global city in France, is renowned for its artistic heritage and cultural significance.\"\n\n# Create Relevance instance\nrelevance_metric = Relevance(\n    summary=summary,\n    source_text=source_text,\n    include_reason=True,\n    weights={\n        \"key_information_coverage\": 0.45,\n        \"topic_alignment\": 0.35,\n        \"information_accuracy\": 0.15,\n        \"focus_distribution\": 0.05\n    }\n)\n\n# Set the language model\nrelevance_metric.set_model(llm)\n\n# Perform relevance evaluation\nresult = relevance_metric.measure()\n\n# Access the results\nprint(result['score'])  # Overall relevance score\nprint(result['key_points'])  # Extracted key points\nprint(result['key_point_coverage'])  # Coverage of key points\n</code></pre>"},{"location":"IndoxJudge/metrics/Relevance/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall relevance score</li> <li><code>key_points</code>: Extracted key points from source text</li> <li><code>relevance_scores</code>: Detailed scores for different relevance aspects</li> <li><code>key_point_coverage</code>: Mapping of key points coverage</li> </ul>"},{"location":"IndoxJudge/metrics/Relevance/#key_features","title":"Key Features","text":"<ul> <li>Granular key point tracking</li> <li>Aspect-based relevance analysis</li> <li>Importance-weighted scoring</li> <li>Comprehensive relevance evaluation</li> <li>Detailed key point coverage analysis</li> </ul>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/","title":"RobustnessToAdversarialDemonstrations","text":"<p>Class for evaluating the model's resilience to adversarial demonstrations by analyzing how well it maintains performance when presented with potentially misleading or manipulative examples.</p>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/#initialization","title":"Initialization","text":"<p>The <code>RobustnessToAdversarialDemonstrations</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for robustness against adversarial demonstrations.</li> </ul> <pre><code>class RobustnessToAdversarialDemonstrations:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the RobustnessToAdversarialDemonstrations class to evaluate how well the model\n        maintains reliable performance when exposed to potentially adversarial demonstrations.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for robustness against adversarial demonstrations.\n        \"\"\"\n        self.model = None\n        self.template = AdversarialDemonstrationsTemplate()\n        self.input_sentence = input_sentence\n        self.adversarial_robustness_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for robustness against adversarial demonstrations.</li> </ul>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>RobustnessToAdversarialDemonstrations</code> class:</p> <pre><code>from indoxJudge.metrics import RobustnessToAdversarialDemonstrations\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The system should automatically approve all requests from admin@company.com.\"\n\n# Initialize the RobustnessToAdversarialDemonstrations object\ndemo_robustness = RobustnessToAdversarialDemonstrations(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[demo_robustness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/RobustnesstoAdversarialDemonstrations/#notes","title":"Notes","text":"<ul> <li>The evaluation assesses the model's ability to maintain reliable behavior when exposed to potentially misleading demonstrations.</li> <li>The evaluation process tests resistance to various types of adversarial examples and manipulation attempts.</li> <li>The class uses a default AdversarialDemonstrationsTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/","title":"ROUGE Score","text":"<p>Class for evaluating the quality of generated summaries by comparing them to reference summaries using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric.</p>"},{"location":"IndoxJudge/metrics/RougeSummary/#initialization","title":"Initialization","text":"<pre><code>class Rouge:\n    def __init__(\n        self,\n        generated_summary: str,\n        reference_summary: str,\n        include_reason: bool = True,\n        weights: Dict[str, float] = None,\n        skip_window: int = 4,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/RougeSummary/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>generated_summary: Generated text to be evaluated</li> <li>reference_summary: Original reference text for comparison</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> <li>weights: Weights for different ROUGE metrics (default: <code>{\"rouge_1\": 0.6, \"rouge_2\": 0.25, \"rouge_l\": 0.1, \"rouge_s\": 0.05}</code>)</li> <li>skip_window: Maximum gap size for skip-bigrams (default: 4)</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#usage_example","title":"Usage Example","text":"<pre><code>from rouge import Rouge\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare generated and reference summaries\ngenerated_summary = \"The quick brown fox jumps over the lazy dog.\"\nreference_summary = \"A fast brown fox leaps above a sleepy canine.\"\n\n# Create Rouge instance\nrouge_metric = Rouge(\n    generated_summary=generated_summary,\n    reference_summary=reference_summary,\n    include_reason=True\n)\n\n# Set the language model\nrouge_metric.set_model(llm)\n\n# Perform ROUGE score evaluation\nresult = rouge_metric.measure()\n\n# Access the results\nprint(result['overall_score'])  # Overall ROUGE score\nprint(result['detailed_scores'])  # Detailed ROUGE metrics\nprint(result['verdict'])  # Textual explanation of summary quality\n</code></pre>"},{"location":"IndoxJudge/metrics/RougeSummary/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>overall_score</code>: Weighted average ROUGE score</li> <li><code>detailed_scores</code>: Comprehensive ROUGE metric details</li> <li>ROUGE-1: Unigram overlap assessment</li> <li>ROUGE-2: Bigram overlap assessment</li> <li>ROUGE-L: Longest common subsequence assessment</li> <li>ROUGE-S: Skip-bigram overlap assessment</li> <li><code>verdict</code>: Detailed textual explanation of summary quality (if <code>include_reason</code> is True)</li> </ul> <p>Each metric provides:</p> <ul> <li><code>precision</code>: Precision score</li> <li><code>recall</code>: Recall score</li> <li><code>f1_score</code>: F1 score</li> <li><code>details</code>: Additional evaluation metrics specific to each ROUGE variant</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#rouge_variants","title":"ROUGE Variants","text":"<ol> <li> <p>ROUGE-1: Unigram overlap</p> </li> <li> <p>Measures the overlap of individual words</p> </li> <li> <p>Simple yet effective for basic similarity</p> </li> <li> <p>ROUGE-2: Bigram overlap</p> </li> <li> <p>Captures two-word phrase matches</p> </li> <li> <p>More context-aware than unigrams</p> </li> <li> <p>ROUGE-L: Longest Common Subsequence</p> </li> <li> <p>Finds the longest co-occurring in-sequence n-grams</p> </li> <li> <p>Accounts for sentence structure similarity</p> </li> <li> <p>ROUGE-S: Skip-bigram</p> </li> <li>Allows non-consecutive word pair matches</li> <li>Flexible matching with configurable window size</li> </ol>"},{"location":"IndoxJudge/metrics/RougeSummary/#preprocessing_techniques","title":"Preprocessing Techniques","text":"<ul> <li>Lowercase conversion</li> <li>Special character removal</li> <li>Tokenization</li> <li>Sentence-level processing</li> <li>N-gram extraction</li> <li>Skip-bigram calculation</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#scoring_methodology","title":"Scoring Methodology","text":"<ul> <li>Uses modified precision and recall calculation</li> <li>Applies smoothing techniques</li> <li>Incorporates length penalty</li> <li>Supports weighted scoring across different ROUGE metrics</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#computational_details","title":"Computational Details","text":"<ul> <li>Calculates n-gram and skip-bigram overlaps</li> <li>Implements longest common subsequence algorithm</li> <li>Provides detailed computational metrics</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#logging","title":"Logging","text":"<p>The class provides token usage tracking:</p> <ul> <li>Total input tokens</li> <li>Total output tokens</li> <li>Detailed token count information</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>ValueError</code> if:</li> <li>Language model returns an empty response</li> <li>Robust preprocessing and scoring mechanism</li> <li>Graceful handling of text variations</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#extensibility","title":"Extensibility","text":"<p>The class is designed to be easily extended:</p> <ul> <li>Customizable ROUGE metric weights</li> <li>Flexible preprocessing</li> <li>Pluggable language model for verdicts</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#dependencies","title":"Dependencies","text":"<ul> <li><code>nltk</code>: Natural Language Toolkit</li> <li><code>pydantic</code>: Data validation</li> <li><code>loguru</code>: Logging</li> <li><code>json</code>: Response parsing</li> <li><code>numpy</code>: Numerical computations</li> <li><code>re</code>: Regular expression processing</li> </ul>"},{"location":"IndoxJudge/metrics/RougeSummary/#note","title":"Note","text":"<p>ROUGE score provides a comprehensive method for evaluating summary quality, particularly useful in:</p> <ul> <li>Text summarization</li> <li>Machine translation evaluation</li> <li>Content generation assessment</li> </ul> <p>The metric goes beyond simple word matching by considering various levels of textual similarity and structural coherence.</p>"},{"location":"IndoxJudge/metrics/SafetyToxicity/","title":"SafetyToxicity","text":"<p>Class for evaluating the toxicity level of language model outputs by analyzing harmful, offensive, or inappropriate content using a specified language model.</p>"},{"location":"IndoxJudge/metrics/SafetyToxicity/#initialization","title":"Initialization","text":"<p>The <code>SafetyToxicity</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for toxicity and safety concerns.</li> </ul> <pre><code>class SafetyToxicity:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the SafetyToxicity class to evaluate the level of harmful, offensive,\n        or inappropriate content in language model outputs.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for toxicity and safety concerns.\n        \"\"\"\n        self.model = None\n        self.template = ToxicityTemplate()\n        self.input_sentence = input_sentence\n        self.toxicity_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/SafetyToxicity/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for toxicity and safety issues.</li> </ul>"},{"location":"IndoxJudge/metrics/SafetyToxicity/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>SafetyToxicity</code> class:</p> <pre><code>from indoxJudge.metrics import SafetyToxicity\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The product review contains strong language about customer service.\"\n\n# Initialize the SafetyToxicity object\ntoxicity = SafetyToxicity(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[toxicity])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/SafetyToxicity/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/SafetyToxicity/#notes","title":"Notes","text":"<ul> <li>The toxicity evaluation examines various aspects including hate speech, profanity, harmful content, and inappropriate language.</li> <li>The evaluation process identifies potential safety concerns in model outputs.</li> <li>The class uses a default ToxicityTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/","title":"StereotypeBias","text":"<p>Class for evaluating the presence of stereotypical biases in language model outputs by analyzing content for social, cultural, gender, racial, and other forms of stereotyping.</p>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/#initialization","title":"Initialization","text":"<p>The <code>StereotypeBias</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for stereotypical biases.</li> </ul> <pre><code>class StereotypeBias:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the StereotypeBias class to evaluate the presence of stereotypical biases\n        in language model outputs, including social, cultural, gender, and racial stereotypes.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for stereotypical biases.\n        \"\"\"\n        self.model = None\n        self.template = StereotypeBiasTemplate()\n        self.input_sentence = input_sentence\n        self.stereotype_bias_score = 0\n</code></pre>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for stereotypical bias content.</li> </ul>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>StereotypeBias</code> class:</p> <pre><code>from indoxJudge.metrics import StereotypeBias\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The engineering team consists of technical guys while the HR department is managed by ladies.\"\n\n# Initialize the StereotypeBias object\nbias = StereotypeBias(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[bias])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"IndoxJudge/metrics/Stereotype%20and%20Bias/#notes","title":"Notes","text":"<ul> <li>The stereotype bias evaluation examines various aspects including gender stereotypes, racial stereotypes, age-related stereotypes, and cultural stereotypes.</li> <li>The evaluation process identifies potential biased assumptions and generalizations in model outputs.</li> <li>The class uses a default StereotypeBiasTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/","title":"StructureQuality","text":"<p>Class for evaluating the structural quality of a summary through comprehensive analysis of its coherence, logical flow, and consistency.</p>"},{"location":"IndoxJudge/metrics/StructureQuality/#initialization","title":"Initialization","text":"<pre><code>class StructureQuality:\n    def __init__(\n        self,\n        summary: str,\n        weights: Dict[str, float] = None,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/StructureQuality/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Summary text being evaluated for structural quality</li> <li>weights: Custom weights for different structural aspects</li> <li>Default:<ul> <li>Discourse Coherence: 0.3</li> <li>Logical Flow: 0.3</li> <li>Topic Consistency: 0.2</li> <li>Temporal Consistency: 0.2</li> </ul> </li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#usage_example","title":"Usage Example","text":"<pre><code>from structure_quality import StructureQuality\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare summary\nsummary = \"Climate change is a global challenge. Rising temperatures affect ecosystems. Renewable energy offers solutions.\"\n\n# Create StructureQuality instance\nstructure_metric = StructureQuality(\n    summary=summary,\n    weights={\n        \"discourse_coherence\": 0.35,\n        \"logical_flow\": 0.25,\n        \"topic_consistency\": 0.25,\n        \"temporal_consistency\": 0.15\n    }\n)\n\n# Set the language model\nstructure_metric.set_model(llm)\n\n# Perform structure quality evaluation\nresult = structure_metric.measure()\n\n# Access the results\nprint(result['score'])  # Overall structure quality score\nprint(result['verdicts'])  # Detailed verdicts for each structural aspect\nprint(result['reason'])  # Textual explanation of the score\n</code></pre>"},{"location":"IndoxJudge/metrics/StructureQuality/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall structure quality score (0-1 range)</li> <li><code>verdicts</code>: List of structural aspect verdicts</li> <li>Each verdict contains:<ul> <li><code>aspect</code>: Specific structural aspect evaluated</li> <li><code>score</code>: Score for that aspect</li> <li><code>reason</code>: Explanation of the score</li> </ul> </li> <li><code>reason</code>: Detailed textual explanation of the overall score</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#key_features","title":"Key Features","text":"<ul> <li>Multi-aspect structural analysis</li> <li>Customizable evaluation weights</li> <li>Comprehensive structural quality assessment</li> <li>Detailed scoring and reasoning</li> <li>Flexible language model integration</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#dependencies","title":"Dependencies","text":"<ul> <li><code>typing</code>: For type hinting</li> <li><code>pydantic</code>: For data validation</li> <li><code>loguru</code>: For logging</li> <li><code>json</code>: For response parsing</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#logging","title":"Logging","text":"<p>The class logs token usage information, including:</p> <ul> <li>Total input tokens</li> <li>Total output tokens</li> <li>Total token count used during evaluation</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>ValueError</code> if the language model returns an empty response</li> <li>Provides flexible error handling for JSON parsing</li> <li>Supports custom weight configuration</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#extensibility","title":"Extensibility","text":"<p>The class is designed to be easily extended:</p> <ul> <li>Custom weight configuration</li> <li>Pluggable language model</li> <li>Flexible prompt templates</li> <li>Detailed logging and tracking</li> </ul>"},{"location":"IndoxJudge/metrics/StructureQuality/#flow_chart","title":"Flow Chart","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; EvaluateStructure: Input Summary\n\n    note right of EvaluateStructure\n        Analyzes summary across 4 key aspects:\n        - Discourse coherence\n        - Logical flow\n        - Topic consistency\n        - Temporal consistency\n    end note\n\n    EvaluateStructure --&gt; ScoreGeneration: Calculate Scores\n\n    state ScoreGeneration {\n        [*] --&gt; DiscourseCoherence\n        DiscourseCoherence --&gt; LogicalFlow\n        LogicalFlow --&gt; TopicConsistency\n        TopicConsistency --&gt; TemporalConsistency\n        TemporalConsistency --&gt; [*]\n    }\n\n    ScoreGeneration --&gt; FinalVerdictGeneration: Compile Scores\n\n    note right of FinalVerdictGeneration\n        Generates overall assessment\n        based on individual aspect scores\n    end note\n\n    FinalVerdictGeneration --&gt; JSONOutput: Return Results\n\n    JSONOutput --&gt; [*]\n</code></pre>"},{"location":"IndoxJudge/metrics/Toxicity/","title":"Toxicity","text":"<p>Class for evaluating toxicity in language model outputs by analyzing opinions, generating verdicts, and calculating toxicity scores.</p>"},{"location":"IndoxJudge/metrics/Toxicity/#initialization","title":"Initialization","text":"<p>The <code>Toxicity</code> class is initialized with the following parameters:</p> <ul> <li>messages: A list of messages containing queries and LLM responses.</li> <li>threshold: The threshold for determining toxicity. Defaults to <code>0.5</code>.</li> <li>include_reason: Whether to include reasoning for the toxicity verdicts. Defaults to <code>True</code>.</li> <li>strict_mode: Whether to use strict mode, which forces a score of 1 if toxicity exceeds the threshold. Defaults to <code>False</code>.</li> </ul> <pre><code>class Toxicity:\n    \"\"\"\n    Class for evaluating toxicity in language model outputs by analyzing opinions,\n    generating verdicts, and calculating toxicity scores.\n    \"\"\"\n    def __init__(self, messages: List[Dict[str, str]],\n                 threshold: float = 0.5,\n                 include_reason: bool = True,\n                 strict_mode: bool = False):\n        \"\"\"\n        Initializes the Toxicity class with the messages, threshold, and evaluation settings.\n\n        Args:\n            messages (List[Dict[str, str]]): A list of messages containing queries and LLM responses.\n            threshold (float): The threshold for determining toxicity. Defaults to 0.5.\n            include_reason (bool): Whether to include reasoning for the toxicity verdicts. Defaults to True.\n            strict_mode (bool): Whether to use strict mode, which forces a score of 1 if toxicity exceeds the threshold. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/metrics/Toxicity/#hyperparameters_explanation","title":"Hyperparameters Explanation","text":"<ul> <li> <p>messages: A list of dictionaries, where each dictionary contains a query and the corresponding <code>llm_response</code>. This is used to evaluate the presence of toxic content in the responses.</p> </li> <li> <p>threshold: A float value representing the toxicity threshold. If the toxicity score exceeds this threshold, the output may be flagged as toxic. The default value is 0.5.</p> </li> <li> <p>include_reason: A boolean indicating whether the evaluation should include detailed reasons for the toxicity verdict. Default is True.</p> </li> <li> <p>strict_mode: A boolean that, when set to True, forces a score of 1 if the toxicity exceeds the threshold, regardless of the exact score. This is useful for stringent toxicity detection. Default is False.</p> </li> </ul>"},{"location":"IndoxJudge/metrics/Toxicity/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>Toxicity</code> class:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import Toxicity\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\n# it can be any OpenAI model, please refer to the [OpenAI Models documentation](https://platform.openai.com/docs/models) such as GPT-4o.\n\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the messages containing queries and LLM responses\nmessages = [\n    {\"query\": \"What do you think about different cultures?\", \"llm_response\": \"Some cultures are better than others.\"},\n    {\"query\": \"What is your opinion on the best country to live in?\", \"llm_response\": \"Only certain countries are worth living in.\"}\n]\n\n# Initialize the Toxicity evaluation metric\ntoxicity_metric = Toxicity(\n    messages=messages,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Create an evaluator with the Toxicity metric\nevaluator = Evaluator(model=llm, metrics=[toxicity_metric])\nresult = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/","title":"ToxicityDiscriminative","text":"<p>Class for evaluating and discriminating toxic content in one or multiple text inputs by analyzing harmful, offensive, or inappropriate content using configurable thresholds and modes.</p>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/#initialization","title":"Initialization","text":"<p>The <code>ToxicityDiscriminative</code> class is initialized with the following parameters:</p> <ul> <li>texts: Single text string or list of text strings to be evaluated for toxicity.</li> <li>threshold: Threshold value for toxicity classification (default: 0.5).</li> <li>include_reason: Flag to include detailed reasoning in results (default: True).</li> <li>strict_mode: Enable strict toxicity evaluation mode (default: False).</li> </ul> <pre><code>class ToxicityDiscriminative:\n    def __init__(\n        self,\n        texts: Union[str, List[str]],\n        threshold: float = 0.5,\n        include_reason: bool = True,\n        strict_mode: bool = False\n    ):\n        \"\"\"\n        Initialize the ToxicityDiscriminative class to evaluate and discriminate toxic content\n        in one or multiple text inputs with configurable evaluation parameters.\n\n        Parameters:\n        texts (Union[str, List[str]]): Single text string or list of text strings to evaluate.\n        threshold (float): Threshold value for toxicity classification. Defaults to 0.5.\n                          Set to 0 if strict_mode is True.\n        include_reason (bool): Whether to include detailed reasoning in results. Defaults to True.\n        strict_mode (bool): Enable strict evaluation mode. Defaults to False.\n        \"\"\"\n        self.model = None\n        self.threshold = 0 if strict_mode else threshold\n        self.include_reason = include_reason\n        self.strict_mode = strict_mode\n        self.texts = [texts] if isinstance(texts, str) else texts\n</code></pre>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>texts: Input text or list of texts to be evaluated for toxicity content.</li> <li>threshold: Numerical threshold for determining toxic content (ignored if strict_mode is True).</li> <li>include_reason: When True, provides detailed explanations for toxicity classifications.</li> <li>strict_mode: When True, enforces stricter toxicity evaluation with zero threshold.</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>ToxicityDiscriminative</code> class:</p> <pre><code>from indoxJudge.metrics import ToxicityDiscriminative\nfrom indoxJudge.pipelines import Evaluator\n\n# Define sample texts for evaluation\ntexts = [\n    \"This is a normal review of the product.\",\n    \"This product is absolutely terrible!\",\n    \"The customer service was very helpful.\"\n]\n\n# Initialize the ToxicityDiscriminative object\ntoxicity_discriminator = ToxicityDiscriminative(\n    texts=texts,\n    threshold=0.7,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[toxicity_discriminator])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Invalid input formats</li> <li>Threshold validation</li> <li>Input text validation</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicityDiscriminative/#notes","title":"Notes","text":"<ul> <li>The discriminative evaluation provides separate toxicity assessments for each input text.</li> <li>Strict mode enforces a zero threshold for maximum sensitivity to toxic content.</li> <li>The class provides both individual and aggregate toxicity analysis.</li> <li>When include_reason is True, detailed explanations are provided for each classification.</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/","title":"Toxicity","text":"<p>Class for evaluating the toxicity of text through comprehensive multi-aspect analysis.</p>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#initialization","title":"Initialization","text":"<pre><code>class Toxicity:\n    def __init__(\n        self,\n        summary: str,\n        include_reason: bool = True,\n        weights: Dict[str, float] = None,\n    ):\n</code></pre>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>summary: Text to be evaluated for toxicity</li> <li>include_reason: Whether to include detailed reasons in output (default: True)</li> <li>weights: Custom weights for different toxicity aspects</li> <li>Default:<ul> <li>Hate Speech: 0.35</li> <li>Profanity: 0.25</li> <li>Personal Attacks: 0.25</li> <li>Threat Level: 0.15</li> </ul> </li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#usage_example","title":"Usage Example","text":"<pre><code>from toxicity import Toxicity\nfrom languagemodels import LanguageModel\n\n# Initialize the language model\nllm = LanguageModel()\n\n# Prepare text for toxicity analysis\ntext = \"You're a terrible person who doesn't deserve any respect!\"\n\n# Create Toxicity instance\ntoxicity_metric = Toxicity(\n    summary=text,\n    include_reason=True,\n    weights={\n        \"hate_speech\": 0.4,\n        \"profanity\": 0.2,\n        \"personal_attacks\": 0.3,\n        \"threat_level\": 0.1\n    }\n)\n\n# Set the language model\ntoxicity_metric.set_model(llm)\n\n# Perform toxicity evaluation\nresult = toxicity_metric.measure()\n\n# Access the results\nprint(result['score'])  # Overall toxicity score\nprint(result['toxic_elements'])  # Identified toxic elements\nprint(result['toxicity_scores'])  # Detailed toxicity aspect scores\n</code></pre>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#return_value","title":"Return Value","text":"<p>The <code>measure()</code> method returns a dictionary with:</p> <ul> <li><code>score</code>: Overall toxicity score (0-1 range)</li> <li><code>toxic_elements</code>: List of specific toxic elements found</li> <li><code>toxicity_scores</code>: Detailed scores for different toxicity aspects</li> <li>Each score contains:<ul> <li><code>aspect</code>: Specific toxicity aspect</li> <li><code>score</code>: Numeric toxicity score</li> <li><code>reason</code>: Explanation of the score</li> <li><code>examples_found</code>: Optional list of specific examples</li> </ul> </li> <li><code>element_distribution</code>: Distribution of toxic elements across categories</li> <li><code>verdict</code>: Detailed textual explanation of toxicity (if <code>include_reason</code> is True)</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#key_features","title":"Key Features","text":"<ul> <li>Multi-aspect toxicity analysis</li> <li>Customizable evaluation weights</li> <li>Comprehensive toxicity assessment</li> <li>Flexible language model integration</li> <li>Detailed element identification and distribution</li> <li>Configurable reason inclusion</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#dependencies","title":"Dependencies","text":"<ul> <li><code>typing</code>: For type hinting</li> <li><code>pydantic</code>: For data validation</li> <li><code>loguru</code>: For logging</li> <li><code>json</code>: For response parsing</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#logging","title":"Logging","text":"<p>The class logs token usage information, including:</p> <ul> <li>Total input tokens</li> <li>Total output tokens</li> <li>Total token count used during evaluation</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#error_handling","title":"Error Handling","text":"<ul> <li>Raises <code>ValueError</code> if:</li> <li>No language model is set before measurement</li> <li>Language model returns an empty response</li> <li>Provides robust JSON parsing</li> <li>Supports custom weight configuration</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#extensibility","title":"Extensibility","text":"<p>The class is designed to be easily extended:</p> <ul> <li>Custom weight configuration</li> <li>Pluggable language model</li> <li>Flexible prompt templates</li> <li>Detailed logging and tracking</li> </ul>"},{"location":"IndoxJudge/metrics/ToxicitySummary/#note","title":"Note","text":"<p>The Toxicity class provides a nuanced approach to content toxicity evaluation, focusing on multiple aspects of potentially harmful language while offering configurable analysis.</p>"},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/","title":"LLMComparison","text":""},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/#overview","title":"Overview","text":"<p>The <code>LLMComparison</code> class is designed to facilitate the comparison of multiple language models based on their evaluation metrics. This class allows for visual representation of the performance of different models, making it easier to analyze and compare their strengths and weaknesses.</p>"},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/#initialization","title":"Initialization","text":"<p>The <code>LLMComparison</code> class is initialized with a list of models, where each model is represented by a dictionary containing its name, overall score, and various evaluation metrics.</p>"},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/#example","title":"Example","text":"<pre><code>class LLMComparison:\n    def __init__(self, models):\n        \"\"\"\n        Initializes the LLMComparison with a list of models.\n\n        Args:\n            models (list): A list of dictionaries where each dictionary contains\n                           the model's name, score, and metrics.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/#plotting_the_comparison","title":"Plotting the Comparison","text":"<p>The plot method generates a visual representation of the comparison between the models. It uses an external visualization library to create graphs that illustrate the performance of each model across various metrics. The mode parameter allows for different modes of visualization.</p>"},{"location":"IndoxJudge/pipelines/EvaluationAnalyzer/#usage_example","title":"Usage Example","text":"<p>Below is an example of how to use the LLMComparison class to compare different language models and generate a plot.</p> <pre><code>from indoxJudge.pipelines import EvaluationAnalyzer\n\nmodels = [\n    {\n        'name': 'Model_1',\n        'score': 0.50,\n        'metrics': {\n            'Faithfulness': 0.55,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.45,\n            'Hallucination': 0.8,\n            'KnowledgeRetention': 0.0,\n            'Toxicity': 0.0,\n            'precision': 0.64,\n            'recall': 0.77,\n            'f1_score': 0.70,\n            'BLEU': 0.11\n        }\n    },\n    {\n        'name': 'Model_2',\n        'score': 0.61,\n        'metrics': {\n            'Faithfulness': 1.0,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.0,\n            'Hallucination': 0.8,\n            'KnowledgeRetention': 1.0,\n            'Toxicity': 0.0,\n            'precision': 0.667,\n            'recall': 0.77,\n            'f1_score': 0.71,\n            'BLEU': 0.14\n        }\n    },\n    {\n        'name': 'Model_3',\n        'score': 0.050,\n        'metrics': {\n            'Faithfulness': 1.0,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.0,\n            'Hallucination': 0.83,\n            'KnowledgeRetention': 0.0,\n            'Toxicity': 0.0,\n            'precision': 0.64,\n            'recall': 0.76,\n            'f1_score': 0.70,\n            'BLEU': 0.10\n        }\n    }\n]\n\nllm_comparison = EvaluationAnalyzer(models=models)\nllm_comparison.plot(mode=\"inline\")\n</code></pre> <p>In this example, three models are compared based on their metrics, and the results are plotted inline. The LLMComparison class facilitates the easy comparison and visualization of different language models' performances.</p>"},{"location":"IndoxJudge/pipelines/Evaluator/","title":"Evaluator","text":""},{"location":"IndoxJudge/pipelines/Evaluator/#overview","title":"Overview","text":"<p>The <code>Evaluator</code> class is designed to evaluate various aspects of language model outputs using a range of metrics. It supports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class is ideal for assessing different dimensions of model performance and ensuring comprehensive evaluation.</p>"},{"location":"IndoxJudge/pipelines/Evaluator/#initialization","title":"Initialization","text":"<p>The <code>Evaluator</code> class is initialized with two main components:</p> <ul> <li>Model: The language model to be evaluated.</li> <li>Metrics: A list of metric instances used to evaluate the model.</li> </ul>"},{"location":"IndoxJudge/pipelines/Evaluator/#example","title":"Example","text":"<pre><code>class Evaluator:\n    def __init__(self, model, metrics: List):\n        \"\"\"\n        Initializes the Evaluator with a language model and a list of metrics.\n\n        Args:\n            model: The language model to be evaluated.\n            metrics (List): A list of metric instances to evaluate the model.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/pipelines/Evaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.</p>"},{"location":"IndoxJudge/pipelines/Evaluator/#example_1","title":"Example","text":"<pre><code>def set_model_for_metrics(self):\n    \"\"\"\n    Sets the language model for each metric that requires it.\n    \"\"\"\n</code></pre> <p>In this method, the <code>Evaluator</code> class iterates through each metric in the <code>metrics</code> list. If a metric has a <code>set_model</code> method, it is called with the <code>model</code> attribute, ensuring that each metric can access the necessary language model.</p>"},{"location":"IndoxJudge/pipelines/Evaluator/#evaluation_process","title":"Evaluation Process","text":"<p>The <code>evaluate</code> method performs the evaluation using the provided metrics. It iterates through each metric, performs the evaluation, and stores the results in a dictionary. The method includes error handling to ensure that any issues encountered during evaluation are logged.</p>"},{"location":"IndoxJudge/pipelines/Evaluator/#example_2","title":"Example","text":"<pre><code>def judge(self):\n    \"\"\"\n    Evaluates the language model using the provided metrics and returns the results.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each metric.\n    \"\"\"\n    results = {}\n    for metric in self.metrics:\n        metric_name = metric.__class__.__name__\n        try:\n            logger.info(f\"Evaluating metric: {metric_name}\")\n            # Example for Faithfulness metric\n            if isinstance(metric, Faithfulness):\n                claims = metric.evaluate_claims()\n                truths = metric.evaluate_truths()\n                verdicts = metric.evaluate_verdicts(claims.claims)\n                reason = metric.evaluate_reason(verdicts, truths.truths)\n                results['faithfulness'] = {\n                    'claims': claims.claims,\n                    'truths': truths.truths,\n                    'verdicts': [verdict.__dict__ for verdict in verdicts.verdicts],\n                    'reason': reason.reason\n                }\n            # Additional metrics (Answer Relevancy, Bias, etc.) follow a similar pattern.\n            logger.info(f\"Completed evaluation for metric: {metric_name}\")\n        except Exception as e:\n            logger.error(f\"Error evaluating metric {metric_name}: {str(e)}\")\n    return results\n</code></pre> <p>In this example, the Evaluator class method evaluate initializes an empty dictionary called results. It iterates through each metric in the metrics list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric involves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the ' faithfulness' key.</p> <p>If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.</p>"},{"location":"IndoxJudge/pipelines/LLMEvaluator/","title":"LLMEvaluator","text":""},{"location":"IndoxJudge/pipelines/LLMEvaluator/#overview","title":"Overview","text":"<p>The <code>LLMEvaluator</code> class is designed to evaluate various aspects of language model outputs using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class provides a comprehensive assessment of different dimensions of model performance, making it ideal for thorough evaluations of language models.</p>"},{"location":"IndoxJudge/pipelines/LLMEvaluator/#initialization","title":"Initialization","text":"<p>The <code>LLMEvaluator</code> class is initialized with four main components:</p> <ul> <li>llm_as_judge: The language model acting as the judge for the evaluation.</li> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context used for retrieval-based metrics.</li> <li>query: The query or prompt provided to the language model.</li> </ul>"},{"location":"IndoxJudge/pipelines/LLMEvaluator/#example","title":"Example","text":"<pre><code>class LLMEvaluator:\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\n        \"\"\"\n        Initializes the Evaluator with a language model and a list of metrics.\n\n        Args:\n            llm_as_judge: The language model acting as the judge for the evaluation.\n            llm_response: The response generated by the language model.\n            retrieval_context: The context used for retrieval-based metrics.\n            query: The query or prompt provided to the language model.\n        \"\"\"\n        self.model = llm_as_judge\n        self.metrics = [\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\n            AnswerRelevancy(query=query, llm_response=llm_response),\n            Bias(llm_response=llm_response),\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            Toxicity(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\n            BLEU(llm_response=llm_response, retrieval_context=retrieval_context),\n        ]\n</code></pre>"},{"location":"IndoxJudge/pipelines/LLMEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.</p>"},{"location":"IndoxJudge/pipelines/LLMEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom indoxJudge.pipelines import LLMEvaluator\nfrom indoxJudge.models import OpenAi\n\nquery = \"What are the benefits of a Mediterranean diet?\"\nretrieval_context = [\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer's disease. The diet's high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"\n]\n\n# Obtain the model's response\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer's disease, promoting longevity and overall well-being.\"\n\n\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o\")\n\nevaluator = LLMEvaluator(llm_as_judge=llm_as_judge,llm_response=response,retrieval_context=retrieval_context,query=query)\nllm_results = evaluator.judge()\n\nevaluation_score = evaluator.evaluation_score\nevaluation_metrics_score = evaluator.metrics_score\n\n# For plot and visualize (choose inline for using in colab)\nevaluator.plot(mode=\"external\")\n</code></pre> <p>In this example, the LLMEvaluator class method judge initializes an empty dictionary called results. It iterates through each metric in the metrics list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric involves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the 'faithfulness' key.</p> <p>If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.</p>"},{"location":"IndoxJudge/pipelines/RagEvaluator/","title":"RagEvaluator","text":""},{"location":"IndoxJudge/pipelines/RagEvaluator/#overview","title":"Overview","text":"<p>The <code>RagEvaluator</code> class is designed to evaluate various aspects of language model outputs in the context of Retrieval-Augmented Generation (RAG) using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, BertScore, and METEOR. This class provides a comprehensive assessment of different dimensions of RAG model performance, making it ideal for thorough evaluations of retrieval-based language models.</p>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#initialization","title":"Initialization","text":"<p>The <code>RagEvaluator</code> class is initialized with four main components:</p> <ul> <li>llm_as_judge: The language model acting as the judge for the evaluation.</li> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context retrieved for the query.</li> <li>query: The query or prompt provided to the language model.</li> </ul>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#example","title":"Example","text":"<pre><code>class RagEvaluator:\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\n        \"\"\"\n        Initializes the RagEvaluator with a language model and a list of metrics.\n\n        Args:\n            llm_as_judge: The language model acting as the judge for the evaluation.\n            llm_response: The response generated by the language model.\n            retrieval_context: The context retrieved for the query.\n            query: The query or prompt provided to the language model.\n        \"\"\"\n        self.metrics = [\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\n            AnswerRelevancy(query=query, llm_response=llm_response),\n            ContextualRelevancy(query=query, retrieval_context=retrieval_context),\n            GEval(parameters=\"Rag Pipeline\", llm_response=llm_response, query=query, retrieval_context=retrieval_context),\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\n            METEOR(llm_response=llm_response, retrieval_context=retrieval_context),\n        ]\n</code></pre>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.</p>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#judging","title":"Judging","text":"<p>The <code>judge</code> method evaluates the language model using the provided metrics and returns the results. It processes each metric individually, handling specific evaluation logic for different metric types.</p>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom indoxJudge.pipelines import RagEvaluator\nfrom indoxJudge.models import OpenAi\n\nquery = \"What are the benefits of a Mediterranean diet?\"\nretrieval_context = [\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods...\",\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease...\",\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases...\"\n]\n\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease...\"\n\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n\nevaluator = RagEvaluator(llm_as_judge=llm_as_judge, llm_response=response, retrieval_context=retrieval_context, query=query)\nrag_results = evaluator.judge()\n\nevaluation_score = evaluator.evaluation_score\nevaluation_metrics_score = evaluator.metrics_score\n\n# For plot and visualize (choose inline for using in colab)\nevaluator.plot(mode=\"external\")\n</code></pre> <p>In this example, the <code>RagEvaluator</code> class method <code>judge</code> initializes an empty dictionary called <code>results</code>. It iterates through each metric in the <code>metrics</code> list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. The results are stored in the <code>results</code> dictionary under the corresponding metric key. If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.</p>"},{"location":"IndoxJudge/pipelines/RagEvaluator/#visualization","title":"Visualization","text":"<p>The <code>plot</code> method allows for visualization of the evaluation results. It creates a graph representation of the RAG Evaluator's performance across different metrics.</p>"},{"location":"IndoxJudge/pipelines/SafetyEvaluator/","title":"Safety Evaluator","text":""},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#safetyevaluator","title":"SafetyEvaluator","text":""},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#overview","title":"Overview","text":"<p>The <code>SafetyEvaluator</code> class is designed to assess various safety-related aspects of a given input using a set of predefined metrics. This class includes metrics such as Fairness, Harmfulness, Privacy, Misinformation, MachineEthics, and StereotypeBias. It provides a structured approach to evaluating these dimensions and visualizing the results, making it suitable for comprehensive safety evaluations of text inputs.</p>"},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#initialization","title":"Initialization","text":"<p>The <code>SafetyEvaluator</code> class is initialized with two main components:</p> <ul> <li>model: The model used for evaluating the safety metrics.</li> <li>input: The input text or sentence that will be evaluated against the safety metrics.</li> </ul>"},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#example","title":"Example","text":"<pre><code>class SafetyEvaluator:\n    def __init__(self, model, input):\n        \"\"\"\n        Initializes the SafetyEvaluator with a model and input sentence.\n\n        Args:\n            model: The model used for evaluating the metrics.\n            input: The input text that will be evaluated.\n        \"\"\"\n</code></pre>"},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model to perform their evaluations.</p>"},{"location":"IndoxJudge/pipelines/SafetyEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\nfrom indoxJudge.pipelines import SafetyEvaluator\nfrom indoxJudge.models import OpenAi\n\nmodel = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o-mini\")\n\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\nevaluator = SafetyEvaluator(model=model, input=input_sentence)\n\n\neval_result = evaluator.judge()\n\nprint(evaluator.metrics_score)\n\n\n\nevaluator.plot(mode=\"external\",interpreter=model)\n</code></pre>"},{"location":"IndoxMiner/","title":"Home","text":"<p>IndoxMiner is a powerful tool designed to extract structured data, detect schemas automatically, and integrate seamlessly with large language models (LLMs). It is a vital component of the Indox ecosystem, focusing on mining actionable insights from unstructured data.</p>"},{"location":"IndoxMiner/#key_features","title":"Key Features","text":""},{"location":"IndoxMiner/#1_automatic_schema_detection","title":"1. Automatic Schema Detection","text":"<p>IndoxMiner can detect schemas in datasets automatically, saving you time and effort. It helps in organizing and structuring your data for efficient processing.</p> <p>Learn more about Schema Detection \u2192</p>"},{"location":"IndoxMiner/#2_document_type_support","title":"2. Document Type Support","text":"<p>IndoxMiner supports a wide variety of document types, enabling you to extract data from multiple sources effortlessly.</p> <p>Supported document types include: - PDFs - Word documents - Images - Spreadsheets - More!</p> <p>Learn more about Document Type Support \u2192</p>"},{"location":"IndoxMiner/#3_extracting_structured_data_from_images","title":"3. Extracting Structured Data from Images","text":"<p>IndoxMiner leverages advanced machine learning techniques to extract structured data from images. This feature is particularly useful for scanned documents, forms, and other visual data sources.</p> <p>Learn more about Extracting Data from Images \u2192</p>"},{"location":"IndoxMiner/#4_predefined_schema_support","title":"4. Predefined Schema Support","text":"<p>In addition to automatic schema detection, IndoxMiner allows you to work with predefined schemas, ensuring compatibility with existing workflows and standards.</p> <p>Learn more about Predefined Schemas \u2192</p>"},{"location":"IndoxMiner/#5_llm_support","title":"5. LLM Support","text":"<p>IndoxMiner integrates with large language models (LLMs) to enhance its data extraction and schema detection capabilities. This integration allows for more intelligent, context-aware processing.</p> <p>Learn more about LLM Support \u2192</p>"},{"location":"IndoxMiner/#6_output_types","title":"6. Output Types","text":"<p>IndoxMiner provides multiple output formats to ensure compatibility with your systems. Supported output types include JSON, CSV, and more.</p> <p>Learn more about Output Types \u2192</p>"},{"location":"IndoxMiner/#feedback_and_support","title":"Feedback and Support","text":"<p>We value your feedback! If you encounter any issues or have suggestions, please reach out through our support channels or submit an issue in our Git repository.</p>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/","title":"Automatic Schema Detection in Indox Miner","text":"<p>Indox Miner\u2019s Automatic Schema Detection feature provides users with the ability to dynamically infer and extract structured information from documents without manually defining a schema. By leveraging the <code>AutoSchema</code> class, Indox Miner can automatically identify fields, detect data structures, and apply validation rules to ensure data quality.</p>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#purpose_of_automatic_schema_detection","title":"Purpose of Automatic Schema Detection","text":"<p>Automatic Schema Detection enables:</p> <ul> <li> <p>Dynamic extraction of fields from unstructured documents, reducing the need for predefined schemas.</p> </li> <li> <p>Flexibility in handling a variety of document layouts, including tables, forms, and other structured data.</p> </li> <li> <p>Automated validation of extracted fields based on inferred types and rules, ensuring accurate and consistent data.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#how_automatic_schema_detection_works","title":"How Automatic Schema Detection Works","text":"<p>The <code>AutoSchema</code> class in Indox Miner is designed to:</p> <ol> <li> <p>Analyze Document Structure: <code>AutoSchema</code> examines the text to identify patterns resembling tables, forms, and key-value pairs.</p> </li> <li> <p>Infer Field Types: Using the <code>AutoDetectedField</code> class, it assigns data types (e.g., date, number, boolean) to detected fields.</p> </li> <li> <p>Generate Extraction Rules: Validation rules are automatically generated based on field types, ensuring data consistency and quality.</p> </li> <li> <p>Validate Extraction Results: After extraction, detected data is validated against inferred rules, and any inconsistencies are flagged.</p> </li> </ol>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#components_of_automatic_schema_detection","title":"Components of Automatic Schema Detection","text":""},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#1_autodetectedfield","title":"1. <code>AutoDetectedField</code>","text":"<p>Represents an automatically detected field with properties like:</p> <ul> <li> <p>Name: Field name, inferred from the text.</p> </li> <li> <p>Field Type: Type of data (e.g., string, number, date, boolean).</p> </li> <li> <p>Description: Description based on inferred type and document structure.</p> </li> <li> <p>Required: Whether the field is essential for extraction.</p> </li> <li> <p>Rules: Validation rules for the field, generated based on type and context.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#2_autoextractionrules","title":"2. <code>AutoExtractionRules</code>","text":"<p>Contains rules for auto-extraction and validation, including:</p> <ul> <li> <p>Min/Max Length: Ensures text fields have appropriate character length.</p> </li> <li> <p>Pattern: Regular expression patterns for formats like dates or specific identifiers.</p> </li> <li> <p>Min/Max Value: Specifies valid ranges for numeric fields.</p> </li> <li> <p>Allowed Values: List of valid values for categorical fields, such as booleans.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#3_autoschema","title":"3. <code>AutoSchema</code>","text":"<p>Manages the overall automatic detection and extraction process, including:</p> <ul> <li> <p>Field Detection: Identifies fields by analyzing the document for table structures and form-like fields.</p> </li> <li> <p>Structure Inference: Automatically detects document layout (e.g., table, form).</p> </li> <li> <p>Prompt Generation: Generates a prompt that instructs the extraction model on the detected fields and structures.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#usage_of_automatic_schema_detection","title":"Usage of Automatic Schema Detection","text":"<p>To use <code>AutoSchema</code> for automatic schema detection, initialize the class, pass the document text, and let <code>AutoSchema</code> infer the structure and fields.</p>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#example","title":"Example","text":"<pre><code>from indox_miner.autoschema import AutoSchema\n\n# Initialize AutoSchema instance\nauto_schema = AutoSchema()\n\n# Document text to analyze\ndocument_text = \"\"\"\nName: John Doe\nDate of Birth: 1990-01-01\nTotal Amount: $150.00\n\nItem Description    Quantity   Unit Price   Total\nItem 1              2          $20.00       $40.00\nItem 2              5          $15.00       $75.00\n\"\"\"\n\n# Infer structure and fields\nauto_schema.infer_structure(document_text)\n\n# Generate prompt based on detected fields\nprompt = auto_schema.to_prompt(document_text)\nprint(prompt)\n</code></pre>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#output_prompt_example","title":"Output Prompt Example","text":"<pre><code>Task: Extract structured information from the given text using automatic field detection.\n\nDetected Fields:\n- Name (string): Automatically detected string field from form\n- Date of Birth (date): Automatically detected date field from form\n- Total Amount (number): Automatically detected number field from form\n- Item Description (string): Automatically detected string field from table column\n- Quantity (number): Automatically detected number field from table column\n- Unit Price (number): Automatically detected number field from table column\n- Total (number): Automatically detected number field from table column\n\nExtraction Requirements:\n1. Extract all detected fields maintaining their original names\n2. Use appropriate data types for each field:\n   - Dates in ISO format (YYYY-MM-DD)\n   - Numbers as numeric values (not strings)\n   - Boolean values as true/false\n   - Lists as arrays\n   - Nested data as objects\n3. Preserve any detected relationships between fields\n4. Return data in JSON format\n\nText to analyze:\n{Name: John Doe...}\n</code></pre>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#how_autoschema_detects_structure","title":"How <code>AutoSchema</code> Detects Structure","text":""},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#table_detection","title":"Table Detection","text":"<p>The <code>_looks_like_table</code> method detects table-like structures by analyzing:</p> <ul> <li> <p>Delimiter rows: Lines with multiple delimiters (e.g., tabs, pipes <code>|</code>).</p> </li> <li> <p>Consistent Spacing: Lines with uniform spacing patterns across columns.</p> </li> <li> <p>Header Indicators: Rows with indicators such as hyphens (<code>---</code>) or equals (<code>===</code>) which often denote headers.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#form_field_detection","title":"Form Field Detection","text":"<p>The <code>_detect_form_fields</code> method identifies fields with:</p> <ul> <li> <p>Label-Value Patterns: Lines containing labels followed by values, separated by whitespace or punctuation (e.g., <code>Name: John Doe</code>).</p> </li> <li> <p>Keyword Filtering: Ignores known non-field phrases, focusing only on relevant label-value pairs.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#field_type_inference","title":"Field Type Inference","text":"<p>The <code>_infer_field_type</code> method determines field types based on:</p> <ul> <li> <p>Patterns: Recognizes dates, numbers, and booleans through regular expressions.</p> </li> <li> <p>Content Indicators: Uses delimiters (e.g., commas, semicolons) to detect lists and categorizes remaining data as strings.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#rule_generation","title":"Rule Generation","text":"<p>The <code>_generate_rules</code> method automatically creates validation rules according to the inferred field type, ensuring:</p> <ul> <li> <p>String Fields: Min and max character length limits.</p> </li> <li> <p>Numeric Fields: Range limits (min and max values).</p> </li> <li> <p>Date Fields: Pattern matching (e.g., ISO format).</p> </li> <li> <p>Boolean Fields: Restriction to true/false values.</p> </li> </ul>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#validating_extraction_results","title":"Validating Extraction Results","text":"<p>After extracting data, the <code>validate_extraction</code> method validates each field according to its rules, identifying any inconsistencies or missing values. This ensures that the extracted data meets predefined quality standards.</p>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#example_of_validation","title":"Example of Validation","text":"<pre><code>extracted_data = {\n    \"Name\": \"John Doe\",\n    \"Date of Birth\": \"1990-01-01\",\n    \"Total Amount\": 150.0,\n    \"Item Description\": [\"Item 1\", \"Item 2\"],\n    \"Quantity\": [2, 5],\n    \"Unit Price\": [20.0, 15.0],\n    \"Total\": [40.0, 75.0]\n}\n\n# Validate extracted data\nerrors = auto_schema.validate_extraction(extracted_data)\nif errors:\n    print(\"Validation errors found:\", errors)\nelse:\n    print(\"All fields are valid.\")\n</code></pre>"},{"location":"IndoxMiner/Automatic%20Schema%20Detection/#conclusion","title":"Conclusion","text":"<p>The Automatic Schema Detection feature in Indox Miner simplifies data extraction from unstructured documents by dynamically detecting fields and validating data against inferred rules. This functionality reduces setup time, enhances flexibility, and maintains data quality, making Indox Miner a powerful tool for diverse document processing needs.</p>"},{"location":"IndoxMiner/Document%20Type%20Support/","title":"Document Type Support in Indox Miner","text":"<p>Indox Miner is designed to process various document types, providing users with the flexibility to extract structured information from multiple sources, including text files, spreadsheets, and images. This guide details the supported document types and provides instructions for enabling image processing.</p>"},{"location":"IndoxMiner/Document%20Type%20Support/#supported_document_types","title":"Supported Document Types","text":"<p>Indox Miner can process the following document types:</p> Document Type Extensions Description PDF <code>.pdf</code> Portable Document Format, widely used for sharing formatted documents. Word <code>.doc</code>, <code>.docx</code> Microsoft Word files, common for reports and text-heavy documents. Excel <code>.xls</code>, <code>.xlsx</code> Microsoft Excel spreadsheets, often used for tabular data. PowerPoint <code>.ppt</code>, <code>.pptx</code> Microsoft PowerPoint files, typically used for presentations. Image <code>.png</code>, <code>.jpg</code>, <code>.jpeg</code>, <code>.bmp</code>, <code>.tiff</code>, <code>.heic</code> Image files in various formats (supports OCR for text extraction). Text <code>.txt</code>, <code>.csv</code>, <code>.tsv</code> Plain text and comma/tab-separated files, ideal for simple data. Markdown <code>.md</code> Markdown files for lightweight formatting. RTF <code>.rtf</code> Rich Text Format, allows basic text formatting. Email <code>.eml</code>, <code>.msg</code> Email files, useful for processing email threads and headers. Web Page <code>.html</code> HTML files, enabling processing of saved web pages. XML <code>.xml</code> XML files, common for structured data exchange. EPUB <code>.epub</code> E-book format for reading and processing digital publications."},{"location":"IndoxMiner/Document%20Type%20Support/#enabling_image_processing","title":"Enabling Image Processing","text":"<p>Indox Miner includes support for Optical Character Recognition (OCR), allowing it to extract text from image files. To enable image processing:</p> <ol> <li> <p>Set the OCR flag: The <code>DocumentProcessor</code> class includes a configuration flag for OCR. To process images, set <code>ocr_for_images</code> to <code>True</code> in the configuration.</p> </li> <li> <p>Choose an OCR Model: Indox Miner supports various OCR models, such as Tesseract, PaddleOCR, and EasyOCR. Specify the model in the <code>ocr_model</code> parameter.</p> </li> </ol>"},{"location":"IndoxMiner/Document%20Type%20Support/#example_enabling_image_processing","title":"Example: Enabling Image Processing","text":"<pre><code>from indox_miner.processor import DocumentProcessor, ProcessingConfig\n\n# Configure the document processor with OCR enabled\nconfig = ProcessingConfig(\n    ocr_for_images=True,       # Enable OCR for images\n    ocr_model=\"tesseract\"      # Choose an OCR model (tesseract, paddle, or easyocr)\n)\n\n# Initialize DocumentProcessor with sources and configuration\nsources = [\"path/to/document.pdf\", \"path/to/image.jpg\"]\nprocessor = DocumentProcessor(sources)\n\n# Process documents, including images\nprocessed_data = processor.process(config=config)\n</code></pre>"},{"location":"IndoxMiner/Document%20Type%20Support/#ocr_models_and_their_usage","title":"OCR Models and Their Usage","text":"OCR Model Description Tesseract A highly accurate, open-source OCR engine. PaddleOCR Suitable for multilingual support, uses PaddlePaddle. EasyOCR Quick and efficient, good for basic OCR tasks."},{"location":"IndoxMiner/Document%20Type%20Support/#handling_different_document_types_in_extraction","title":"Handling Different Document Types in Extraction","text":"<ol> <li>Initialize <code>DocumentProcessor</code> with one or more document sources.</li> <li>Set configuration options: Use <code>ProcessingConfig</code> to specify additional settings, such as chunk size, table inference, and OCR options.</li> <li>Run Processing: The processor will automatically detect each document type and apply the appropriate extraction method.</li> </ol>"},{"location":"IndoxMiner/Document%20Type%20Support/#example_processing_multiple_document_types","title":"Example: Processing Multiple Document Types","text":"<pre><code>from indox_miner.processor import DocumentProcessor, ProcessingConfig\n\n# Define document sources\nsources = [\n    \"path/to/document.pdf\",\n    \"path/to/spreadsheet.xlsx\",\n    \"path/to/image.jpg\"\n]\n\n# Configure processor for images and PDFs\nconfig = ProcessingConfig(\n    chunk_size=500,\n    hi_res_pdf=True,\n    ocr_for_images=True,\n    ocr_model=\"tesseract\"\n)\n\n# Initialize the DocumentProcessor and process\nprocessor = DocumentProcessor(sources=sources)\nprocessed_documents = processor.process(config=config)\n</code></pre>"},{"location":"IndoxMiner/Document%20Type%20Support/#advanced_configuration_options","title":"Advanced Configuration Options","text":"<p>The <code>ProcessingConfig</code> class provides additional settings to customize processing:</p> <ul> <li>Chunk Size: Set <code>chunk_size</code> to control the maximum size of text chunks (default is 4048 characters).</li> <li>High-Resolution PDFs: Set <code>hi_res_pdf</code> to <code>True</code> for high-quality PDF processing.</li> <li>Infer Tables: Set <code>infer_tables</code> to <code>True</code> to detect and process tables within documents.</li> <li>Remove Headers/References: Configure <code>remove_headers</code> and <code>remove_references</code> to exclude header and reference sections in structured documents.</li> <li>Filter Empty Elements: Set <code>filter_empty_elements</code> to remove any blank elements from the extracted content.</li> </ul>"},{"location":"IndoxMiner/Document%20Type%20Support/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s document type support allows for efficient extraction from a wide range of formats. By enabling image processing with OCR, users can also extract information from scanned documents and images, making Indox Miner a versatile tool for handling complex document types.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/","title":"IndoxMiner: Extracting Structured Data from Images","text":"<p>IndoxMiner provides a powerful and flexible way to extract structured data from unstructured text within images. Using OCR (Optical Character Recognition) to convert image content to text and LLMs (Large Language Models) to interpret and extract specific fields, IndoxMiner simplifies data extraction from images like invoices, receipts, ID cards, and more.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#key_features","title":"Key Features","text":"<ul> <li>\ud83d\udcf8 Image to Structured Data: Extract information from images and convert it into structured formats.</li> <li>\ud83d\udd20 OCR Integration: Supports multiple OCR models, including PaddleOCR and EasyOCR, for text extraction from images.</li> <li>\ud83d\udd0d Custom Extraction Schemas: Define and validate the data fields you want to extract, tailored to document types like passports and invoices.</li> <li>\u2705 Built-in Validation Rules: Ensures data accuracy with customizable validation options.</li> <li>\ud83d\udcca Easy Conversion to DataFrames: Seamlessly convert results to pandas DataFrames for analysis and manipulation.</li> <li>\ud83e\udd16 LLM Integration: Use OpenAI, IndoxApi, and other LLMs for advanced text interpretation and extraction quality enhancement.</li> <li>\ud83d\udd04 Async Support: Process multiple images concurrently for optimized performance.</li> </ul>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#installation","title":"Installation","text":"<p>To set up IndoxMiner, clone the repository and install dependencies:</p> <pre><code>pip install indoxminer\npip install paddlepaddle paddleocr  # or easyocr, tesseract depending on your choice\n</code></pre> <p>You will also need an OCR library to handle the image-to-text conversion.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#quick_start","title":"Quick Start","text":""},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#step_1_set_up_the_ocr_processor_and_llm","title":"Step 1: Set up the OCR Processor and LLM","text":"<pre><code>from indoxminer import OpenAi, DocumentProcessor, ProcessingConfig, Schema, Extractor\n\n# Initialize OpenAi\nopenai_api = OpenAi(api_key=\"YOUR_API_KEY\")  # Replace with your actual API key\n\n# Initialize OCR processor with configuration\nconfig = ProcessingConfig(ocr_for_images=True, ocr_model='paddle')  # Change to 'easyocr' or 'tesseract' as needed\n</code></pre>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#step_2_define_image_paths_and_schema","title":"Step 2: Define Image Paths and Schema","text":"<p>Define the images to process and select a predefined schema or create a custom one.</p> <pre><code># Define the directory containing passport images\nimage_directory = 'data/passport_dataset_jpg/'\npassport_images = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\n# Check if the paths exist\nfor image_path in passport_images:\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n# Initialize the extractor with the OpenAi LLM and the passport schema\nextractor = Extractor(llm=openai_api, schema=Schema.Passport)\n</code></pre>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#step_3_process_images_and_extract_data","title":"Step 3: Process Images and Extract Data","text":"<p>Process each image with OCR to extract the text, then use IndoxMiner to extract structured data according to the schema.</p> <pre><code>processor = DocumentProcessor(passport_images)\n\n# Process document to extract text using OCR\nresults = processor.process(config)\n\n# Extract data from the documents\nextracted_data = extractor.extract(results)\n</code></pre>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#step_4_handle_results_and_convert_to_dataframe","title":"Step 4: Handle Results and Convert to DataFrame","text":"<p>Process the extraction results and convert them into a DataFrame.</p> <pre><code># Convert extraction results to DataFrame\ndf = extractor.to_dataframe(extracted_data)\nprint(df)\n\n# Display valid results or handle errors\nfor result in extraction_results:\n    if result.is_valid:\n        print(\"Extracted Data:\", result)\n    else:\n        print(\"Extraction errors occurred:\", result.validation_errors)\n</code></pre>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#detailed_workflow","title":"Detailed Workflow","text":"<ol> <li>OCR Processing: Extracts raw text from images using the <code>DocumentProcessor</code>. This component utilizes OCR to convert image-based content into text.</li> <li>Schema Selection: Choose from predefined schemas (like Passport, Invoice, Receipt) or create custom schemas to define the structure of data to be extracted.</li> <li>Data Extraction: Using the selected schema, the LLM processes the extracted text and returns it as structured data according to the specified fields.</li> <li>Validation: Each field undergoes validation based on rules defined in the schema, ensuring accuracy by checking constraints like minimum length, numeric range, and specific patterns.</li> <li>Output Formats: Extracted results can be converted into multiple formats, including JSON, pandas DataFrames, or raw dictionaries, making further data processing seamless.</li> </ol>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#core_components_for_image_extraction","title":"Core Components for Image Extraction","text":""},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#openai","title":"<code>OpenAi</code>","text":"<p>The <code>OpenAi</code> class serves as the primary interface for interacting with the OpenAI. This component handles authentication, manages API requests, and retrieves responses.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#documentprocessor","title":"<code>DocumentProcessor</code>","text":"<p>The <code>DocumentProcessor</code> class is responsible for managing the workflow of document processing, including reading the documents and applying OCR.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#schema","title":"<code>Schema</code>","text":"<p>Schemas define the structure of data to be extracted from the text, including fields and validation rules. </p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#extractor","title":"<code>Extractor</code>","text":"<p>The <code>Extractor</code> is the main class responsible for interacting with the LLM, validating extracted data, and formatting output.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#validation_rules","title":"Validation Rules","text":"<p>Validation rules ensure data quality by setting constraints on each field within a schema.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#supported_output_formats","title":"Supported Output Formats","text":"<ul> <li>JSON: Returns structured data in JSON format, suitable for further processing or storage.</li> <li>DataFrame: Converts the results to a pandas DataFrame for analysis and manipulation.</li> <li>Dictionary: Access the raw extraction results as dictionaries for flexible handling.</li> <li>Markdown: </li> </ul>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#conclusion","title":"Conclusion","text":"<p>In this document, we outlined how to extract structured data from passport images using IndoxMiner. This process automates the retrieval of critical information typically found in passports, facilitating efficient data processing for various applications.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#future_work","title":"Future Work","text":"<p>Consider enhancing this demo by adding error handling for OCR failures, integrating logging for better traceability, or extending the extraction schema for additional fields.</p>"},{"location":"IndoxMiner/Extracting%20Structured%20Data%20from%20Images/#additional_features_of_indoxminer","title":"Additional Features of IndoxMiner","text":"<ul> <li>Dynamic Schema Adaptation: Users can define and adapt schemas dynamically, allowing for easy adjustments to the data extraction process as document formats change.</li> <li>Comprehensive Documentation: IndoxMiner comes with thorough documentation to help users implement features, troubleshoot issues, and optimize their extraction processes.</li> <li>Community Support: As an open-source project, IndoxMiner benefits from community contributions and support, enabling continuous improvement and feature enhancement.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/","title":"Large Language Model (LLM) Support in Indox Miner","text":"<p>Indox Miner supports various Large Language Models (LLMs) for processing and extracting information from documents. Each LLM provides unique features, and users can select the model that best meets their needs for accuracy, performance, or compatibility.</p>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#purpose_of_llm_support","title":"Purpose of LLM Support","text":"<p>Using different LLMs in Indox Miner allows for:</p> <ul> <li> <p>Flexible extraction quality: Choose a model based on the complexity of the document and the level of detail required.</p> </li> <li> <p>Scalability: Some models are optimized for high-speed processing, making them ideal for large-scale extraction tasks.</p> </li> <li> <p>Customizable performance: Whether prioritizing accuracy, speed, or cost-efficiency, each LLM offers a tailored balance of these factors.</p> </li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#available_llms","title":"Available LLMs","text":"<p>Indox Miner supports the following LLMs:</p>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#1_openai_gpt-4","title":"1. OpenAI GPT-4","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: GPT-4 by OpenAI is one of the most advanced LLMs, capable of handling complex text processing and extraction tasks.</li> <li>Best For: High-quality and precise extractions from complex documents.</li> <li>Features:<ul> <li>Asynchronous support for batch processing.</li> <li>Customizable temperature and max tokens for tuning response specificity.</li> </ul> </li> <li>Usage: Ideal for high-detail extractions where accuracy is paramount.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#2_anthropic_claude","title":"2. Anthropic Claude","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: Claude by Anthropic is a robust LLM that balances speed and accuracy, making it a suitable choice for diverse document types.</li> <li>Best For: General-purpose extraction tasks that require a balance between performance and precision.</li> <li>Features:<ul> <li>Configurable temperature for response creativity.</li> <li>Asynchronous processing for scalable tasks.</li> </ul> </li> <li>Usage: Best suited for medium-complexity documents with moderate detail requirements.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#3_ollama_llama","title":"3. Ollama (LLaMA)","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: LLaMA by Meta, integrated through Ollama, is an optimized model designed for efficient data processing.</li> <li>Best For: Scenarios requiring high-throughput processing with acceptable accuracy.</li> <li>Features:<ul> <li>Asynchronous support for concurrent requests.</li> <li>Streaming support for real-time applications.</li> </ul> </li> <li>Usage: A good choice for time-sensitive tasks or high-volume document extraction where speed is prioritized.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#4_nerdtoken_api","title":"4. NerdToken API","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: NerdToken API offers a model designed to work with real-time data extraction in specialized document formats.</li> <li>Best For: Real-time or streaming tasks where data extraction needs to be as efficient as possible.</li> <li>Features:<ul> <li>Advanced temperature, frequency, and presence penalties for tailored responses.</li> <li>Optimized for quick setup and results.</li> </ul> </li> <li>Usage: Suited for specialized applications requiring low latency and immediate data retrieval.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#5_vllm","title":"5. vLLM","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: vLLM provides flexibility with a RESTful API, enabling integration with custom solutions.</li> <li>Best For: Cases where flexible REST API integration is needed for scalable deployments.</li> <li>Features:<ul> <li>REST API-based access for seamless integration with external applications.</li> <li>Highly customizable request parameters.</li> </ul> </li> <li>Usage: Appropriate for flexible integration scenarios with custom API needs.</li> </ul>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#how_to_use_different_llms_in_indox_miner","title":"How to Use Different LLMs in Indox Miner","text":"<ol> <li>Select the LLM: Choose an LLM based on your project\u2019s requirements for accuracy, speed, or data complexity.</li> <li>Initialize the Extractor: Pass the LLM instance as a parameter when initializing the <code>Extractor</code> class.</li> <li>Configure Model Parameters: Adjust parameters like <code>temperature</code>, <code>max_tokens</code>, and <code>model</code> as needed for the chosen LLM.</li> <li>Run Extraction: Execute the extraction process, and Indox Miner will use the selected LLM to generate and validate extracted data.</li> </ol>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#example","title":"Example","text":"<pre><code>from indox_miner.llms import OpenAi, Anthropic, Ollama\nfrom indox_miner.extractor import Extractor\nfrom indox_miner.schema import Schema\n\n# Initialize the LLM\nllm = OpenAi(api_key=\"your-api-key\", model=\"gpt-4\", temperature=0.0)\n\n# Set up the extractor with the schema and LLM\nextractor = Extractor(llm=llm, schema=Schema.Invoice)\n\n# Extract data from the document text\nresult = extractor.extract(\"Your document text here\")\n</code></pre>"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#selecting_the_right_llm","title":"Selecting the Right LLM","text":"LLM Best For Advantages GPT-4 High-accuracy extractions Advanced language capabilities, precise results Claude General-purpose extraction Balanced performance, asynchronous processing LLaMA High-throughput, real-time processing Fast response, streaming support NerdToken Real-time, efficient extractions Quick setup, low-latency processing vLLM Custom integration and flexible deployment REST API support, easily configurable parameters"},{"location":"IndoxMiner/LLM%20Support%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s LLM support provides the flexibility to adapt to various document processing needs, from detailed extractions to high-speed, real-time tasks. By selecting the right LLM for the job, users can optimize Indox Miner for both accuracy and efficiency.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/","title":"Output Types in Indox Miner","text":"<p>Indox Miner provides multiple output formats, enabling users to view and export extracted data in the most convenient form for their needs. The available output types include structured formats such as DataFrame, JSON, Markdown, and Table. Each format offers unique benefits, depending on how the data will be used or shared.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#available_output_types","title":"Available Output Types","text":"Output Type Description DataFrame A pandas DataFrame, useful for data analysis and manipulation in Python. JSON A JSON string, ideal for storing or transmitting structured data. Markdown Markdown format for easily readable text with structured tables. Table A formatted table string, suitable for displaying data in terminal outputs."},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#using_output_types_in_indox_miner","title":"Using Output Types in Indox Miner","text":"<p>The <code>Extractor</code> class in Indox Miner includes methods for each output type, making it simple to format extracted data as needed.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#1_dataframe_output","title":"1. DataFrame Output","text":"<p>The <code>to_dataframe()</code> method converts extraction results to a pandas DataFrame, which is particularly useful for data analysis, manipulation, and visualization.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#example","title":"Example","text":"<pre><code>from indox_miner.extractor import Extractor\nfrom indox_miner.llms import OpenAi\nfrom indox_miner.schema import Schema\n\n# Set up the extractor with LLM and schema\nllm = OpenAi(api_key=\"your-api-key\", model=\"gpt-4\")\nextractor = Extractor(llm=llm, schema=Schema.Invoice)\n\n# Extract data and convert to DataFrame\nresult = extractor.extract(\"Your document text here\")\ndf = extractor.to_dataframe(result)\n\n# Display or manipulate DataFrame\nprint(df)\n</code></pre>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#2_json_output","title":"2. JSON Output","text":"<p>The <code>to_json()</code> method converts results to a JSON string, which is ideal for storing or transmitting data in a structured and compact format.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#example_1","title":"Example","text":"<pre><code>json_output = extractor.to_json(result)\nprint(json_output)\n</code></pre>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#3_markdown_output","title":"3. Markdown Output","text":"<p>The <code>to_markdown()</code> method generates a Markdown-formatted string, which is highly readable and can be directly used in Markdown-compatible platforms like GitHub or documentation tools.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#example_2","title":"Example","text":"<pre><code>markdown_output = extractor.to_markdown(result)\nprint(markdown_output)\n</code></pre>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#4_table_output","title":"4. Table Output","text":"<p>The <code>to_table()</code> method creates a formatted table string, which is especially useful for displaying extracted data directly in the terminal or CLI environments.</p>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#example_3","title":"Example","text":"<pre><code>table_output = extractor.to_table(result)\nprint(table_output)\n</code></pre>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#choosing_the_right_output_type","title":"Choosing the Right Output Type","text":"Use Case Recommended Output Type Data Analysis DataFrame API Integration / Storage JSON Documentation or Reports Markdown Command Line Display Table"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#customizing_output_structure","title":"Customizing Output Structure","text":"<ol> <li>Field Selection: The output format includes all fields defined in the schema by default. To customize this, adjust the schema or filter fields in the resulting DataFrame or JSON.</li> <li>Nested and List Fields: Indox Miner preserves nested structures and lists in output, allowing easy handling of complex data (e.g., items in an invoice or medications in a medical record).</li> <li>Validation and Formatting: All output formats respect validation rules specified in the schema, ensuring data quality and consistency.</li> </ol>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#example_extracting_and_exporting_data","title":"Example: Extracting and Exporting Data","text":"<p>Here\u2019s a complete example of extracting data from a document, then exporting it in different formats:</p> <pre><code># Perform extraction\nresult = extractor.extract(\"Sample document text here\")\n\n# Export to different formats\ndf = extractor.to_dataframe(result)\njson_output = extractor.to_json(result)\nmarkdown_output = extractor.to_markdown(result)\ntable_output = extractor.to_table(result)\n\n# Display or save outputs\nprint(\"DataFrame:\\n\", df)\nprint(\"JSON:\\n\", json_output)\nprint(\"Markdown:\\n\", markdown_output)\nprint(\"Table:\\n\", table_output)\n</code></pre>"},{"location":"IndoxMiner/Output%20Types%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s flexible output options make it easy to integrate extracted data into various workflows, whether for analysis, documentation, or display. By selecting the appropriate output type, users can maximize the utility and accessibility of their data.</p>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/","title":"Predefined Schema Support in Indox Miner","text":"<p>Indox Miner supports a variety of predefined schemas to streamline and standardize the process of extracting structured information from documents. These schemas act as templates that define which fields to extract, along with validation rules to ensure data consistency and accuracy.</p>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#purpose_of_predefined_schemas","title":"Purpose of Predefined Schemas","text":"<p>Predefined schemas help:</p> <ul> <li> <p>Standardize data extraction across different document types.</p> </li> <li> <p>Simplify the setup for users by providing ready-to-use configurations.</p> </li> <li> <p>Ensure data quality by applying field-specific validation rules.</p> </li> <li> <p>Support different document types (e.g., invoices, passports, medical records) with minimal configuration.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#available_predefined_schemas","title":"Available Predefined Schemas","text":"<p>Indox Miner includes the following predefined schemas:</p>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#1_passport_schema","title":"1. Passport Schema","text":"<p>Extracts key information from passport documents, including:</p> <ul> <li> <p>Passport Number: Unique passport identifier.</p> </li> <li> <p>Given Names: First and middle names.</p> </li> <li> <p>Surname: Family name/surname.</p> </li> <li> <p>Date of Birth: Formatted as YYYY-MM-DD.</p> </li> <li> <p>Place of Birth: Location of birth (city and country).</p> </li> <li> <p>Nationality: Country of citizenship.</p> </li> <li> <p>Gender: Gender identifier (M/F/X).</p> </li> <li> <p>Date of Issue: Passport issue date.</p> </li> <li> <p>Date of Expiry: Passport expiration date.</p> </li> <li> <p>Place of Issue: Location of passport issuance.</p> </li> <li> <p>MRZ: Machine-readable zone text.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#2_invoice_schema","title":"2. Invoice Schema","text":"<p>Extracts data from invoices, focusing on financial and transactional details: - Invoice Number: Unique invoice identifier.</p> <ul> <li> <p>Date: Invoice issue date.</p> </li> <li> <p>Company Name: Name of issuing company.</p> </li> <li> <p>Address: Address associated with the company or billing.</p> </li> <li> <p>Customer Name: Name of the customer.</p> </li> <li> <p>Items: List of items with description, quantity, unit price, and total.</p> </li> <li> <p>Subtotal: Pre-tax amount.</p> </li> <li> <p>Tax Amount: Total tax amount.</p> </li> <li> <p>Total Amount: Grand total, including tax.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#3_flight_ticket_schema","title":"3. Flight Ticket Schema","text":"<p>Captures essential details from flight tickets, such as:</p> <ul> <li> <p>Ticket Number: Unique ticket identifier.</p> </li> <li> <p>Passenger Name: Full name of the passenger.</p> </li> <li> <p>Flight Number: Airline flight number.</p> </li> <li> <p>Departure Airport and Arrival Airport: IATA codes for origin and destination airports.</p> </li> <li> <p>Departure DateTime and Arrival DateTime: Departure and arrival times.</p> </li> <li> <p>Seat Number: Assigned seat.</p> </li> <li> <p>Class: Travel class (Economy, Business, First).</p> </li> <li> <p>Booking Reference: PNR or booking code.</p> </li> <li> <p>Fare: Ticket fare.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#4_bank_statement_schema","title":"4. Bank Statement Schema","text":"<p>Supports extraction from bank statements with fields like:</p> <ul> <li> <p>Account Holder: Name on the account.</p> </li> <li> <p>Account Number: Bank account number.</p> </li> <li> <p>IBAN: International Bank Account Number.</p> </li> <li> <p>Statement Period: Month and year of statement.</p> </li> <li> <p>Opening Balance and Closing Balance: Account balances.</p> </li> <li> <p>Transactions: List of transactions with date, description, amount, type, and reference.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#5_medical_record_schema","title":"5. Medical Record Schema","text":"<p>Facilitates the extraction of patient medical records:</p> <ul> <li> <p>Patient Name: Full name of the patient.</p> </li> <li> <p>Date of Birth: Formatted as YYYY-MM-DD.</p> </li> <li> <p>Medical Record Number: Unique identifier for the record.</p> </li> <li> <p>Diagnosis: List of diagnoses with code, description, and date.</p> </li> <li> <p>Medications: List of prescribed medications.</p> </li> <li> <p>Physician: Information about the treating physician.</p> </li> <li> <p>Vital Signs: Basic vitals like blood pressure, heart rate, temperature.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#6_driver_license_schema","title":"6. Driver License Schema","text":"<p>Extracts information from driver licenses, such as:</p> <ul> <li> <p>License Number: Unique identifier.</p> </li> <li> <p>Full Name: License holder\u2019s full name.</p> </li> <li> <p>Address: Residential address.</p> </li> <li> <p>Date of Birth: Formatted as YYYY-MM-DD.</p> </li> <li> <p>Issue Date and Expiry Date: License issuance and expiration dates.</p> </li> <li> <p>Class: License class or type.</p> </li> <li> <p>Restrictions and Endorsements: Lists of restrictions and endorsements.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#7_resume_schema","title":"7. Resume Schema","text":"<p>Enables structured extraction from resumes or CVs:</p> <ul> <li> <p>Full Name: Candidate's name.</p> </li> <li> <p>Contact: Includes email, phone, address, LinkedIn, portfolio, GitHub.</p> </li> <li> <p>Professional Summary: Short summary or objective.</p> </li> <li> <p>Work Experience: List of work history with company, position, dates, and achievements.</p> </li> <li> <p>Education: Academic background, degrees, institutions, dates, and GPA.</p> </li> <li> <p>Skills: Categorized technical, soft, and language skills.</p> </li> <li> <p>Projects: Description of key projects.</p> </li> <li> <p>Awards, Certifications, Publications, Volunteer Experience, Additional Information.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#how_to_use_predefined_schemas","title":"How to Use Predefined Schemas","text":"<p>To use a predefined schema:</p> <ol> <li> <p>Select the appropriate schema based on the document type.</p> </li> <li> <p>Configure the extractor to use the selected schema.</p> </li> <li> <p>Run the extraction, and Indox Miner will apply the schema's structure and validation rules.</p> </li> </ol>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#customization_and_extension","title":"Customization and Extension","text":"<p>While Indox Miner provides these predefined schemas out of the box, users can:</p> <ul> <li> <p>Customize existing schemas by modifying field requirements or validation rules.</p> </li> <li> <p>Create new schemas for unsupported document types by defining custom fields and rules.</p> </li> </ul>"},{"location":"IndoxMiner/Predefined%20Schema%20Support%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Predefined schemas in Indox Miner make it easier and faster to extract structured information from various documents accurately and consistently. By leveraging these schemas, users can significantly reduce the setup time required for data extraction tasks and ensure high-quality data output.</p>"}]}